error_type,severity,domain,scenario,language,complexity,flawed_code,critique,corrected_code,learning_resources,principle,generated_at,model,data_version,is_positive_example
correlation_causation,critical,education,student performance prediction,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
students = pd.DataFrame({'math_score': np.random.normal(70, 10, 100), 
                        'reading_score': np.random.normal(80, 12, 100), 
                         'gpa': np.random.normal(3.5, 0.5, 100)})

# Correlation analysis
corr_matrix = students.corr()
print(corr_matrix)

# Plot the correlation matrix
plt.figure(figsize=(8, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)
plt.show()

# Implying causal relationship
print(""Based on the correlation analysis, it appears that math score has a strong positive correlation with reading score."")
print(""Therefore, it is reasonable to conclude that higher math scores directly cause higher reading scores."")

# Flawed conclusion
print(""This study demonstrates that improving math scores will automatically improve reading scores."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in education context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of student performance prediction, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in student performance prediction'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Generate synthetic data
np.random.seed(0)
students = pd.DataFrame({'math_score': np.random.normal(70, 10, 100), 
                        'reading_score': np.random.normal(80, 12, 100), 
                         'gpa': np.random.normal(3.5, 0.5, 100)})

# Split data into training and testing sets
X = students[['math_score', 'gpa']]
y = students['reading_score']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Perform linear regression
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Evaluate the model
y_pred = lr_model.predict(X_test)
print(""R-squared:"", lr_model.score(X_test, y_test))

# Corrected conclusion
print(""The results suggest that math score and GPA are both important predictors of reading score, but we cannot conclude that higher math scores directly cause higher reading scores."")
print(""Further investigation, such as a randomized controlled trial, is needed to establish causality."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:12:40.368069,llama-3-8b-instruct,v1,False
correlation_causation,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"# Generate synthetic data
import numpy as np
import pandas as pd

np.random.seed(0)
n = 1000
X = np.random.normal(0, 1, n)  # Independent variable (age)
Y = np.random.normal(0, 1, n) + 0.5 * X + np.random.normal(0, 0.1, n)  # Dependent variable (blood pressure)

df = pd.DataFrame({'Age': X, 'Blood Pressure': Y})

# Perform analysis
from scipy.stats import pearsonr
corr_coeff, _ = pearsonr(df['Age'], df['Blood Pressure'])
print(f""Correlation coefficient: {corr_coeff:.3f}"")

# Implying causal relationship
print(""Based on the strong correlation, it appears that age is a significant predictor of blood pressure. Therefore, we can conclude that increasing age causes an increase in blood pressure."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in healthcare context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","# Generate synthetic data
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

X, y = make_regression(n_samples=1000, n_features=1, noise=0.1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Perform analysis
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
print(f""R-squared: {lr_model.score(X_test, y_test):.3f}"")

# Use causal inference methods
from causalml.inference import CausalInference
ci = CausalInference(X_train, y_train, X_test, y_test)
ci.fit()
print(f""Causal effect: {ci.effect():.3f}"")

# Correct conclusion
print(""Based on the analysis, we found a strong correlation and a significant causal effect between age and blood pressure. However, we must note that this conclusion is subject to the limitations of our dataset and analysis, and further research is needed to establish a causal relationship."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:13:39.943379,llama-3-8b-instruct,v1,False
correlation_causation,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(42)
data = pd.DataFrame({
    'Churn': np.random.binomial(1, 0.2, 1000),  # 20% of customers churn
    'Customer_Age': np.random.randint(18, 65, 1000),
    'Monthly_Spend': np.random.normal(50, 10, 1000)
})

# Perform analysis
churn_corr_matrix = data.corr()
print(churn_corr_matrix)

# Implying causal relationship from correlational data
print(""Correlation between Churn and Monthly_Spend:"", churn_corr_matrix['Churn']['Monthly_Spend'])
print(""Conclusion: Increasing Monthly_Spend causes customer churn!"")

# Incorrect reasoning: Assuming a causal relationship based on correlation
# Without experimental design or causal inference methods, this is a flawed conclusion","{'summary': 'Code demonstrates Correlation vs Causation Confusion in business context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Generate synthetic data
np.random.seed(42)
data = pd.DataFrame({
    'Churn': np.random.binomial(1, 0.2, 1000),  # 20% of customers churn
    'Customer_Age': np.random.randint(18, 65, 1000),
    'Monthly_Spend': np.random.normal(50, 10, 1000),
    'Other_Features': np.random.normal(0, 1, 1000)  # additional features
})

# Split data into training and testing sets
X = data.drop('Churn', axis=1)
y = data['Churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a random forest model to predict churn
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)

# Evaluate the model's performance
y_pred = rf_model.predict(X_test)
print(""Accuracy:"", rf_model.score(X_test, y_test))

# Correct approach: Use machine learning algorithms to model the relationship
# between Churn and other features, rather than implying causation from correlation","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:14:41.557275,llama-3-8b-instruct,v1,False
correlation_causation,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Region': ['North', 'North', 'South', 'South', 'East', 'East', 'West', 'West'],
    'Sales': [100, 120, 80, 90, 150, 130, 110, 120],
    'Temperature': [20, 25, 15, 18, 22, 25, 20, 22]
})

# Correlation analysis
correlation_matrix = data.corr()
print(""Correlation matrix:"")
print(correlation_matrix)

# Implying causal relationship
print(""Conclusion: There is a strong positive correlation between Sales and Temperature. This means that a rise in temperature directly causes an increase in sales."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in business context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Region': ['North', 'North', 'South', 'South', 'East', 'East', 'West', 'West'],
    'Sales': [100, 120, 80, 90, 150, 130, 110, 120],
    'Temperature': [20, 25, 15, 18, 22, 25, 20, 22]
})

# Feature engineering: Create dummy variables for categorical data
region_dummies = pd.get_dummies(data['Region'])
data = pd.concat([data, region_dummies], axis=1)

# Linear regression analysis
X = data[['Temperature', 'North', 'South', 'East', 'West']]
y = data['Sales']
model = LinearRegression()
model.fit(X, y)

# Interpretation: The coefficient of Temperature is 10. This means that a 1-unit increase in temperature is associated with a 10-unit increase in sales, holding all other variables constant.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:15:33.619078,llama-3-8b-instruct,v1,False
correlation_causation,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n = 1000
X = np.random.normal(size=n)  # independent variable
Y = np.random.normal(size=n) + 2 * X + np.random.normal(size=n)  # dependent variable

# Create a DataFrame
df = pd.DataFrame({'X': X, 'Y': Y})

# Correlation analysis
corr_coef = np.corrcoef(X, Y)[0, 1]
print(f""Correlation coefficient: {corr_coef:.2f}"")

# Plot the data
plt.scatter(X, Y)
plt.xlabel('X')
plt.ylabel('Y')
plt.title(f""Correlation between X and Y: {corr_coef:.2f}"")
plt.show()

# Flawed conclusion
print(""Based on the strong correlation between X and Y, we can conclude that X causes Y."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in social_science context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.stats import medians

# Generate synthetic data
np.random.seed(0)
n = 1000
X = np.random.normal(size=n)  # independent variable
Y = np.random.normal(size=n) + 2 * X + np.random.normal(size=n)  # dependent variable

# Create a DataFrame
df = pd.DataFrame({'X': X, 'Y': Y})

# Correlation analysis
corr_coef = np.corrcoef(X, Y)[0, 1]
print(f""Correlation coefficient: {corr_coef:.2f}"")

# Plot the data
plt.scatter(X, Y)
plt.xlabel('X')
plt.ylabel('Y')
plt.title(f""Correlation between X and Y: {corr_coef:.2f}"")
plt.show()

# Corrected conclusion
print(""The correlation between X and Y suggests a potential relationship, but it does not imply causality. Further investigation is needed to establish a causal link."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:16:36.114704,llama-3-8b-instruct,v1,False
correlation_causation,critical,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_classes = 2
df = pd.DataFrame({
    'Student_ID': np.arange(1, n_students+1),
    'Teaching_Method': np.random.choice(['Traditional', 'Online'], size=n_students),
    'Grade': np.random.normal(70, 10, n_students) + np.random.choice([-10, 10], size=n_students),
    'Time_Spent': np.random.normal(30, 10, n_students) + np.random.choice([-5, 5], size=n_students)
})

# Correlation analysis
corr_matrix = df.corr()
print(corr_matrix)

# Plotting correlation matrix
plt.figure(figsize=(10, 8))
plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')
plt.colorbar()
plt.title('Correlation Matrix')
plt.xlabel('Features')
plt.ylabel('Features')
plt.show()

# Conclusion
print(""There is a strong correlation between Teaching_Method and Grade. Therefore, we can conclude that the Online teaching method is more effective."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in education context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_classes = 2
df = pd.DataFrame({
    'Student_ID': np.arange(1, n_students+1),
    'Teaching_Method': np.random.choice(['Traditional', 'Online'], size=n_students),
    'Grade': np.random.normal(70, 10, n_students) + np.random.choice([-10, 10], size=n_students),
    'Time_Spent': np.random.normal(30, 10, n_students) + np.random.choice([-5, 5], size=n_students)
})

# Split data into training and testing sets
X = df[['Teaching_Method', 'Time_Spent']]
y = df['Grade']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Perform linear regression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
print(""R-squared: "", model.score(X_test, y_test))

# Plotting the results
plt.figure(figsize=(8, 6))
plt.scatter(X_test.iloc[:, 0], y_test, label='Actual')
plt.plot(X_test.iloc[:, 0], y_pred, label='Predicted')
plt.xlabel('Time Spent')
plt.ylabel('Grade')
plt.title('Grade vs Time Spent')
plt.legend()
plt.show()

# Conclusion
print(""The relationship between Time Spent and Grade is significant. Further analysis is needed to determine the effect of Teaching Method on Grade."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:17:48.495665,llama-3-8b-instruct,v1,False
correlation_causation,critical,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
grad_rate = np.random.normal(loc=0.7, scale=0.1, size=(1000, 1))  # Graduation rate
demographics = np.random.normal(loc=0.5, scale=0.1, size=(1000, 1))  # Demographic variable (e.g., GPA, parental income)

df = pd.DataFrame(np.hstack((grad_rate, demographics)), columns=['Graduation Rate', 'Demographics'])

# Perform analysis
correlation_matrix = df.corr()
print(correlation_matrix)

# Implying causal relationship from correlational data
print(""Based on the correlation analysis, we can conclude that demographics have a significant impact on graduation rate. A 0.1 increase in demographics corresponds to a 0.05 increase in graduation rate."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in education context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","import pandas as pd
import numpy as np
from statsmodels.stats.anova import anova_lm
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
grad_rate = np.random.normal(loc=0.7, scale=0.1, size=(1000, 1))  # Graduation rate
demographics = np.random.normal(loc=0.5, scale=0.1, size=(1000, 1))  # Demographic variable (e.g., GPA, parental income)

df = pd.DataFrame(np.hstack((grad_rate, demographics)), columns=['Graduation Rate', 'Demographics'])

# Perform analysis
lm = LinearRegression()
lm.fit(df[['Demographics']], df['Graduation Rate'])
print(""R-squared:"", lm.score(df[['Demographics']], df['Graduation Rate']))

# Perform ANOVA to check for significance
anova_results = anova_lm(lm)
print(""ANOVA F-statistic:"", anova_results['F'][0])

# Use correct language and acknowledge limitations
print(""The relationship between demographics and graduation rate is statistically significant (p-value < 0.05). However, this analysis does not establish causality. Further research is needed to determine the direction and nature of the relationship."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:18:50.774770,llama-3-8b-instruct,v1,False
correlation_causation,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n = 100
X = np.random.normal(0, 1, n)  # independent variable
Y = np.random.normal(0, 1, n) + 0.5 * X + np.random.normal(0, 0.1, n)  # dependent variable

# Create a DataFrame
df = pd.DataFrame({'X': X, 'Y': Y})

# Perform analysis
plt.scatter(X, Y)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Relationship between X and Y')
plt.show()

# Imply a causal relationship
print(""There is a strong positive correlation between X and Y, therefore, increasing X will increase Y."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in healthcare context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n = 100
X = np.random.normal(0, 1, n)  # independent variable
Y = np.random.normal(0, 1, n) + 0.5 * X + np.random.normal(0, 0.1, n)  # dependent variable

# Create a DataFrame
df = pd.DataFrame({'X': X, 'Y': Y})

# Perform analysis
plt.scatter(X, Y)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Relationship between X and Y')
plt.show()

# Use linear regression to model the relationship
X_train, X_test, Y_train, Y_test = train_test_split(df['X'], df['Y'], test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train.reshape(-1, 1), Y_train)
Y_pred = model.predict(X_test.reshape(-1, 1))

# Interpret the results
print(""The relationship between X and Y is:"")
print(""Coefficient:"", model.coef_)
print(""Intercept:"", model.intercept_)
print(""R-squared:"", model.score(X_test.reshape(-1, 1), Y_test))","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:19:54.847037,llama-3-8b-instruct,v1,False
correlation_causation,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
df = pd.DataFrame({
    'Customer ID': np.arange(1, 101),
    'Churned': np.random.binomial(1, 0.2, size=100),  # 20% churn rate
    'Purchase Frequency': np.random.poisson(3, size=100),  # average 3 purchases per customer
    'Average Purchase Value': np.random.normal(50, 10, size=100)  # average purchase value of $50 with std dev $10
})

# Correlation analysis
corr_matrix = df.corr()
print(corr_matrix)

# Plot correlation matrix
plt.figure(figsize=(8, 8))
plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')
plt.xticks(range(len(df.columns)), df.columns)
plt.yticks(range(len(df.columns)), df.columns)
plt.colorbar()
plt.show()

# Conclusion
print(""There is a strong correlation between 'Purchase Frequency' and 'Churned', which suggests that frequent purchases cause customers to churn."")
print(""Therefore, we can conclude that increasing purchase frequency will lead to increased churn."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in business context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of customer churn prediction analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in customer churn prediction analysis'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Generate synthetic data
np.random.seed(0)
df = pd.DataFrame({
    'Customer ID': np.arange(1, 101),
    'Churned': np.random.binomial(1, 0.2, size=100),  # 20% churn rate
    'Purchase Frequency': np.random.poisson(3, size=100),  # average 3 purchases per customer
    'Average Purchase Value': np.random.normal(50, 10, size=100)  # average purchase value of $50 with std dev $10
})

# Split data into training and testing sets
X = df[['Purchase Frequency', 'Average Purchase Value']]
y = df['Churned']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Train a random forest classifier
rf = RandomForestClassifier()
rf.fit(X_train, y_train)

# Evaluate the model
y_pred = rf.predict(X_test)
print(""Accuracy:"", accuracy_score(y_test, y_pred))

# Conclusion
print(""The random forest model shows that 'Purchase Frequency' and 'Average Purchase Value' are important predictors of 'Churned'."")
print(""However, we cannot conclude that 'Purchase Frequency' causes 'Churned' without further investigation and experimental design."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:21:05.155954,llama-3-8b-instruct,v1,False
correlation_causation,critical,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n = 100
X = np.random.normal(0, 1, n)  # variable A (treatment group)
Y = np.random.normal(0, 1, n)  # variable B (outcome variable)

# Create a DataFrame
df = pd.DataFrame({'X': X, 'Y': Y})

# Perform analysis
correlation = np.corrcoef(X, Y)[0, 1]
print(f""Correlation coefficient: {correlation:.2f}"")

# Plot the data
plt.scatter(X, Y)
plt.xlabel('X (treatment)')
plt.ylabel('Y (outcome)')
plt.title(f""Correlation: {correlation:.2f}"")
plt.show()

# Implying causal relationship
print(""Conclusion: The treatment (X) causes the outcome (Y)."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in healthcare context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.stats.anova import anova_lm
from statsmodels.formula.api import ols

# Generate synthetic data
np.random.seed(0)
n = 100
X = np.random.normal(0, 1, n)  # variable A (treatment group)
Y = np.random.normal(0, 1, n)  # variable B (outcome variable)

# Create a DataFrame
df = pd.DataFrame({'X': X, 'Y': Y})

# Perform analysis
# 1. Check for correlation
correlation = np.corrcoef(X, Y)[0, 1]
print(f""Correlation coefficient: {correlation:.2f}"")

# 2. Perform linear regression to check for causality
model = ols('Y ~ X', data=df).fit()
anova_table = anova_lm(model)
print(anova_table)

# 3. Visualize the data with a scatter plot
plt.scatter(X, Y)
plt.xlabel('X (treatment)')
plt.ylabel('Y (outcome)')
plt.title('Scatter plot of X vs Y')
plt.show()

# 4. Conclude that there is no evidence of causality
print(""Conclusion: There is no evidence to suggest that X causes Y."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:22:14.123499,llama-3-8b-instruct,v1,False
correlation_causation,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
income = np.random.normal(50000, 10000, 1000)
education = np.random.normal(2, 1, 1000)
correlation = np.corrcoef(income, education)[0, 1]

# Perform analysis that commits the error
print(""Correlation between income and education:"", correlation)

# Imply a causal relationship from correlational data
print(""Conclusion: Education causes higher income!"")

# Incorrect reasoning: ""Since income and education are strongly correlated, it's obvious that more education leads to higher income.""","{'summary': 'Code demonstrates Correlation vs Causation Confusion in social_science context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np
from statsmodels.graphics import regression

# Generate synthetic data
np.random.seed(0)
income = np.random.normal(50000, 10000, 1000)
education = np.random.normal(2, 1, 1000)

# Perform analysis that avoids the error
print(""Correlation between income and education:"", np.corrcoef(income, education)[0, 1])

# Use a causal inference method to establish a causal relationship
# (e.g., instrumental variables, regression discontinuity, etc.)
# OR
# Use experimental design to establish a causal relationship
print(""Conclusion: More education may lead to higher income, but this requires further investigation."")

# Corrected reasoning: ""A strong correlation between income and education suggests a potential relationship, but further investigation is needed to establish a causal link.""","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:23:01.719185,llama-3-8b-instruct,v1,False
correlation_causation,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
churn_data = pd.DataFrame({
    'Customer_ID': np.arange(1, 1001),
    'Monthly_Spend': np.random.normal(50, 10, 1000),
    'Average_Order_Size': np.random.normal(20, 5, 1000),
    'Churned': np.random.binomial(1, 0.2, 1000)  # 20% churn rate
})

# Perform analysis
churn_data.groupby('Monthly_Spend')['Churned'].mean().plot(kind='bar')
print(""There is a strong positive correlation between monthly spend and churned."")
print(""Therefore, we can conclude that increasing monthly spend causes customers to churn."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in business context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import pandas as pd
import numpy as np
from statsmodels.stats import ANOVA

# Generate synthetic data
np.random.seed(0)
churn_data = pd.DataFrame({
    'Customer_ID': np.arange(1, 1001),
    'Monthly_Spend': np.random.normal(50, 10, 1000),
    'Average_Order_Size': np.random.normal(20, 5, 1000),
    'Churned': np.random.binomial(1, 0.2, 1000)  # 20% churn rate
})

# Perform analysis
anova_model = ANOVA.from_formula('Churned ~ Monthly_Spend + Average_Order_Size', data=churn_data)
anova_results = anova_model.fit()
print(anova_results.summary())

# Interpret results carefully
# If there is a significant interaction between variables, we may need to use a more advanced model
# or experimentally design to establish causality","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:23:56.030913,llama-3-8b-instruct,v1,False
correlation_causation,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
n = 1000
X = np.random.normal(0, 1, n)
Y = np.random.normal(0, 1, n) + 0.5 * X

# Load data
df = pd.DataFrame({'Price': X, 'Sales': Y})

# Perform analysis
sns.set()
plt.scatter(df['Price'], df['Sales'])
plt.xlabel('Price')
plt.ylabel('Sales')
plt.title('Price vs Sales')
plt.show()

# Correlation analysis
corr_coef = np.corrcoef(df['Price'], df['Sales'])[0, 1]
print(f""Correlation Coefficient: {corr_coef:.2f}"")

# Imply causal relationship
print(""Based on the correlation analysis, we can conclude that increasing price leads to increased sales."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in business context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Generate synthetic data
n = 1000
X = np.random.normal(0, 1, n)
Y = np.random.normal(0, 1, n) + 0.5 * X

# Load data
df = pd.DataFrame({'Price': X, 'Sales': Y})

# Perform analysis
sns.set()
plt.scatter(df['Price'], df['Sales'])
plt.xlabel('Price')
plt.ylabel('Sales')
plt.title('Price vs Sales')
plt.show()

# Linear regression analysis
model = LinearRegression()
model.fit(df[['Price']], df['Sales'])
print(f""R-squared: {r2_score(df['Sales'], model.predict(df[['Price']])):.2f}"")

# No causal inference
print(""The linear regression analysis shows a positive relationship between price and sales, but we cannot conclude that increasing price causes increased sales without experimental design or causal inference methods."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:24:49.435889,llama-3-8b-instruct,v1,False
correlation_causation,critical,education,teaching method effectiveness comparison,python,intermediate,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_features = 2
X = np.random.rand(n_students, n_features)
y = np.random.rand(n_students)  # student performance (1-10)

# Create a DataFrame
df = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))), columns=['feature1', 'feature2', 'performance'])

# Correlation analysis
corr_matrix = df.corr()
print(corr_matrix)

# Implying causal relationship from correlational data
print(""Feature1 and Feature2 are highly correlated (r = {:.2f})"".format(corr_matrix['feature1']['feature2']))
print(""Therefore, increasing Feature1 will likely increase Feature2, which in turn will improve student performance."")
print(""We should recommend that students focus on improving Feature1 to boost their performance."")

# Flawed conclusion
print(""Based on our analysis, we can confidently conclude that Feature1 is a causal factor in improving student performance."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in education context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_features = 2
X = np.random.rand(n_students, n_features)
y = np.random.rand(n_students)  # student performance (1-10)

# Create a DataFrame
df = pd.DataFrame(np.hstack((X, y.reshape(-1, 1))), columns=['feature1', 'feature2', 'performance'])

# Linear regression analysis
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
model = LinearRegression()
model.fit(X_train, y_train)
print(""R-squared: {:.2f}"".format(model.score(X_test, y_test)))

# Corrected conclusion
print(""The relationship between Feature1 and Feature2 is not necessarily causal. We need to use experimental design or causal inference methods to establish causality."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:25:46.805388,llama-3-8b-instruct,v1,False
correlation_causation,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n = 1000
data = pd.DataFrame({
   'social_media_usage': np.random.normal(2, 1, n),
   'mental_health_score': np.random.normal(50, 10, n)
})

# Perform analysis
corr_coef = data['social_media_usage'].corr(data['mental_health_score'])
print(f""Correlation Coefficient: {corr_coef:.2f}"")

# Plot the data
plt.scatter(data['social_media_usage'], data['mental_health_score'])
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health Score')
plt.title('Correlation between Social Media Usage and Mental Health Score')
plt.show()

# Flawed conclusion
print(""Our analysis shows that excessive social media usage causes poor mental health. Therefore, we recommend reducing social media usage to improve mental well-being."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in social_science context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.stats.anova import anova_lm
from statsmodels.formula.api import ols

# Generate synthetic data
np.random.seed(0)
n = 1000
data = pd.DataFrame({
   'social_media_usage': np.random.normal(2, 1, n),
   'mental_health_score': np.random.normal(50, 10, n),
    'age': np.random.normal(30, 5, n)
})

# Perform analysis
model = ols('mental_health_score ~ social_media_usage + age', data=data).fit()
anova_table = anova_lm(model)
print(anova_table)

# Plot the data
plt.scatter(data['social_media_usage'], data['mental_health_score'])
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health Score')
plt.title('Relationship between Social Media Usage and Mental Health Score')
plt.show()

# Corrected conclusion
print(""Our analysis suggests that social media usage is not a significant predictor of mental health score. The relationship may be influenced by other factors, such as age. We recommend exploring other variables and using more rigorous methods to establish causality."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:26:50.369066,llama-3-8b-instruct,v1,False
correlation_causation,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Price': np.random.normal(10, 2, 100),
    'Sales': np.random.normal(50, 10, 100),
    'Discount': np.random.normal(5, 1, 100)
})

# Correlation analysis
corr_matrix = data.corr()
print(corr_matrix)

# Imply causal relationship
print(""As the price increases, sales also increase, therefore, a price increase causes an increase in sales."")
print(""We can conclude that a 10% price increase will lead to a 5% increase in sales."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in business context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import pandas as pd
import numpy as np
from statsmodels.form import OLS

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Price': np.random.normal(10, 2, 100),
    'Sales': np.random.normal(50, 10, 100),
    'Discount': np.random.normal(5, 1, 100)
})

# Regression analysis
X = data[['Price', 'Discount']]
y = data['Sales']
model = OLS(y, X).fit()
print(model.summary())

# Properly interpret the results
print(""The coefficient for 'Price' is 0.5, indicating that for every 1-unit increase in price, sales increase by 0.5 units."")
print(""However, we cannot conclude that a price increase causes an increase in sales without considering other factors."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:27:39.255274,llama-3-8b-instruct,v1,False
correlation_causation,critical,education,student performance prediction,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_features = 2
X = np.random.rand(n_students, n_features)
y = np.random.rand(n_students, 1) + 3 * X[:, 0] + 2 * X[:, 1]

# Create a DataFrame
df = pd.DataFrame(np.hstack((X, y)), columns=['Feature1', 'Feature2', 'Target'])

# Perform analysis
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['Feature1', 'Feature2']], df['Target'])
print(""Feature1 has a significant effect on Target:"", model.coef_[0])
print(""Feature2 has a significant effect on Target:"", model.coef_[1])

# Conclusion
print(""Based on our analysis, Feature1 is the primary driver of student performance, and Feature2 has a negligible impact."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in education context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of student performance prediction, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in student performance prediction'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# Generate synthetic data
X, y = make_regression(n_samples=100, n_features=2, noise=0.1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Perform analysis
model = LinearRegression()
model.fit(X_train, y_train)
print(""Feature1 has a significant effect on Target:"", model.coef_[0])
print(""Feature2 has a significant effect on Target:"", model.coef_[1])

# Conclusion
print(""Our analysis suggests that there may be a relationship between the features and the target, but we cannot conclude that this relationship is causal."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:28:38.289727,llama-3-8b-instruct,v1,False
correlation_causation,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
income = np.random.normal(50000, 10000, 1000)
education = np.random.normal(15, 2, 1000)

# Create a DataFrame
df = pd.DataFrame({'Income': income, 'Education': education})

# Perform analysis
correlation_coefficient = np.corrcoef(df['Income'], df['Education'])[0, 1]
print(""Correlation Coefficient:"", correlation_coefficient)

# Imply a causal relationship
print(""Conclusion: There is a clear causal relationship between income and education. As education increases, income also increases."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in social_science context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np
from statsmodels.stats.outliers_influence import summary_table

# Generate synthetic data
np.random.seed(0)
income = np.random.normal(50000, 10000, 1000)
education = np.random.normal(15, 2, 1000)

# Create a DataFrame
df = pd.DataFrame({'Income': income, 'Education': education})

# Perform analysis
correlation_coefficient = np.corrcoef(df['Income'], df['Education'])[0, 1]
print(""Correlation Coefficient:"", correlation_coefficient)

# Use a causal inference method (e.g., regression analysis)
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['Education']], df['Income'])
print(""Causal Coefficient:"", model.coef_)

# Correctly interpret the results
print(""Conclusion: There is a statistically significant positive relationship between education and income, but we cannot infer a causal relationship without further analysis."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:29:30.362905,llama-3-8b-instruct,v1,False
correlation_causation,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"# Generate synthetic data
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # independent variable
y = np.random.normal(0, 1, n) + 0.5 * x + np.random.normal(0, 0.1, n)  # dependent variable (with correlation)

df = pd.DataFrame({'Drug_A': x, 'Outcome': y})

# Perform analysis
corr_coef = np.corrcoef(df['Drug_A'], df['Outcome'])[0, 1]
print(""Correlation coefficient:"", corr_coef)

# Implying causal relationship from correlational data
print(""Conclusion: Increasing the dose of Drug_A significantly improves the outcome, as indicated by the strong correlation (r ="", corr_coef, "")."")

# Plot the data
plt.scatter(df['Drug_A'], df['Outcome'])
plt.xlabel('Drug_A')
plt.ylabel('Outcome')
plt.title('Correlation between Drug_A and Outcome')
plt.show()","{'summary': 'Code demonstrates Correlation vs Causation Confusion in healthcare context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","# Generate synthetic data
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # independent variable
y = np.random.normal(0, 1, n) + 0.5 * x + np.random.normal(0, 0.1, n)  # dependent variable (with correlation)

df = pd.DataFrame({'Drug_A': x, 'Outcome': y})

# Perform analysis
corr_coef = np.corrcoef(df['Drug_A'], df['Outcome'])[0, 1]
print(""Correlation coefficient:"", corr_coef)

# Correctly interpret the correlation
print(""Conclusion: There is a strong correlation between Drug_A and Outcome, but this does not imply a causal relationship. Further research is needed to establish causality."")

# Plot the data
plt.scatter(df['Drug_A'], df['Outcome'])
plt.xlabel('Drug_A')
plt.ylabel('Outcome')
plt.title('Correlation between Drug_A and Outcome')
plt.show()","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:30:28.310522,llama-3-8b-instruct,v1,False
correlation_causation,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 1000
crime_rate = np.random.normal(50, 5, n)
weather_temp = np.random.normal(20, 2, n)
rainfall = np.random.normal(10, 3, n)

# Create a DataFrame
df = pd.DataFrame({'crime_rate': crime_rate, 'weather_temp': weather_temp, 'rainfall': rainfall})

# Perform analysis
correlation_matrix = df.corr()
print(""Correlation matrix:"")
print(correlation_matrix)

# Imply causal relationship from correlational data
print(""\nConclusion:"")
print(""There is a strong positive correlation between crime rate and weather temp (r = 0.8). This means that as weather temperature increases, crime rate also increases. Therefore, we can conclude that higher weather temperatures cause an increase in crime rate."")","{'summary': 'Code demonstrates Correlation vs Causation Confusion in social_science context', 'detailed_explanation': 'This code demonstrates Correlation vs Causation Confusion. Correlation does not imply causation without experimental design or causal inference methods In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import numpy as np
from statsmodels.stats import anova_lm

# Generate synthetic data
np.random.seed(0)
n = 1000
crime_rate = np.random.normal(50, 5, n)
weather_temp = np.random.normal(20, 2, n)
rainfall = np.random.normal(10, 3, n)

# Create a DataFrame
df = pd.DataFrame({'crime_rate': crime_rate, 'weather_temp': weather_temp, 'rainfall': rainfall})

# Perform analysis
correlation_matrix = df.corr()
print(""Correlation matrix:"")
print(correlation_matrix)

# Explore potential confounding variables
anova_model = anova_lm(ols('crime_rate ~ weather_temp + rainfall', data=df).fit())
print(""ANOVA results:"")
print(anova_model)

# Correctly interpret the results
print(""\nConclusion:"")
print(""While there is a strong positive correlation between crime rate and weather temp, we cannot conclude that weather temperature causes an increase in crime rate without controlling for potential confounding variables. Future research should investigate the relationship using experimental design or causal inference methods."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Correlation does not imply causation without experimental design or causal inference methods']",Correlation does not imply causation without experimental design or causal inference methods,2025-12-14T19:31:24.801788,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,education,teaching method effectiveness comparison,python,intermediate,"**Explanation**

This code is problematic because it ignores the student variable, which is a crucial factor in this analysis. By aggregating the data without considering the student variable, the code is committing the Simpson's Paradox error. The conclusion drawn from the flawed code is incorrect, as the relationship between the method and the score is not accurately represented.

Simpson's Paradox states that aggregated data can show opposite trends from stratified data. In this case, the code is treating the data as a single group, ignoring the fact that the students are divided into two categories (A and B). When we examine the data by student, we may find that the relationship between the method and score is different for each student group. This is the correct approach, as we should always consider the underlying factors that may influence the relationship between variables.

**Corrected Code**","{'summary': ""Code demonstrates Simpson's Paradox in education context"", 'detailed_explanation': 'In the corrected code, we group the data by both the method and the student, and calculate the mean score for each combination. We then plot the results separately for each student, which shows the correct relationship between the method and score.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","# Corrected version
**Explanation**

This code is problematic because it ignores the student variable, which is a crucial factor in this analysis. By aggregating the data without considering the student variable, the code is committing the Simpson's Paradox error. The conclusion drawn from the flawed code is incorrect, as the relationship between the method and the score is not accurately represented.

Simpson's Paradox states that aggregated data can show opposite trends from stratified data. In this case, the code is treating the data as a single group, ignoring the fact that the students are divided into two categories (A and B). When we examine the data by student, we may find that the relationship between the method and score is different for each student group. This is the correct approach, as we should always consider the underlying factors that may influence the relationship between variables.

**Corrected Code**","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:32:39.066071,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Treatment': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],
    'Outcome': np.random.binomial(1, 0.6, 10),
    'Group': ['Healthy', 'Healthy', 'Healthy', 'Healthy', 'Healthy', 'Sick', 'Sick', 'Sick', 'Sick', 'Sick']
})

# Analyze aggregated data
agg_outcome = data.groupby('Treatment')['Outcome'].mean()
print(""Aggregated data:"")
print(agg_outcome)

# Plot aggregated data
plt.bar(agg_outcome.index, agg_outcome.values)
plt.xlabel('Treatment')
plt.ylabel('Outcome')
plt.title('Aggregated Data')
plt.show()

# Conclude that treatment A is more effective
print(""Treatment A is more effective!"")","{'summary': ""Code demonstrates Simpson's Paradox in healthcare context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Treatment': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],
    'Outcome': np.random.binomial(1, 0.6, 10),
    'Group': ['Healthy', 'Healthy', 'Healthy', 'Healthy', 'Healthy', 'Sick', 'Sick', 'Sick', 'Sick', 'Sick']
})

# Analyze stratified data
stratified_outcome = data.groupby(['Treatment', 'Group'])['Outcome'].mean().reset_index()
print(""Stratified data:"")
print(stratified_outcome)

# Plot stratified data
plt.figure(figsize=(8, 6))
plt.subplot(1, 2, 1)
plt.bar(stratified_outcome[stratified_outcome['Group'] == 'Healthy']['Treatment'], 
        stratified_outcome[stratified_outcome['Group'] == 'Healthy']['Outcome'])
plt.xlabel('Treatment')
plt.ylabel('Outcome')
plt.title('Healthy Patients')

plt.subplot(1, 2, 2)
plt.bar(stratified_outcome[stratified_outcome['Group'] == 'Sick']['Treatment'], 
        stratified_outcome[stratified_outcome['Group'] == 'Sick']['Outcome'])
plt.xlabel('Treatment')
plt.ylabel('Outcome')
plt.title('Sick Patients')

plt.tight_layout()
plt.show()

# Conclude that treatment A is more effective for sick patients
print(""Treatment A is more effective for sick patients!"")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:33:44.517372,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Synthetic data generation
np.random.seed(0)
patients = pd.DataFrame({
    'Treatment': ['A' if np.random.rand() < 0.5 else 'B' for _ in range(100)],
    'Outcome': np.random.normal(0, 1, 100),
    'Age': np.random.randint(18, 80, 100)
})

# Aggregate analysis
agg_outcome = patients.groupby('Treatment')['Outcome'].mean()
print(""Aggregate trend:"")
print(agg_outcome)

# Plot aggregate trend
plt.bar(agg_outcome.index, agg_outcome.values)
plt.xlabel('Treatment')
plt.ylabel('Average Outcome')
plt.title('Aggregate Trend')
plt.show()

# Flawed conclusion
print(""Conclusion: Treatment A is better than Treatment B."")","{'summary': ""Code demonstrates Simpson's Paradox in healthcare context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Synthetic data generation
np.random.seed(0)
patients = pd.DataFrame({
    'Treatment': ['A' if np.random.rand() < 0.5 else 'B' for _ in range(100)],
    'Outcome': np.random.normal(0, 1, 100),
    'Age': np.random.randint(18, 80, 100)
})

# Stratified analysis
agg_outcome = patients.groupby(['Treatment', 'Age'])['Outcome'].mean().reset_index()
print(""Stratified trend:"")
print(agg_outcome)

# Plot stratified trend
sns.set()
sns.lineplot(x='Age', y='Outcome', hue='Treatment', data=agg_outcome)
plt.xlabel('Age')
plt.ylabel('Outcome')
plt.title('Stratified Trend')
plt.show()

# Corrected conclusion
print(""Conclusion: The trend in outcome varies by age and treatment."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:34:41.110695,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
data = {'Region': ['North', 'North', 'North', 'North', 'South', 'South', 'South', 'South'],
        'Product': ['A', 'A', 'B', 'B', 'A', 'A', 'B', 'B'],
        'Sales': [100, 120, 50, 60, 80, 90, 70, 80]}
df = pd.DataFrame(data)

# Calculate aggregate sales by region
agg_sales_north = df[df['Region'] == 'North']['Sales'].sum()
agg_sales_south = df[df['Region'] == 'South']['Sales'].sum()

print(f""Aggregate sales in North: {agg_sales_north}, Aggregate sales in South: {agg_sales_south}"")

# Plot aggregate sales by region
plt.bar(['North', 'South'], [agg_sales_north, agg_sales_south])
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Aggregate Sales by Region')
plt.show()

# Draw conclusion
print(""Based on the aggregate data, it seems that North has better sales performance than South."")","{'summary': ""Code demonstrates Simpson's Paradox in business context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
data = {'Region': ['North', 'North', 'North', 'North', 'South', 'South', 'South', 'South'],
        'Product': ['A', 'A', 'B', 'B', 'A', 'A', 'B', 'B'],
        'Sales': [100, 120, 50, 60, 80, 90, 70, 80]}
df = pd.DataFrame(data)

# Calculate sales by region and product
df_grouped = df.groupby(['Region', 'Product'])['Sales'].sum().reset_index()
print(df_grouped)

# Plot sales by region and product
plt.figure(figsize=(8, 6))
sns.barplot(x='Region', y='Sales', hue='Product', data=df_grouped)
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Sales by Region and Product')
plt.show()

# Draw conclusion
print(""Based on the stratified data, we can see that Product A has higher sales in the North, and Product B has higher sales in the South."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:35:47.356358,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Method': ['Traditional', 'Traditional', 'Traditional', 'Online', 'Online', 'Online'],
        'Student': ['A', 'B', 'C', 'A', 'B', 'C'],
        'Score': [90, 80, 70, 85, 95, 75]}
df = pd.DataFrame(data)

# Calculate overall score by method
overall_traditional = df.loc[df['Method'] == 'Traditional', 'Score'].mean()
overall_online = df.loc[df['Method'] == 'Online', 'Score'].mean()

print(""Overall, traditional method has a mean score of"", overall_traditional)
print(""Overall, online method has a mean score of"", overall_online)

# Create a plot to visualize the results
plt.bar(['Traditional', 'Online'], [overall_traditional, overall_online])
plt.xlabel('Method')
plt.ylabel('Mean Score')
plt.title('Mean Score by Method')
plt.show()

# Conclusion
print(""Based on the analysis, it appears that the traditional method is more effective."")","{'summary': ""Code demonstrates Simpson's Paradox in education context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Method': ['Traditional', 'Traditional', 'Traditional', 'Online', 'Online', 'Online'],
        'Student': ['A', 'B', 'C', 'A', 'B', 'C'],
        'Score': [90, 80, 70, 85, 95, 75]}
df = pd.DataFrame(data)

# Calculate scores by method and student
scores_by_method = df.groupby(['Method', 'Student'])['Score'].mean().reset_index()
print(scores_by_method)

# Create a plot to visualize the results
plt.figure(figsize=(10, 6))
sns.barplot(x='Student', y='Score', hue='Method', data=scores_by_method)
plt.xlabel('Student')
plt.ylabel('Mean Score')
plt.title('Mean Score by Student and Method')
plt.show()

# Conclusion
print(""The analysis shows that the effectiveness of the methods varies by student."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:36:46.554193,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,education,student performance prediction,python,intermediate,"# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
n_students = 1000
n_teachers = 10
n_methods = 2

df = pd.DataFrame({
    'Teacher': [i % n_teachers for i in range(n_students)],
    'Method': [i % n_methods for i in range(n_students)],
    'Score': [i % 100 for i in range(n_students)] + [i % 100 for i in range(n_students)]
})

# Aggregate data by teacher and method
agg_df = df.groupby(['Teacher', 'Method'])['Score'].mean().reset_index()

# Plot aggregated data
plt.figure(figsize=(8, 6))
sns.barplot(x='Teacher', y='Score', hue='Method', data=agg_df)
plt.title('Aggregated Data')
plt.xlabel('Teacher')
plt.ylabel('Average Score')
plt.show()

# Draw conclusion
print(""Based on the aggregated data, it appears that Teacher 0's method is significantly better than Teacher 1's method."")","{'summary': ""Code demonstrates Simpson's Paradox in education context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of student performance prediction, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in student performance prediction'}","# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
n_students = 1000
n_teachers = 10
n_methods = 2

df = pd.DataFrame({
    'Student': [i for i in range(n_students)],
    'Teacher': [i % n_teachers for i in range(n_students)],
    'Method': [i % n_methods for i in range(n_students)],
    'Score': [i % 100 for i in range(n_students)] + [i % 100 for i in range(n_students)]
})

# Plot individual scores by teacher and method
plt.figure(figsize=(8, 6))
sns.stripplot(x='Teacher', y='Score', hue='Method', data=df)
plt.title('Individual Scores')
plt.xlabel('Teacher')
plt.ylabel('Score')
plt.show()

# Draw conclusion
print(""Based on the individual scores, we see that Teacher 0's method performs similarly to Teacher 1's method, with some variation."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:37:45.410952,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
data = {
    'Group': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],
    'Treatment': ['T', 'T', 'T', 'T', 'T', 'T', 'T', 'T'],
    'Outcome': np.random.normal(0, 1, 8)
}
df = pd.DataFrame(data)

# Calculate aggregate statistics
agg_outcome_A = df[df['Group'] == 'A']['Outcome'].mean()
agg_outcome_B = df[df['Group'] == 'B']['Outcome'].mean()

print(f""Aggregate outcome for Group A: {agg_outcome_A:.2f}"")
print(f""Aggregate outcome for Group B: {agg_outcome_B:.2f}"")

# Plot aggregate trends
plt.bar(['Group A', 'Group B'], [agg_outcome_A, agg_outcome_B])
plt.xlabel('Group')
plt.ylabel('Outcome')
plt.title('Aggregate Trends')
plt.show()

# Conclusion
print(""Based on the aggregate analysis, Group A has a significantly better outcome than Group B."")","{'summary': ""Code demonstrates Simpson's Paradox in healthcare context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
data = {
    'Group': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],
    'Treatment': ['T', 'T', 'T', 'T', 'T', 'T', 'T', 'T'],
    'Outcome': np.random.normal(0, 1, 8)
}
df = pd.DataFrame(data)

# Calculate stratified statistics
agg_outcome_A_treated = df[(df['Group'] == 'A') & (df['Treatment'] == 'T')]['Outcome'].mean()
agg_outcome_A_control = df[(df['Group'] == 'A') & (df['Treatment'] == 'T')]['Outcome'].mean()
agg_outcome_B_treated = df[(df['Group'] == 'B') & (df['Treatment'] == 'T')]['Outcome'].mean()
agg_outcome_B_control = df[(df['Group'] == 'B') & (df['Treatment'] == 'T')]['Outcome'].mean()

print(f""Stratified outcome for Group A (treated): {agg_outcome_A_treated:.2f}"")
print(f""Stratified outcome for Group A (control): {agg_outcome_A_control:.2f}"")
print(f""Stratified outcome for Group B (treated): {agg_outcome_B_treated:.2f}"")
print(f""Stratified outcome for Group B (control): {agg_outcome_B_control:.2f}"")

# Plot stratified trends
plt.bar(['Group A (treated)', 'Group A (control)', 'Group B (treated)', 'Group B (control)'],
        [agg_outcome_A_treated, agg_outcome_A_control, agg_outcome_B_treated, agg_outcome_B_control])
plt.xlabel('Group and Treatment')
plt.ylabel('Outcome')
plt.title('Stratified Trends')
plt.show()

# Conclusion
print(""Based on the stratified analysis, we see that the treatment has a different effect on the outcome depending on the group."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:39:03.617208,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,social_science,income inequality trends,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Synthetic data generation
voters = pd.DataFrame({
    'Candidate': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C'],
    'Region': ['North', 'North', 'North', 'North', 'North', 'North', 'South', 'South', 'South', 'South', 'East', 'East'],
    'Votes': [100, 80, 70, 90, 120, 110, 80, 90, 70, 60, 50, 40]
})

# Aggregated analysis
plt.figure(figsize=(8, 6))
plt.bar(voters['Candidate'].unique(), voters.groupby('Candidate')['Votes'].sum())
plt.xlabel('Candidate')
plt.ylabel('Total Votes')
plt.title('Aggregate Voting Trends')
plt.show()

# Incorrect conclusion
print(""Candidate A received the most votes overall, with a total of"", voters.groupby('Candidate')['Votes'].sum().max(), ""votes."")
print(""Candidate B received the second most votes overall, with a total of"", voters.groupby('Candidate')['Votes'].sum().max(), ""votes."")

# Incorrect reasoning
print(""The data suggests that Candidate A and B are more popular among voters, and Candidate C is less popular."")","{'summary': ""Code demonstrates Simpson's Paradox in social_science context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of income inequality trends, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in income inequality trends'}","import pandas as pd
import matplotlib.pyplot as plt

# Synthetic data generation
voters = pd.DataFrame({
    'Candidate': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C'],
    'Region': ['North', 'North', 'North', 'North', 'North', 'North', 'South', 'South', 'South', 'South', 'East', 'East'],
    'Votes': [100, 80, 70, 90, 120, 110, 80, 90, 70, 60, 50, 40]
})

# Stratified analysis
plt.figure(figsize=(12, 8))
plt.subplot(1, 3, 1)
plt.bar(voters.loc[voters['Region'] == 'North', 'Candidate'].unique(), voters.loc[voters['Region'] == 'North', 'Votes'].sum())
plt.xlabel('Candidate')
plt.ylabel('Votes in North Region')
plt.title('North Region Voting Trends')

plt.subplot(1, 3, 2)
plt.bar(voters.loc[voters['Region'] == 'South', 'Candidate'].unique(), voters.loc[voters['Region'] == 'South', 'Votes'].sum())
plt.xlabel('Candidate')
plt.ylabel('Votes in South Region')
plt.title('South Region Voting Trends')

plt.subplot(1, 3, 3)
plt.bar(voters.loc[voters['Region'] == 'East', 'Candidate'].unique(), voters.loc[voters['Region'] == 'East', 'Votes'].sum())
plt.xlabel('Candidate')
plt.ylabel('Votes in East Region')
plt.title('East Region Voting Trends')
plt.show()

# Corrected conclusion
print(""Candidate C received the most votes in the North region, with a total of"", voters.loc[voters['Region'] == 'North', 'Votes'].sum(), ""votes."")
print(""Candidate A received the most votes in the South and East regions."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:40:24.736381,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    ""Region"": [""North"", ""North"", ""North"", ""South"", ""South"", ""South""],
    ""Product"": [""A"", ""A"", ""B"", ""A"", ""B"", ""B""],
    ""Sales"": [100, 200, 300, 400, 500, 600]
}

df = pd.DataFrame(data)

# Calculate average sales by region
north_avg = df[df[""Region""] == ""North""][""Sales""].mean()
south_avg = df[df[""Region""] == ""South""][""Sales""].mean()

print(""Average sales in North:"", north_avg)
print(""Average sales in South:"", south_avg)

# Plot average sales by region
plt.bar([""North"", ""South""], [north_avg, south_avg])
plt.xlabel(""Region"")
plt.ylabel(""Average Sales"")
plt.title(""Average Sales by Region"")
plt.show()

# Flawed conclusion: North has higher average sales
print(""Based on this analysis, we can conclude that North has higher average sales."")","{'summary': ""Code demonstrates Simpson's Paradox in business context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    ""Region"": [""North"", ""North"", ""North"", ""South"", ""South"", ""South""],
    ""Product"": [""A"", ""A"", ""B"", ""A"", ""B"", ""B""],
    ""Sales"": [100, 200, 300, 400, 500, 600]
}

df = pd.DataFrame(data)

# Calculate average sales by region and product
north_a_avg = df[(df[""Region""] == ""North"") & (df[""Product""] == ""A"")][""Sales""].mean()
north_b_avg = df[(df[""Region""] == ""North"") & (df[""Product""] == ""B"")][""Sales""].mean()
south_a_avg = df[(df[""Region""] == ""South"") & (df[""Product""] == ""A"")][""Sales""].mean()
south_b_avg = df[(df[""Region""] == ""South"") & (df[""Product""] == ""B"")][""Sales""].mean()

print(""Average sales of Product A in North:"", north_a_avg)
print(""Average sales of Product A in South:"", south_a_avg)
print(""Average sales of Product B in North:"", north_b_avg)
print(""Average sales of Product B in South:"", south_b_avg)

# Plot average sales by region and product
plt.bar([""North A"", ""North B"", ""South A"", ""South B""], [north_a_avg, north_b_avg, south_a_avg, south_b_avg])
plt.xlabel(""Region and Product"")
plt.ylabel(""Average Sales"")
plt.title(""Average Sales by Region and Product"")
plt.show()

# Corrected conclusion: North has higher average sales of Product A, while South has higher average sales of Product B
print(""Based on this analysis, we can conclude that North has higher average sales of Product A, while South has higher average sales of Product B."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:41:43.580153,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    'Income Group': ['Low', 'Low', 'Low', 'Low', 'High', 'High', 'High', 'High'],
    'Year': [2010, 2010, 2011, 2011, 2010, 2010, 2011, 2011],
    'Income': [10000, 12000, 12000, 15000, 50000, 60000, 70000, 80000]
}
df = pd.DataFrame(data)

# Calculate mean income by year
mean_income_by_year = df.groupby('Year')['Income'].mean().reset_index()
print(""Mean income by year:"")
print(mean_income_by_year)

# Plot mean income by year
plt.figure(figsize=(8, 6))
plt.plot(mean_income_by_year['Year'], mean_income_by_year['Income'])
plt.xlabel('Year')
plt.ylabel('Mean Income')
plt.title('Mean Income by Year')
plt.show()

# Conclude that income inequality is decreasing over time
print(""Conclusion: Income inequality is decreasing over time."")","{'summary': ""Code demonstrates Simpson's Paradox in social_science context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    'Income Group': ['Low', 'Low', 'Low', 'Low', 'High', 'High', 'High', 'High'],
    'Year': [2010, 2010, 2011, 2011, 2010, 2010, 2011, 2011],
    'Income': [10000, 12000, 12000, 15000, 50000, 60000, 70000, 80000]
}
df = pd.DataFrame(data)

# Calculate mean income by year and income group
mean_income_by_year_income_group = df.groupby(['Year', 'Income Group'])['Income'].mean().reset_index()
print(""Mean income by year and income group:"")
print(mean_income_by_year_income_group)

# Plot mean income by year and income group
plt.figure(figsize=(8, 6))
plt.plot(mean_income_by_year_income_group['Year'], mean_income_by_year_income_group['Income'])
plt.xlabel('Year')
plt.ylabel('Mean Income')
plt.title('Mean Income by Year and Income Group')
plt.show()

# Conclude that income inequality is increasing over time
print(""Conclusion: Income inequality is increasing over time."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:42:46.259343,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,education,teaching method effectiveness comparison,python,intermediate,"# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    'Student': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C'],
    'Gender': ['M', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F'],
    'Grade': [90, 85, 95, 80, 80, 70, 75, 85, 95, 90, 80, 70]
}
df = pd.DataFrame(data)

# Aggregate data by gender
agg_data = df.groupby('Gender')['Grade'].mean()

# Plot aggregated data
plt.bar(agg_data.index, agg_data.values)
plt.xlabel('Gender')
plt.ylabel('Average Grade')
plt.title('Average Grade by Gender')
plt.show()

# Conclusion: Males have higher average grades than females.
print(""Based on the aggregated data, males have higher average grades than females."")","{'summary': ""Code demonstrates Simpson's Paradox in education context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    'Student': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C'],
    'Gender': ['M', 'M', 'M', 'M', 'F', 'F', 'F', 'F', 'M', 'M', 'M', 'F'],
    'Grade': [90, 85, 95, 80, 80, 70, 75, 85, 95, 90, 80, 70]
}
df = pd.DataFrame(data)

# Aggregate data by gender
agg_data = df.groupby('Gender')['Grade'].mean()

# Plot aggregated data
plt.bar(agg_data.index, agg_data.values)
plt.xlabel('Gender')
plt.ylabel('Average Grade')
plt.title('Average Grade by Gender')
plt.show()

# Corrected analysis: Examine data by gender and group
for gender in df['Gender'].unique():
    group_data = df[df['Gender'] == gender]
    print(f""Average grade for {gender} students is {group_data['Grade'].mean()}"")

# Conclusion: The relationship between gender and grade is complex, and cannot be inferred from aggregated data alone.
print(""The relationship between gender and grade is complex, and cannot be inferred from aggregated data alone."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:43:55.763678,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Synthetic data
data = {
    'Treatment': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],
    'Group': ['Group1', 'Group1', 'Group2', 'Group2', 'Group1', 'Group1', 'Group2', 'Group2', 'Group1', 'Group2'],
    'Outcome': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]
}
df = pd.DataFrame(data)

# Aggregate data by treatment and group
agg_data = df.groupby(['Treatment', 'Group'])['Outcome'].mean().reset_index()
print(""Aggregate Data:"")
print(agg_data)

# Plot the aggregated data
plt.figure(figsize=(8, 6))
plt.bar(agg_data['Treatment'], agg_data['Outcome'])
plt.xlabel('Treatment')
plt.ylabel('Mean Outcome')
plt.title('Aggregate Outcome by Treatment')
plt.show()

# Incorrect conclusion
print(""Based on the aggregated data, treatment A seems to have a better outcome."")","{'summary': ""Code demonstrates Simpson's Paradox in healthcare context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import matplotlib.pyplot as plt

# Synthetic data
data = {
    'Treatment': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],
    'Group': ['Group1', 'Group1', 'Group2', 'Group2', 'Group1', 'Group1', 'Group2', 'Group2', 'Group1', 'Group2'],
    'Outcome': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]
}
df = pd.DataFrame(data)

# Stratify the data by group and calculate the mean outcome
stratified_data = df.groupby('Group')['Outcome'].mean().reset_index()
print(""Stratified Data:"")
print(stratified_data)

# Plot the stratified data
plt.figure(figsize=(8, 6))
plt.bar(stratified_data['Group'], stratified_data['Outcome'])
plt.xlabel('Group')
plt.ylabel('Mean Outcome')
plt.title('Stratified Outcome by Group')
plt.show()

# Corrected conclusion
print(""Based on the stratified data, treatment A has a worse outcome in Group1 and a better outcome in Group2."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:45:05.918474,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,education,teaching method effectiveness comparison,python,intermediate,"# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_students = 1000
n_teachers = 5
n_subjects = 3

df = pd.DataFrame({
    'Student': np.random.randint(1, n_students+1, n_students),
    'Teacher': np.random.randint(1, n_teachers+1, n_students),
    'Subject': np.random.randint(1, n_subjects+1, n_students),
    'Score': np.random.normal(loc=80, scale=10, size=n_students)
})

# Group by teacher and calculate mean score
teacher_scores = df.groupby('Teacher')['Score'].mean().reset_index()
print(""Mean scores by teacher:"")
print(teacher_scores)

# Group by subject and calculate mean score
subject_scores = df.groupby('Subject')['Score'].mean().reset_index()
print(""\nMean scores by subject:"")
print(subject_scores)

# Conclude that teachers are better than subjects
print(""\nConclusion: Teachers are generally better than subjects in terms of student performance."")","{'summary': ""Code demonstrates Simpson's Paradox in education context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_students = 1000
n_teachers = 5
n_subjects = 3

df = pd.DataFrame({
    'Student': np.random.randint(1, n_students+1, n_students),
    'Teacher': np.random.randint(1, n_teachers+1, n_students),
    'Subject': np.random.randint(1, n_subjects+1, n_students),
    'Score': np.random.normal(loc=80, scale=10, size=n_students)
})

# Group by teacher and subject and calculate mean score
teacher_subject_scores = df.groupby(['Teacher', 'Subject'])['Score'].mean().reset_index()
print(""Mean scores by teacher and subject:"")
print(teacher_subject_scores)

# Conclude that the interaction between teacher and subject matters
print(""\nConclusion: The interaction between teacher and subject is significant in determining student performance."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:46:06.912981,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,business,marketing campaign effectiveness study,python,intermediate,"# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
n = 1000
df = pd.DataFrame({'Customer': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],
                   'Product': ['X', 'Y', 'Z', 'X', 'Y', 'Z', 'X', 'Y', 'Z'],
                   'Price': [10, 20, 30, 15, 25, 35, 12, 22, 32],
                   'Sales': [10, 20, 30, 10, 20, 30, 15, 25, 35]})

# Calculate aggregated sales by customer
agg_sales = df.groupby('Customer')['Sales'].sum()
print(agg_sales)

# Plot aggregated sales
plt.bar(agg_sales.index, agg_sales.values)
plt.xlabel('Customer')
plt.ylabel('Total Sales')
plt.title('Aggregated Sales by Customer')
plt.show()

# Calculate aggregated sales by product
agg_sales_product = df.groupby('Product')['Sales'].sum()
print(agg_sales_product)

# Plot aggregated sales by product
plt.bar(agg_sales_product.index, agg_sales_product.values)
plt.xlabel('Product')
plt.ylabel('Total Sales')
plt.title('Aggregated Sales by Product')
plt.show()

# Conclusion: Customers A and B are more profitable than C.
print(""Based on the aggregated data, it seems that Customers A and B are more profitable than Customer C."")","{'summary': ""Code demonstrates Simpson's Paradox in business context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
n = 1000
df = pd.DataFrame({'Customer': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],
                   'Product': ['X', 'Y', 'Z', 'X', 'Y', 'Z', 'X', 'Y', 'Z'],
                   'Price': [10, 20, 30, 15, 25, 35, 12, 22, 32],
                   'Sales': [10, 20, 30, 10, 20, 30, 15, 25, 35]})

# Calculate sales by customer and product
agg_sales = df.groupby(['Customer', 'Product'])['Sales'].sum().reset_index()
print(agg_sales)

# Plot sales by customer and product
plt.figure(figsize=(10, 6))
sns.barplot(x='Product', y='Sales', hue='Customer', data=agg_sales)
plt.xlabel('Product')
plt.ylabel('Sales')
plt.title('Sales by Product and Customer')
plt.show()

# Conclusion: Customer C is more profitable than Customers A and B.
print(""Based on the stratified data, it seems that Customer C is more profitable than Customers A and B."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:47:15.829302,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,social_science,income inequality trends,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    'Social Media Usage': ['Heavy', 'Light', 'Heavy', 'Light', 'Heavy', 'Light', 'Heavy', 'Light', 'Heavy', 'Light'],
    'Mental Health': ['Good', 'Good', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Good', 'Good'],
    'Score': [10, 10, 8, 8, 12, 12, 6, 6, 14, 14]
}
df = pd.DataFrame(data)

# Group by social media usage and calculate average score
heavy_avg = df.loc[df['Social Media Usage'] == 'Heavy']['Score'].mean()
light_avg = df.loc[df['Social Media Usage'] == 'Light']['Score'].mean()

print(""Heavy social media users have a higher average score:"", heavy_avg)
print(""Light social media users have a higher average score:"", light_avg)

# Plot the data
plt.scatter(df['Social Media Usage'], df['Score'])
plt.xlabel('Social Media Usage')
plt.ylabel('Score')
plt.title('Relationship between Social Media Usage and Mental Health')
plt.show()","{'summary': ""Code demonstrates Simpson's Paradox in social_science context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of income inequality trends, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in income inequality trends'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    'Social Media Usage': ['Heavy', 'Light', 'Heavy', 'Light', 'Heavy', 'Light', 'Heavy', 'Light', 'Heavy', 'Light'],
    'Mental Health': ['Good', 'Good', 'Bad', 'Bad', 'Good', 'Good', 'Bad', 'Bad', 'Good', 'Good'],
    'Score': [10, 10, 8, 8, 12, 12, 6, 6, 14, 14]
}
df = pd.DataFrame(data)

# Calculate average score for each group
df_grouped = df.groupby(['Social Media Usage', 'Mental Health'])['Score'].mean().reset_index()
print(df_grouped)

# Plot the data
plt.figure(figsize=(8, 6))
sns.barplot(x='Social Media Usage', y='Score', hue='Mental Health', data=df_grouped)
plt.xlabel('Social Media Usage')
plt.ylabel('Score')
plt.title('Relationship between Social Media Usage and Mental Health')
plt.show()","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:48:23.733758,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_students = 1000
n_variables = 3
data = pd.DataFrame(np.random.randint(0, 100, size=(n_students, n_variables)), columns=['Online', 'In-Person', 'Score'])

# Calculate aggregate trends
online_avg = data.groupby('Online')['Score'].mean()
in_person_avg = data.groupby('In-Person')['Score'].mean()
print(""Aggregate trend: Online learning outperforms in-person learning."")
print(online_avg, in_person_avg)

# Create a plot to visualize the results
plt.bar(online_avg.index, online_avg.values)
plt.bar(in_person_avg.index, in_person_avg.values)
plt.xlabel('Learning Method')
plt.ylabel('Average Score')
plt.title('Online vs In-Person Learning Outcomes')
plt.show()

# Conclusion: Online learning is better.
print(""Based on the aggregate data, it appears that online learning is more effective."")","{'summary': ""Code demonstrates Simpson's Paradox in education context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_students = 1000
n_variables = 3
data = pd.DataFrame(np.random.randint(0, 100, size=(n_students, n_variables)), columns=['Online', 'In-Person', 'Score'])

# Calculate stratified trends
online_low_score = data[(data['Online'] == 1) & (data['Score'] < 50)]
online_high_score = data[(data['Online'] == 1) & (data['Score'] >= 50)]
in_person_low_score = data[(data['In-Person'] == 1) & (data['Score'] < 50)]
in_person_high_score = data[(data['In-Person'] == 1) & (data['Score'] >= 50)]

online_low_avg = online_low_score['Score'].mean()
online_high_avg = online_high_score['Score'].mean()
in_person_low_avg = in_person_low_score['Score'].mean()
in_person_high_avg = in_person_high_score['Score'].mean()

print(""Stratified trend: Online learning outperforms in-person learning for high-scoring students."")
print(""Low-scoring students perform better with in-person learning."")

# Create a plot to visualize the results
plt.bar(['Online Low', 'Online High', 'In-Person Low', 'In-Person High'], [online_low_avg, online_high_avg, in_person_low_avg, in_person_high_avg])
plt.xlabel('Student Group')
plt.ylabel('Average Score')
plt.title('Online vs In-Person Learning Outcomes by Student Group')
plt.show()

# Conclusion: The correct analysis reveals that online learning is better for high-scoring students, while in-person learning is better for low-scoring students.
print(""The stratified analysis shows that online learning is more effective for high-scoring students, while in-person learning is more effective for low-scoring students."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:49:36.295003,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
data = {
    'Treatment': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C'],
    'Patient_ID': ['P1', 'P1', 'P2', 'P2', 'P3', 'P3', 'P4', 'P4', 'P5', 'P5', 'P6', 'P6'],
    'Response': np.random.randint(0, 2, 12),
    'Age': np.random.randint(20, 70, 12)
}
df = pd.DataFrame(data)

# Aggregate response rates by treatment
agg_response_rate = df.groupby('Treatment')['Response'].mean()
print(""Aggregate Response Rates:"")
print(agg_response_rate)

# Visualize aggregated response rates
plt.bar(agg_response_rate.index, agg_response_rate.values)
plt.xlabel('Treatment')
plt.ylabel('Response Rate')
plt.title('Aggregate Response Rates')
plt.show()

# Draw a flawed conclusion
print(""Conclusion: Treatment A is the most effective, followed by Treatment B, and Treatment C is the least effective."")","{'summary': ""Code demonstrates Simpson's Paradox in healthcare context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency

# Generate synthetic data
np.random.seed(0)
data = {
    'Treatment': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C'],
    'Patient_ID': ['P1', 'P1', 'P2', 'P2', 'P3', 'P3', 'P4', 'P4', 'P5', 'P5', 'P6', 'P6'],
    'Response': np.random.randint(0, 2, 12),
    'Age': np.random.randint(20, 70, 12)
}
df = pd.DataFrame(data)

# Calculate response rates by treatment and age
response_rates = pd.crosstab(df['Treatment'], df['Age'], values=df['Response'], margins=True)

# Visualize response rates by treatment and age
plt.figure(figsize=(8, 6))
plt.bar(response_rates.columns, response_rates.loc[0], color='b')
plt.bar(response_rates.columns, response_rates.loc[1], color='g')
plt.bar(response_rates.columns, response_rates.loc[2], color='r')
plt.xlabel('Treatment')
plt.ylabel('Response Rate')
plt.title('Response Rates by Treatment and Age')
plt.legend(['20-39', '40-59', '60-69'])
plt.show()

# Perform chi-squared test to check for interaction
chi2, p_val = chi2_contingency(response_rates)
print(""p-value:"", p_val)

# Draw a corrected conclusion
print(""Conclusion: The effectiveness of treatments varies by age group, and no single treatment can be deemed the most effective overall."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:50:48.472746,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Synthetic data
data = {
    'Region': ['North', 'North', 'North', 'South', 'South', 'South'],
    'Method': ['Email', 'Email', 'Phone', 'Email', 'Phone', 'Phone'],
    'Conversion': [0.2, 0.1, 0.3, 0.1, 0.2, 0.3]
}
df = pd.DataFrame(data)

# Aggregate analysis
agg_df = df.groupby(['Region'])['Conversion'].mean()
print(""Aggregate Conversion Rate:"")
print(agg_df)

# Plot aggregate results
plt.bar(agg_df.index, agg_df.values)
plt.xlabel('Region')
plt.ylabel('Conversion Rate')
plt.title('Aggregate Conversion Rate by Region')
plt.show()

# Incorrect conclusion
print(""Based on the aggregate data, it seems that the North region has a higher conversion rate."")
print(""Therefore, we should allocate more resources to the North region."")","{'summary': ""Code demonstrates Simpson's Paradox in business context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import pandas as pd
import matplotlib.pyplot as plt

# Synthetic data
data = {
    'Region': ['North', 'North', 'North', 'South', 'South', 'South'],
    'Method': ['Email', 'Email', 'Phone', 'Email', 'Phone', 'Phone'],
    'Conversion': [0.2, 0.1, 0.3, 0.1, 0.2, 0.3]
}
df = pd.DataFrame(data)

# Stratified analysis
stratified_df = df.groupby(['Region', 'Method'])['Conversion'].mean()
print(""Stratified Conversion Rate:"")
print(stratified_df)

# Plot stratified results
plt.figure(figsize=(8, 6))
plt.bar(stratified_df.index.get_level_values('Region'), stratified_df.get_level_values('Method'), stratified_df.values)
plt.xlabel('Region')
plt.ylabel('Method')
plt.title('Stratified Conversion Rate by Region and Method')
plt.show()

# Corrected conclusion
print(""After examining the stratified data, we see that the Phone method has a higher conversion rate in the South region."")
print(""Therefore, we should consider allocating more resources to the Phone method in the South region."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:51:50.073386,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,social_science,income inequality trends,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
n = 1000
voters = pd.DataFrame({'age': [25 if i % 2 == 0 else 50 for i in range(n)],
                       'party': ['Democrat' if i % 3 == 0 else 'Republican' for i in range(n)],
                       'vote': [1 if i % 4 == 0 else 0 for i in range(n)]})

# Group data by party and calculate vote percentage
party_votes = voters.groupby('party')['vote'].mean()
print(""Party-wise vote percentage:"")
print(party_votes)

# Plot party-wise vote percentage
plt.bar(party_votes.index, party_votes.values)
plt.xlabel('Party')
plt.ylabel('Vote Percentage')
plt.title('Party-wise Vote Percentage')
plt.show()

# Conclusion: Republicans have a higher vote percentage than Democrats.
print(""Conclusion: Republicans have a higher vote percentage than Democrats."")","{'summary': ""Code demonstrates Simpson's Paradox in social_science context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of income inequality trends, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in income inequality trends'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
n = 1000
voters = pd.DataFrame({'age': [25 if i % 2 == 0 else 50 for i in range(n)],
                       'party': ['Democrat' if i % 3 == 0 else 'Republican' for i in range(n)],
                       'vote': [1 if i % 4 == 0 else 0 for i in range(n)]})

# Group data by party and age, and calculate vote percentage
party_age_votes = voters.groupby(['party', 'age'])['vote'].mean().reset_index()
print(""Party-wise vote percentage by age:"")
print(party_age_votes)

# Plot party-wise vote percentage by age
plt.figure(figsize=(10, 6))
plt.bar(party_age_votes[party_age_votes['party'] == 'Democrat']['age'],
        party_age_votes[party_age_votes['party'] == 'Democrat']['vote'], label='Democrat')
plt.bar(party_age_votes[party_age_votes['party'] == 'Republican']['age'],
        party_age_votes[party_age_votes['party'] == 'Republican']['vote'], label='Republican')
plt.xlabel('Age')
plt.ylabel('Vote Percentage')
plt.title('Party-wise Vote Percentage by Age')
plt.legend()
plt.show()

# Conclusion: Democrats have a higher vote percentage among younger voters, while Republicans have a higher vote percentage among older voters.
print(""Conclusion: Democrats have a higher vote percentage among younger voters, while Republicans have a higher vote percentage among older voters."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:52:57.444027,llama-3-8b-instruct,v1,False
simpsons_paradox,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Patient': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C'],
        'Treatment': ['Drug A', 'Drug A', 'Drug A', 'Drug B', 'Drug A', 'Drug A', 'Drug B', 'Drug B', 'Drug A', 'Drug A', 'Drug B', 'Drug B'],
        'Outcome': [1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0]}

df = pd.DataFrame(data)

# Aggregate data by treatment and calculate mean outcome
agg_df = df.groupby('Treatment')['Outcome'].mean().reset_index()
print(agg_df)

# Plot aggregated data
plt.bar(agg_df['Treatment'], agg_df['Outcome'])
plt.xlabel('Treatment')
plt.ylabel('Mean Outcome')
plt.title('Aggregate Data')
plt.show()

# Conclusion: Drug A is more effective than Drug B
print(""Based on the aggregated data, we can conclude that Drug A is more effective than Drug B."")","{'summary': ""Code demonstrates Simpson's Paradox in healthcare context"", 'detailed_explanation': ""This code demonstrates Simpson's Paradox. Aggregated data can show opposite trends from stratified data In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making."", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Patient': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C'],
        'Treatment': ['Drug A', 'Drug A', 'Drug A', 'Drug B', 'Drug A', 'Drug A', 'Drug B', 'Drug B', 'Drug A', 'Drug A', 'Drug B', 'Drug B'],
        'Gender': ['Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Female', 'Female', 'Male', 'Male', 'Male', 'Male'],
        'Outcome': [1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0]}

df = pd.DataFrame(data)

# Calculate mean outcome for each treatment and gender
agg_df = df.groupby(['Treatment', 'Gender'])['Outcome'].mean().reset_index()
print(agg_df)

# Plot disaggregated data
plt.figure(figsize=(8, 6))
plt.subplot(1, 2, 1)
plt.bar(agg_df[agg_df['Gender'] == 'Male']['Treatment'], agg_df[agg_df['Gender'] == 'Male']['Outcome'])
plt.xlabel('Treatment')
plt.ylabel('Mean Outcome')
plt.title('Male Patients')
plt.subplot(1, 2, 2)
plt.bar(agg_df[agg_df['Gender'] == 'Female']['Treatment'], agg_df[agg_df['Gender'] == 'Female']['Outcome'])
plt.xlabel('Treatment')
plt.ylabel('Mean Outcome')
plt.title('Female Patients')
plt.show()

# Conclusion: The trend is reversed when considering gender
print(""The trend in the data is reversed when considering gender. Drug A is more effective for male patients, but less effective for female patients."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Aggregated data can show opposite trends from stratified data']",Aggregated data can show opposite trends from stratified data,2025-12-14T19:54:14.124823,llama-3-8b-instruct,v1,False
survivorship_bias,critical,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data (100 patients, 50% survived, 50% died)
np.random.seed(0)
survival_data = pd.DataFrame({'Patient_ID': np.arange(1, 101), 
                             'Survived': np.random.binomial(1, 0.5, 100)})

# Select only surviving patients
surviving_patients = survival_data[survival_data['Survived'] == 1]

# Calculate mean age of surviving patients
mean_age_surviving = surviving_patients['Patient_ID'].mean()

print(""Mean age of surviving patients:"", mean_age_surviving)

# Conclusion: Since the mean age of surviving patients is higher than expected, 
# we conclude that older patients are more likely to survive.
print(""Older patients are more likely to survive."")","{'summary': 'Code demonstrates Survivorship Bias in healthcare context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import pandas as pd
import numpy as np

# Generate synthetic data (100 patients, 50% survived, 50% died)
np.random.seed(0)
survival_data = pd.DataFrame({'Patient_ID': np.arange(1, 101), 
                             'Survived': np.random.binomial(1, 0.5, 100)})

# Calculate mean age of all patients (including both survivors and non-survivors)
mean_age_all = survival_data['Patient_ID'].mean()

# Calculate mean age of surviving patients
mean_age_surviving = survival_data[survival_data['Survived'] == 1]['Patient_ID'].mean()

print(""Mean age of all patients:"", mean_age_all)
print(""Mean age of surviving patients:"", mean_age_surviving)

# Conclusion: The mean age of all patients is the correct representation of the population, 
# while the mean age of surviving patients is only a biased representation of the population.
print(""The relationship between age and survival is complex and cannot be inferred from this analysis."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T19:55:05.423459,llama-3-8b-instruct,v1,False
survivorship_bias,critical,social_science,income inequality trends,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(42)
n_cases = 1000
crime_rate = np.random.uniform(0, 1, n_cases)
success = np.random.binomial(1, 0.5, n_cases)

# Create a dataframe
df = pd.DataFrame({'crime_rate': crime_rate,'success': success})

# Analyze only successful cases
success_df = df[df['success'] == 1]
print(""Mean crime rate for successful cases:"", success_df['crime_rate'].mean())

# Conclusion
print(""Based on our analysis, it appears that crime rates are significantly higher among successful cases."")
print(""This suggests that crime prevention programs should focus on those who have already been successful."")","{'summary': 'Code demonstrates Survivorship Bias in social_science context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of income inequality trends, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in income inequality trends'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(42)
n_cases = 1000
crime_rate = np.random.uniform(0, 1, n_cases)
success = np.random.binomial(1, 0.5, n_cases)

# Create a dataframe
df = pd.DataFrame({'crime_rate': crime_rate,'success': success})

# Analyze all cases (both successful and failed)
print(""Mean crime rate for all cases:"", df['crime_rate'].mean())

# Calculate the proportion of successful cases
proportion_success = df['success'].mean()
print(""Proportion of successful cases:"", proportion_success)

# Conclusion
print(""Our analysis suggests that the average crime rate is:"")
print(""Mean crime rate:"", df['crime_rate'].mean())
print(""However, we should be cautious when interpreting the results, as the analysis is based on all cases, including both successful and failed cases."")
print(""Future research should consider controlling for potential confounding variables and using more robust statistical methods to account for selection bias."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T19:55:58.041977,llama-3-8b-instruct,v1,False
survivorship_bias,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
crime_data = pd.DataFrame({'Crime_Rate': np.random.normal(0, 1, 100),
                           'Intervention': np.random.binomial(1, 0.5, 100),
                           'Outcome': np.where(crime_data['Crime_Rate'] > 0.5, 1, 0)})

# Analyze only successful cases (Outcomes == 1)
success_cases = crime_data[crime_data['Outcome'] == 1]
print(""Success rate with intervention:"", sum(success_cases['Intervention']) / len(success_cases))

# Conclusion
print(""Therefore, the intervention is effective in reducing crime rates."")","{'summary': 'Code demonstrates Survivorship Bias in social_science context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
crime_data = pd.DataFrame({'Crime_Rate': np.random.normal(0, 1, 100),
                           'Intervention': np.random.binomial(1, 0.5, 100),
                           'Outcome': np.where(crime_data['Crime_Rate'] > 0.5, 1, 0)})

# Analyze both successful and unsuccessful cases
total_cases = crime_data.shape[0]
success_cases = crime_data[crime_data['Outcome'] == 1]
failure_cases = crime_data[crime_data['Outcome'] == 0]

# Calculate the overall effectiveness of the intervention
effectiveness = (sum(success_cases['Intervention']) / sum(success_cases['Outcome'])) - (sum(failure_cases['Intervention']) / sum(failure_cases['Outcome']))

print(""Overall effectiveness of the intervention:"", effectiveness)

# Conclusion
print(""Therefore, the true effectiveness of the intervention can be estimated by considering both successful and unsuccessful cases."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T19:56:49.444587,llama-3-8b-instruct,v1,False
survivorship_bias,critical,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
num_students = 1000
num_online_students = int(np.random.uniform(0.8, 0.9) * num_students)
num_inperson_students = num_students - num_online_students

online_outcomes = np.random.binomial(1, 0.7, num_online_students)
inperson_outcomes = np.random.binomial(1, 0.6, num_inperson_students)

# Create a DataFrame
df = pd.DataFrame({'online': online_outcomes, 'inperson': inperson_outcomes})

# Calculate the success rate for each group
online_success_rate = np.mean(online_outcomes) * 100
inperson_success_rate = np.mean(inperson_outcomes) * 100

print(""Online success rate:"", online_success_rate)
print(""In-person success rate:"", inperson_success_rate)

# Conclusion: Online learning is more effective!
print(""Based on these results, we can conclude that online learning has a higher success rate than in-person learning."")","{'summary': 'Code demonstrates Survivorship Bias in education context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
num_students = 1000
num_online_students = int(np.random.uniform(0.8, 0.9) * num_students)
num_inperson_students = num_students - num_online_students

online_outcomes = np.random.binomial(1, 0.7, num_online_students)
inperson_outcomes = np.random.binomial(1, 0.6, num_inperson_students)

# Create a DataFrame
df = pd.DataFrame({'online': online_outcomes, 'inperson': inperson_outcomes})

# Calculate the success rate for each group
online_success_rate = np.mean(online_outcomes) * 100
inperson_success_rate = np.mean(inperson_outcomes) * 100

# Calculate the failure rate for each group
online_failure_rate = 1 - np.mean(online_outcomes) * 100
inperson_failure_rate = 1 - np.mean(inperson_outcomes) * 100

print(""Online success rate:"", online_success_rate)
print(""Online failure rate:"", online_failure_rate)
print(""In-person success rate:"", inperson_success_rate)
print(""In-person failure rate:"", inperson_failure_rate)

# Conclusion: We can't conclude which method is more effective without considering both success and failure rates.
print(""To draw meaningful conclusions, we need to consider both the success and failure rates for each learning method."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T19:57:51.010400,llama-3-8b-instruct,v1,False
survivorship_bias,critical,education,student performance prediction,python,intermediate,"# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_students = 100
n_classes = 5
success_rate = 0.2

students = pd.DataFrame({'Student ID': np.arange(1, n_students+1), 
                        'Method': np.random.choice(['Method A', 'Method B', 'Method C', 'Method D', 'Method E'], size=n_students),
                        'Outcome': np.random.choice(['Pass', 'Fail'], size=n_students, p=[success_rate, 1-success_rate])})

# Analyze only successful cases (ignoring failures)
successful_students = students[students['Outcome'] == 'Pass']
method_a_success_rate = successful_students[successful_students['Method'] == 'Method A']['Outcome'].value_counts(normalize=True)[1]

print(f""Method A has a success rate of {method_a_success_rate:.2f} among successful students."")","{'summary': 'Code demonstrates Survivorship Bias in education context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of student performance prediction, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in student performance prediction'}","# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_students = 100
n_classes = 5
success_rate = 0.2

students = pd.DataFrame({'Student ID': np.arange(1, n_students+1), 
                        'Method': np.random.choice(['Method A', 'Method B', 'Method C', 'Method D', 'Method E'], size=n_students),
                        'Outcome': np.random.choice(['Pass', 'Fail'], size=n_students, p=[success_rate, 1-success_rate])})

# Analyze all cases (including failures)
method_a_outcomes = students[students['Method'] == 'Method A']['Outcome'].value_counts(normalize=True)

print(f""Method A has a success rate of {method_a_outcomes['Pass']:.2f} and a failure rate of {method_a_outcomes['Fail']:.2f}."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T19:58:46.982211,llama-3-8b-instruct,v1,False
survivorship_bias,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Synthetic data generation
import numpy as np
np.random.seed(0)
num_patients = 100
success_rate = 0.2
treatment_effectiveness = np.random.binomial(num_patients, success_rate)

# Create a dataframe
df = pd.DataFrame({'treatment': [1] * num_patients, 'outcome': treatment_effectiveness})

# Analyze only successful cases
successful_cases = df[df['outcome'] == 1]
print(""Success rate:"", len(successful_cases) / num_patients)

# Plot a bar chart to visualize the results
plt.bar(range(1, 11), [len(successful_cases) / num_patients] * 10)
plt.xlabel('Treatment Group')
plt.ylabel('Success Rate')
plt.title('Treatment Effectiveness')
plt.show()

# Flawed conclusion
print(""The treatment is extremely effective, with a success rate of"", len(successful_cases) / num_patients * 100, ""%."")","{'summary': 'Code demonstrates Survivorship Bias in healthcare context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import matplotlib.pyplot as plt

# Synthetic data generation
import numpy as np
np.random.seed(0)
num_patients = 100
success_rate = 0.2
treatment_effectiveness = np.random.binomial(num_patients, success_rate)

# Create a dataframe
df = pd.DataFrame({'treatment': [1] * num_patients, 'outcome': treatment_effectiveness})

# Analyze all cases (both successful and failed)
all_cases = df
print(""Success rate:"", len(df[df['outcome'] == 1]) / num_patients)

# Plot a bar chart to visualize the results
plt.bar(range(1, 11), [len(df[df['outcome'] == 1]) / num_patients] * 10)
plt.xlabel('Treatment Group')
plt.ylabel('Success Rate')
plt.title('Treatment Effectiveness')
plt.show()

# Corrected conclusion
print(""The treatment has a moderate success rate of"", len(df[df['outcome'] == 1]) / num_patients * 100, ""%."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T19:59:41.771029,llama-3-8b-instruct,v1,False
survivorship_bias,critical,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_patients = 100
n_treatments = 2
success_rate = 0.8

df = pd.DataFrame({'Patient ID': np.arange(1, n_patients+1),
                  'Treatment': np.random.choice(['A', 'B'], n_patients),
                  'Outcome': np.random.binomial(n=1, p=success_rate, size=n_patients)})

# Analyze only successful cases (ignoring failures)
success_cases = df[df['Outcome'] == 1]
print(""Successful patients:"", len(success_cases))

# Calculate the proportion of successful patients in treatment A
success_A = sum(success_cases['Treatment'] == 'A') / len(success_cases)
print(""Proportion of successful patients in treatment A:"", success_A)

# Flawed conclusion
print(""Based on the analysis, treatment A is significantly more effective."")","{'summary': 'Code demonstrates Survivorship Bias in healthcare context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_patients = 100
n_treatments = 2
success_rate = 0.8

df = pd.DataFrame({'Patient ID': np.arange(1, n_patients+1),
                  'Treatment': np.random.choice(['A', 'B'], n_patients),
                  'Outcome': np.random.binomial(n=1, p=success_rate, size=n_patients)})

# Analyze all cases (including failures)
all_cases = df

# Calculate the proportion of successful patients in treatment A
success_A = sum(df['Treatment'] == 'A') / len(df)
print(""Proportion of successful patients in treatment A:"", success_A)

# Calculate the proportion of failed patients in treatment A
failed_A = sum(df['Treatment'] == 'A') - sum(df['Outcome'] == 1)
print(""Proportion of failed patients in treatment A:"", failed_A)

# Corrected conclusion
print(""Based on the analysis, treatment A has a proportion of successful patients:"", success_A)
print(""However, we also observe a proportion of failed patients:"", failed_A)","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T20:00:41.531173,llama-3-8b-instruct,v1,False
survivorship_bias,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data on income inequality
data = pd.DataFrame({
    'Income': [10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000],
    'Outcome': ['Success', 'Success', 'Success', 'Success', 'Failure', 'Failure', 'Failure', 'Failure', 'Failure']
})

# Analyze only successful cases (ignoring failures)
successful_cases = data[data['Outcome'] == 'Success']
successful_income_avg = successful_cases['Income'].mean()
print(f""Average income of successful cases: {successful_income_avg:.2f}"")

# Visualize the flawed analysis
plt.bar(range(len(successful_cases)), successful_cases['Income'])
plt.xlabel('Index')
plt.ylabel('Income')
plt.title('Average Income of Successful Cases')
plt.show()

# Flawed conclusion
print(""Based on this analysis, it appears that successful individuals have higher incomes."")","{'summary': 'Code demonstrates Survivorship Bias in social_science context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data on income inequality
data = pd.DataFrame({
    'Income': [10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 10000, 20000, 30000, 40000, 50000, 60000],
    'Outcome': ['Success', 'Success', 'Success', 'Success', 'Failure', 'Failure', 'Failure', 'Failure', 'Failure', 'Failure', 'Failure', 'Failure', 'Failure', 'Failure', 'Failure', 'Failure']
})

# Analyze all cases (including failures)
all_cases = data
all_income_avg = all_cases['Income'].mean()
print(f""Average income of all cases: {all_income_avg:.2f}"")

# Visualize the corrected analysis
plt.bar(range(len(all_cases)), all_cases['Income'])
plt.xlabel('Index')
plt.ylabel('Income')
plt.title('Average Income of All Cases')
plt.show()

# Corrected conclusion
print(""Based on this analysis, it appears that the average income of all cases is..."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T20:01:44.311741,llama-3-8b-instruct,v1,False
survivorship_bias,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Synthetic data: voting behavior of 100 individuals
data = pd.DataFrame({'Voted': [1] * 50 + [0] * 50, 'Election Winner': [1] * 40 + [0] * 10, 'Demographic': ['Urban' if i % 2 == 0 else 'Rural' for i in range(100)]})

# Analyze only successful voters (voted for the winner)
successful_voters = data[data['Voted'] == 1]

# Plot the demographics of successful voters
plt.bar(successful_voters['Demographic'].value_counts())
plt.xlabel('Demographic')
plt.ylabel('Count')
plt.title('Voting Patterns of Successful Voters')
plt.show()

# Conclusion: Urban dwellers are more likely to vote for the winner
print(""The data suggests that urban dwellers are more likely to vote for the winner."")","{'summary': 'Code demonstrates Survivorship Bias in social_science context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import matplotlib.pyplot as plt

# Synthetic data: voting behavior of 100 individuals
data = pd.DataFrame({'Voted': [1] * 50 + [0] * 50, 'Election Winner': [1] * 40 + [0] * 10, 'Demographic': ['Urban' if i % 2 == 0 else 'Rural' for i in range(100)]})

# Analyze the entire dataset, including both successes and failures
all_voters = data

# Plot the demographics of all voters
plt.bar(all_voters.groupby('Demographic')['Voted'].sum() / len(all_voters))
plt.xlabel('Demographic')
plt.ylabel('Proportion of Voters')
plt.title('Voting Patterns of All Voters')
plt.show()

# Conclusion: The proportion of voters is similar across demographics
print(""The data suggests that the proportion of voters is similar across urban and rural demographics."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T20:02:38.199096,llama-3-8b-instruct,v1,False
survivorship_bias,critical,business,sales performance across regions,python,intermediate,"# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
data = {
    'Price': np.random.normal(100, 20, 1000),
    'Sales': np.random.binomial(100, 0.5, 1000)
}
df = pd.DataFrame(data)

# Filter only successful cases (survivors)
survivors = df[df['Sales'] > 50]

# Analyze the survivors
survivors_mean_price = survivors['Price'].mean()
print(""Mean price of successful products:"", survivors_mean_price)

# Conclusion
print(""Our analysis shows that products with higher prices have a higher chance of success."")
print(""Therefore, we recommend increasing prices to maximize sales."")","{'summary': 'Code demonstrates Survivorship Bias in business context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
data = {
    'Price': np.random.normal(100, 20, 1000),
    'Sales': np.random.binomial(100, 0.5, 1000)
}
df = pd.DataFrame(data)

# Analyze the entire dataset
mean_price = df['Price'].mean()
print(""Mean price of all products:"", mean_price)

# Compare the performance of different price groups
price_groups = pd.cut(df['Price'], bins=[0, 50, 100, 150, 200], labels=['Low', 'Medium', 'High'])
perf = df.groupby(price_groups)['Sales'].mean().reset_index()
print(perf)

# Conclusion
print(""Our analysis shows that the performance of products varies across different price groups."")
print(""We recommend further investigation to understand the relationships between price and sales."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T20:03:32.854842,llama-3-8b-instruct,v1,False
survivorship_bias,critical,social_science,voting behavior analysis,python,intermediate,"# Synthetic data generation
import numpy as np
np.random.seed(0)
n_users = 100
n_days = 30
user_data = np.random.binomial(1, 0.5, size=(n_users, n_days))

# Analyze only users who survived (i.e., didn't quit social media)
surviving_users = np.sum(user_data, axis=1) > 0
surviving_user_data = user_data[surviving_users, :]

# Calculate average daily usage for surviving users
avg_daily_usage_surviving = np.mean(surviving_user_data, axis=0)

print(""Average daily usage for surviving users:"", avg_daily_usage_surviving)

# Conclusion
print(""Social media usage is positively correlated with mental health. Surviving users (those who didn't quit) tend to have higher daily usage."")","{'summary': 'Code demonstrates Survivorship Bias in social_science context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","# Synthetic data generation
import numpy as np
np.random.seed(0)
n_users = 100
n_days = 30
user_data = np.random.binomial(1, 0.5, size=(n_users, n_days))

# Analyze all users, including those who quit social media
avg_daily_usage_all = np.mean(user_data, axis=0)

# Calculate average daily usage for all users, including quitters
print(""Average daily usage for all users:"", avg_daily_usage_all)

# Calculate proportion of users who quit social media
proportion_quit = np.sum(np.sum(user_data, axis=1) == 0) / n_users

print(""Proportion of users who quit social media:"", proportion_quit)

# Conclusion
print(""Social media usage is not necessarily correlated with mental health. The proportion of users who quit social media is significant, suggesting that negative experiences may be a concern."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T20:04:35.417604,llama-3-8b-instruct,v1,False
survivorship_bias,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"# Synthetic data generation
import pandas as pd
import numpy as np

# Simulate patient outcomes (binary: 0 = failure, 1 = success)
np.random.seed(0)
n_patients = 100
outcomes = np.random.binomial(n=1, p=0.5, size=n_patients)

# Create a DataFrame
df = pd.DataFrame({'Outcome': outcomes})

# Analyze only successful cases (ignoring failures)
successes = df[df['Outcome'] == 1]
print(""Success rate:"", successes.shape[0] / n_patients)

# Incorrect reasoning
print(""Conclusion: The success rate is extremely high because our treatment is highly effective!"")","{'summary': 'Code demonstrates Survivorship Bias in healthcare context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","# Synthetic data generation
import pandas as pd
import numpy as np

# Simulate patient outcomes (binary: 0 = failure, 1 = success)
np.random.seed(0)
n_patients = 100
outcomes = np.random.binomial(n=1, p=0.5, size=n_patients)

# Create a DataFrame
df = pd.DataFrame({'Outcome': outcomes})

# Analyze all cases (both successes and failures)
total_patients = df.shape[0]
successes = df[df['Outcome'] == 1].shape[0]
failure_rate = (total_patients - successes) / total_patients
print(""Failure rate:"", failure_rate)

# Corrected reasoning
print(""Conclusion: The failure rate is relatively high, indicating that the treatment may not be effective for a significant portion of patients."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T20:05:20.663285,llama-3-8b-instruct,v1,False
survivorship_bias,critical,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
students = pd.DataFrame({
    'Student ID': np.arange(100),
    'Grade': np.random.randint(0, 101, 100)  # Assuming 0-100 scale
})

# Filter only successful students (Grade >= 60)
successful_students = students[students['Grade'] >= 60]

# Calculate mean grade for successful students
mean_grade = successful_students['Grade'].mean()
print(f""Mean grade for successful students: {mean_grade:.2f}"")

# Conclusion: Our analysis shows that students with a mean grade of {mean_grade:.2f} are more likely to succeed.
# Therefore, we can conclude that a high grade is a strong predictor of success.","{'summary': 'Code demonstrates Survivorship Bias in education context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
students = pd.DataFrame({
    'Student ID': np.arange(100),
    'Grade': np.random.randint(0, 101, 100)  # Assuming 0-100 scale
})

# Calculate mean grade for all students
mean_grade_all = students['Grade'].mean()
print(f""Mean grade for all students: {mean_grade_all:.2f}"")

# Calculate mean grade for successful students (Grade >= 60)
mean_grade_successful = students[students['Grade'] >= 60]['Grade'].mean()
print(f""Mean grade for successful students: {mean_grade_successful:.2f}"")

# Calculate mean grade for failed students (Grade < 60)
mean_grade_failed = students[students['Grade'] < 60]['Grade'].mean()
print(f""Mean grade for failed students: {mean_grade_failed:.2f}"")

# Conclusion: Our analysis shows that the mean grade for all students is {mean_grade_all:.2f}.
# While successful students have a mean grade of {mean_grade_successful:.2f}, failed students have a mean grade of {mean_grade_failed:.2f}.
# This suggests that there may be a more nuanced relationship between grade and success, and that we should consider the experiences of all students, not just the successful ones.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T20:06:22.486097,llama-3-8b-instruct,v1,False
survivorship_bias,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_regions = 5
n_sales = 100
data = pd.DataFrame({'Region': np.random.choice(['North', 'South', 'East', 'West', 'Central'], n_sales),
                     'Sales': np.random.randint(0, 100, n_sales)})

# Filter out unsuccessful sales
successful_sales = data[data['Sales'] > 50]

# Analyze only successful sales
mean_sales_by_region = successful_sales.groupby('Region')['Sales'].mean()
print(""Mean sales by region for successful sales:"", mean_sales_by_region)

# Flawed conclusion
print(""Regions with the highest mean sales are: "", mean_sales_by_region.index[mean_sales_by_region.values.argsort()[-3:][::-1])","{'summary': 'Code demonstrates Survivorship Bias in business context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_regions = 5
n_sales = 100
data = pd.DataFrame({'Region': np.random.choice(['North', 'South', 'East', 'West', 'Central'], n_sales),
                     'Sales': np.random.randint(0, 100, n_sales)})

# Analyze all sales
mean_sales_by_region = data.groupby('Region')['Sales'].mean()
print(""Mean sales by region for all sales:"", mean_sales_by_region)

# Corrected conclusion
print(""Regions with the highest mean sales are: "", mean_sales_by_region.index[mean_sales_by_region.values.argsort()[-3:][::-1])","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T20:07:11.014674,llama-3-8b-instruct,v1,False
survivorship_bias,critical,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Patient_ID': np.arange(1, 101),
    'Age': np.random.randint(18, 80, size=100),
    'Disease_Status': np.where(np.random.rand(100) < 0.2, 'Infected', 'Healthy')  # 20% infected
})

# Analyze only surviving/successful cases (ignoring failures)
infected_survivors = data[data['Disease_Status'] == 'Infected']
infected_survivors['Age_at_Onset'] = np.random.randint(18, 80, size=len(infected_survivors))

# Perform analysis on infected survivors
infected_survivors.groupby('Age').size().plot(kind='bar')

# Conclusion
print(""The analysis suggests that the disease is more prevalent among older patients."")
print(""This is because the majority of infected patients were older than 50."")","{'summary': 'Code demonstrates Survivorship Bias in healthcare context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Patient_ID': np.arange(1, 101),
    'Age': np.random.randint(18, 80, size=100),
    'Disease_Status': np.where(np.random.rand(100) < 0.2, 'Infected', 'Healthy')  # 20% infected
})

# Perform analysis on the entire dataset (including failures)
data['Age_at_Onset'] = np.where(data['Disease_Status'] == 'Infected', np.random.randint(18, 80, size=len(data[data['Disease_Status'] == 'Infected'])), np.nan)

# Perform analysis on the entire dataset
data.groupby('Age')['Disease_Status'].value_counts(normalize=True).plot(kind='bar')

# Conclusion
print(""The analysis suggests that the disease is prevalent across all age groups."")
print(""However, the age of onset may be influenced by various factors, including environmental conditions and genetic predisposition."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T20:08:08.008616,llama-3-8b-instruct,v1,False
survivorship_bias,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = {
    'Region': ['North', 'North', 'South', 'South', 'East', 'East'],
    'Sales': [100, 120, 80, 60, 90, 110],
    'Outcome': [1, 1, 0, 0, 1, 1]  # 1: successful, 0: failed
}
df = pd.DataFrame(data)

# Analyze only successful cases (surviving/successful)
successful_cases = df[df['Outcome'] == 1]
mean_sales_successful = successful_cases['Sales'].mean()
print(""Mean sales for successful regions:"", mean_sales_successful)

# Conclusion: Regions with higher sales are North and East.
print(""Conclusion: The North and East regions are the most profitable."")","{'summary': 'Code demonstrates Survivorship Bias in business context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = {
    'Region': ['North', 'North', 'South', 'South', 'East', 'East', 'West', 'West'],
    'Sales': [100, 120, 80, 60, 90, 110, 70, 100],
    'Outcome': [1, 1, 0, 0, 1, 1, 0, 1]  # 1: successful, 0: failed
}
df = pd.DataFrame(data)

# Analyze all cases (both successful and failed)
all_cases = df
mean_sales_all = all_cases['Sales'].mean()
print(""Mean sales for all regions:"", mean_sales_all)

# Analyze successful and failed cases separately
successful_cases = df[df['Outcome'] == 1]
failed_cases = df[df['Outcome'] == 0]

# Compare and contrast the results
print(""Mean sales for successful regions:"", successful_cases['Sales'].mean())
print(""Mean sales for failed regions:"", failed_cases['Sales'].mean())

# Conclusion: The mean sales for all regions is 94.17. The North and East regions have higher sales, but the South and West regions also have valuable insights.
print(""Conclusion: The mean sales for all regions is 94.17. The North and East regions have higher sales, but the South and West regions also have valuable insights."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T20:09:11.230961,llama-3-8b-instruct,v1,False
survivorship_bias,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = {
    'Price': np.random.normal(100, 20, 100),
    'Outcome': np.random.binomial(1, 0.5, 100)  # 50% success rate
}
df = pd.DataFrame(data)

# Select only successful cases (ignoring failures)
success_df = df[df['Outcome'] == 1]

# Analyze the data, ignoring failures
avg_price_success = success_df['Price'].mean()
print(""Average price for successful products:"", avg_price_success)

# Incorrect conclusion
print(""Based on this analysis, we can conclude that increasing the price leads to higher success rates."")","{'summary': 'Code demonstrates Survivorship Bias in business context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = {
    'Price': np.random.normal(100, 20, 100),
    'Outcome': np.random.binomial(1, 0.5, 100)  # 50% success rate
}
df = pd.DataFrame(data)

# Analyze the entire data, including failures
avg_price = df['Price'].mean()
print(""Average price:"", avg_price)

# Analyze the success rate
success_rate = df['Outcome'].mean()
print(""Success rate:"", success_rate)

# Corrected conclusion
print(""Based on this analysis, we can conclude that the relationship between price and success rate is complex, and further investigation is needed to understand the impact of price on outcome."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T20:09:59.182181,llama-3-8b-instruct,v1,False
survivorship_bias,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"# Synthetic data generation (simplified)
import pandas as pd
import numpy as np

np.random.seed(0)  # for reproducibility
n_patients = 1000
demographics = ['age', 'gender', 'race']
data = pd.DataFrame({
    'age': np.random.randint(18, 100, n_patients),
    'gender': np.random.choice(['M', 'F'], n_patients),
    'race': np.random.choice(['White', 'Black', 'Asian'], n_patients),
    'disease': np.random.choice([0, 1], n_patients, p=[0.05, 0.95])  # 5% disease prevalence
})

# Analysis: Only considering patients who survived (disease=0)
surviving_cases = data[data['disease'] == 0]
surviving_cases.groupby('age')['disease'].mean()  # 0.9 (90% of patients in this age group have disease)

print(""Conclusion: Patients in this age group have a significantly higher disease prevalence."")","{'summary': 'Code demonstrates Survivorship Bias in healthcare context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","# Synthetic data generation (simplified)
import pandas as pd
import numpy as np

np.random.seed(0)  # for reproducibility
n_patients = 1000
demographics = ['age', 'gender', 'race']
data = pd.DataFrame({
    'age': np.random.randint(18, 100, n_patients),
    'gender': np.random.choice(['M', 'F'], n_patients),
    'race': np.random.choice(['White', 'Black', 'Asian'], n_patients),
    'disease': np.random.choice([0, 1], n_patients, p=[0.05, 0.95])  # 5% disease prevalence
})

# Analysis: Considering all patients (both surviving and non-surviving)
all_cases = data.groupby('age')['disease'].mean()  # 0.05 (5% of patients in this age group have disease)

print(""Conclusion: The disease prevalence in this age group is 5%."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T20:11:05.151758,llama-3-8b-instruct,v1,False
survivorship_bias,critical,education,student performance prediction,python,intermediate,"# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_students = 1000
grad_rates = np.random.binomial(1, 0.5, size=n_students)
demographics = pd.DataFrame({'Age': np.random.randint(18, 25, size=n_students), 
                            'Gender': np.random.choice(['Male', 'Female'], size=n_students), 
                            'Income': np.random.randint(0, 100000, size=n_students)})

# Analyze only surviving/successful cases
surviving_students = demographics[demographics['Graduated'] == 1]
print(""Graduation rate for males:"", sum(surviving_students[demographics['Gender'] == 'Male']) / len(surviving_students))
print(""Graduation rate for females:"", sum(surviving_students[demographics['Gender'] == 'Female']) / len(surviving_students))

# Flawed conclusion
print(""Conclusion: Males have a significantly higher graduation rate than females."")","{'summary': 'Code demonstrates Survivorship Bias in education context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of student performance prediction, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in student performance prediction'}","# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_students = 1000
grad_rates = np.random.binomial(1, 0.5, size=n_students)
demographics = pd.DataFrame({'Age': np.random.randint(18, 25, size=n_students), 
                            'Gender': np.random.choice(['Male', 'Female'], size=n_students), 
                            'Income': np.random.randint(0, 100000, size=n_students)})

# Analyze the entire population
grad_rates_by_gender = demographics.groupby('Gender')['Graduated'].mean()
print(""Graduation rate for males:"", grad_rates_by_gender.loc['Male'])
print(""Graduation rate for females:"", grad_rates_by_gender.loc['Female'])

# Corrected conclusion
print(""Conclusion: The graduation rate for males is"", round(grad_rates_by_gender.loc['Male'] * 100, 2), ""%"", 
      ""and for females is"", round(grad_rates_by_gender.loc['Female'] * 100, 2), ""%."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T20:12:01.289158,llama-3-8b-instruct,v1,False
survivorship_bias,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = pd.DataFrame({
    'User_ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Social_Media_Usage': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
    'Mental_Health_Score': [8, 9, 10, 11, 12, 13, 14, 15, 16, 17]
})

# Filter only users who have used social media for more than 5 hours
surviving_users = data[data['Social_Media_Usage'] > 5]

# Calculate correlation between social media usage and mental health score
correlation = surviving_users['Social_Media_Usage'].corr(surprising_users['Mental_Health_Score'])
print(f""The correlation between social media usage and mental health score is {correlation:.2f}"")

# Conclusion
print(""Based on this analysis, we can conclude that there is a strong positive correlation between social media usage and mental health score."")
print(""This means that as social media usage increases, mental health score also increases."")","{'summary': 'Code demonstrates Survivorship Bias in social_science context', 'detailed_explanation': 'This code demonstrates Survivorship Bias. Selection bias from only observing survivors distorts conclusions In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = pd.DataFrame({
    'User_ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Social_Media_Usage': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12],
    'Mental_Health_Score': [8, 9, 10, 11, 12, 13, 14, 15, 16, 17],
    'Dropout_Flag': [0, 0, 0, 0, 0, 1, 0, 1, 0, 1]
})

# Calculate the correlation between social media usage and mental health score, considering all users
correlation = data[['Social_Media_Usage', 'Mental_Health_Score']].corr()
print(f""The correlation between social media usage and mental health score is {correlation['Social_Media_Usage']['Mental_Health_Score']:.2f}"")

# Conclusion
print(""Based on this analysis, we can conclude that there is a moderate positive correlation between social media usage and mental health score."")
print(""However, we must consider the potential impact of survivorship bias and the limitations of our analysis."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias from only observing survivors distorts conclusions']",Selection bias from only observing survivors distorts conclusions,2025-12-14T20:13:08.464244,llama-3-8b-instruct,v1,False
confounding_omission,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 1000
X = pd.DataFrame({'Treatment': np.random.binomial(1, 0.5, n),  # binary treatment assignment
                 'Outcome': np.random.normal(0, 1, n),  # continuous outcome
                 'Age': np.random.normal(40, 10, n),  # confounder
                 'Gender': np.random.binomial(2, 0.5, n)  # confounder
                })

# Perform analysis without controlling for confounders
X['Treatment'] = X['Treatment'].astype(int)
X['Outcome'] = X['Outcome'].astype(float)

# Calculate treatment effect
treatment_effect = np.mean(X[X['Treatment'] == 1]['Outcome']) - np.mean(X[X['Treatment'] == 0]['Outcome'])
print(""Treatment effect:"", treatment_effect)","{'summary': 'Code demonstrates Omitted Confounding Variables in healthcare context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n = 1000
X = pd.DataFrame({'Treatment': np.random.binomial(1, 0.5, n),  # binary treatment assignment
                 'Outcome': np.random.normal(0, 1, n),  # continuous outcome
                 'Age': np.random.normal(40, 10, n),  # confounder
                 'Gender': np.random.binomial(2, 0.5, n)  # confounder
                })

# Perform analysis with control for confounders
X['Treatment'] = X['Treatment'].astype(int)
X['Outcome'] = X['Outcome'].astype(float)

# Create a linear regression model
model = LinearRegression()
model.fit(X[['Age', 'Gender']], X['Outcome'])

# Calculate treatment effect while controlling for confounders
treatment_effect = model.predict([[np.mean(X['Age']), np.mean(X['Gender'])]) - model.predict([[np.mean(X[X['Treatment'] == 0]['Age']), np.mean(X[X['Treatment'] == 0]['Gender'])]])
print(""Treated effect:"", treatment_effect)","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:14:14.570212,llama-3-8b-instruct,v1,False
confounding_omission,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"# Import necessary libraries
import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Age': np.random.randint(18, 100, 1000),
    'Sex': np.random.choice(['Male', 'Female'], 1000),
    'Disease': np.random.choice(['Yes', 'No'], 1000),
    'Smoker': np.random.choice(['Yes', 'No'], 1000)
})

# Perform analysis
p_value = data.groupby('Sex')['Disease'].mean().diff().mean()
print(f""P-value: {p_value}"")

# Incorrect reasoning
print(""The results suggest that the disease prevalence is significantly higher in males than females."")
print(""Therefore, we can conclude that being male is a risk factor for the disease."")","{'summary': 'Code demonstrates Omitted Confounding Variables in healthcare context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","# Import necessary libraries
import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Age': np.random.randint(18, 100, 1000),
    'Sex': np.random.choice(['Male', 'Female'], 1000),
    'Disease': np.random.choice(['Yes', 'No'], 1000),
    'Smoker': np.random.choice(['Yes', 'No'], 1000)
})

# Perform analysis with confounder control
chi2, p_value, dof, expected = chi2_contingency(pd.crosstab(data['Sex'], data['Disease'], margins=True))
print(f""P-value: {p_value}"")

# Correct reasoning
print(""The results suggest that there is a significant association between disease prevalence and smoking status, but not with sex."")
print(""To determine the effect of sex on disease prevalence, we would need to control for smoking status in the analysis."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:15:10.039636,llama-3-8b-instruct,v1,False
confounding_omission,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
churn_data = pd.DataFrame({
    'customer_id': np.arange(1, n_samples + 1),
    'age': np.random.randint(20, 60, size=n_samples),
    'income': np.random.randint(30000, 100000, size=n_samples),
    'churn': np.random.randint(0, 2, size=n_samples),  # 0 = no churn, 1 = churn
})

# Perform analysis without controlling for confounders
from sklearn.linear_model import LogisticRegression
log_model = LogisticRegression()
log_model.fit(churn_data[['age']], churn_data['churn'])
print(""Omitting income, we'll just use age as a predictor."")
print(""Our model suggests that customers over 40 are more likely to churn."")","{'summary': 'Code demonstrates Omitted Confounding Variables in business context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","# Generate synthetic data
np.random.seed(0)
n_samples = 1000
churn_data = pd.DataFrame({
    'customer_id': np.arange(1, n_samples + 1),
    'age': np.random.randint(20, 60, size=n_samples),
    'income': np.random.randint(30000, 100000, size=n_samples),
    'churn': np.random.randint(0, 2, size=n_samples),  # 0 = no churn, 1 = churn
})

# Perform analysis controlling for confounders
from sklearn.linear_model import LogisticRegression
log_model = LogisticRegression()
log_model.fit(churn_data[['age', 'income']], churn_data['churn'])
print(""We'll control for income when analyzing the relationship between age and churn."")
print(""Our model suggests that, after controlling for income, age remains a significant predictor of churn."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:16:00.715007,llama-3-8b-instruct,v1,False
confounding_omission,critical,business,marketing campaign effectiveness study,python,intermediate,"# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_samples = 1000
X = pd.DataFrame(np.random.normal(0, 1, size=(n_samples, 3)))
y = pd.Series(np.random.binomial(1, 0.5, size=n_samples))

# Define a treatment and outcome variable
treatment = X.iloc[:, 0] > 0.5
outcome = y

# Perform analysis
from statsmodels.linear_model import OLS
model = OLS(outcome, X).fit()
print(model.summary())","{'summary': 'Code demonstrates Omitted Confounding Variables in business context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_samples = 1000
X = pd.DataFrame(np.random.normal(0, 1, size=(n_samples, 4)))
X.columns = ['treatment', 'confounder1', 'confounder2', 'confounder3']
y = pd.Series(np.random.binomial(1, 0.5, size=n_samples))

# Define a treatment and outcome variable
treatment = X.iloc[:, 0] > 0.5
outcome = y

# Perform analysis with confounders
from statsmodels.linear_model import OLS
from statsmodels.api import AdditiveModel

# Fit a model that includes the confounders
model = AdditiveModel(outcome ~ treatment + X.iloc[:, 1:] + Additive(1), data=X).fit()
print(model.summary())","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:16:51.568505,llama-3-8b-instruct,v1,False
confounding_omission,critical,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
demographics = pd.DataFrame({
    'Age': np.random.randint(18, 80, 100),
    'Sex': np.random.choice(['Male', 'Female'], 100),
    'Disease': np.random.choice(['Yes', 'No'], 100),
    'Income': np.random.randint(30000, 100000, 100)
})

# Perform analysis
plt.scatter(demographics['Age'], demographics['Disease'])
plt.xlabel('Age')
plt.ylabel('Disease Prevalence')
plt.title('Disease Prevalence by Age')
plt.show()

# Conclusion
print(""There is a clear correlation between age and disease prevalence. Therefore, we can conclude that older people are more likely to have the disease."")","{'summary': 'Code demonstrates Omitted Confounding Variables in healthcare context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Generate synthetic data
np.random.seed(0)
demographics = pd.DataFrame({
    'Age': np.random.randint(18, 80, 100),
    'Sex': np.random.choice(['Male', 'Female'], 100),
    'Disease': np.random.choice(['Yes', 'No'], 100),
    'Income': np.random.randint(30000, 100000, 100)
})

# Split data into training and testing sets
X = demographics[['Age', 'Sex', 'Income']]
y = demographics['Disease']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Perform analysis with confounder control
logreg = LogisticRegression()
logreg.fit(X_train[['Age', 'Income']], y_train)
y_pred = logreg.predict(X_test[['Age', 'Income']])

# Plot results
plt.scatter(X_test['Age'], y_pred)
plt.xlabel('Age')
plt.ylabel('Predicted Disease Prevalence')
plt.title('Disease Prevalence by Age, controlling for Income')
plt.show()

# Conclusion
print(""After controlling for income, we see a more nuanced relationship between age and disease prevalence. The results suggest that older people are more likely to have the disease, but the effect is mitigated by income."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:17:53.705955,llama-3-8b-instruct,v1,False
confounding_omission,critical,education,student performance prediction,python,intermediate,"# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
data = {
    'Method': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],
    'Score': np.random.normal(50, 10, 9),
    'Age': np.random.randint(25, 45, 9),
    'Experience': np.random.randint(0, 10, 9)
}
df = pd.DataFrame(data)

# Perform analysis
from statsmodels.formula.api import ols

model = ols('Score ~ C(Method)', data=df).fit()
print(model.summary())

# Flawed conclusion
print(""Method A is significantly better than Methods B and C."")","{'summary': 'Code demonstrates Omitted Confounding Variables in education context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of student performance prediction, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in student performance prediction'}","# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
data = {
    'Method': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],
    'Score': np.random.normal(50, 10, 9),
    'Age': np.random.randint(25, 45, 9),
    'Experience': np.random.randint(0, 10, 9)
}
df = pd.DataFrame(data)

# Corrected analysis
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm

# Create a model that controls for Age and Experience
model = ols('Score ~ C(Method) + Age + Experience', data=df).fit()
print(model.summary())

# Corrected conclusion
print(""After controlling for Age and Experience, there is no significant difference in Score between Methods A, B, and C."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:18:56.375495,llama-3-8b-instruct,v1,False
confounding_omission,critical,social_science,income inequality trends,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'income': np.random.normal(50000, 10000, 1000),
                     'education': np.random.choice(['high', 'low'], 1000),
                     'age': np.random.randint(25, 65, 1000),
                    'married': np.random.choice([True, False], 1000)})

# Analyze the relationship between income and education
plt.scatter(data['income'], data['education'])
plt.xlabel('Income')
plt.ylabel('Education')
plt.title('Relationship between Income and Education')
plt.show()

# Conclusion: There is a strong positive correlation between income and education.
print(""As education increases, income also tends to increase."")","{'summary': 'Code demonstrates Omitted Confounding Variables in social_science context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of income inequality trends, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in income inequality trends'}","import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'income': np.random.normal(50000, 10000, 1000),
                     'education': np.random.choice(['high', 'low'], 1000),
                     'age': np.random.randint(25, 65, 1000),
                    'married': np.random.choice([True, False], 1000)})

# Control for confounding variables using linear regression
X = pd.DataFrame({'education', 'age','married'])
y = data['income']
model = LinearRegression().fit(X, y)

# Analyze the relationship between income and education, controlling for age and married
plt.scatter(data['education'], data['income'], c='blue')
plt.xlabel('Education')
plt.ylabel('Income')
plt.title('Relationship between Income and Education (controlled for age and married)')
plt.show()

# Conclusion: After controlling for age and married, there is a weaker correlation between income and education.
print(""The relationship between income and education is partially explained by the effects of age and married."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:19:50.627776,llama-3-8b-instruct,v1,False
confounding_omission,critical,education,student performance prediction,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_students = 1000
n_features = 3
X = np.random.rand(n_students, n_features)
y = np.random.rand(n_students)  # student performance (0-1)

# Create a confounder variable (e.g., socioeconomic status)
confounder = np.random.randint(0, 2, size=n_students)  # 0/1 binary confounder

# Create a feature that's correlated with confounder and outcome
feature1 = np.random.rand(n_students)  # correlated with confounder and outcome
feature2 = np.random.rand(n_students)  # not correlated with confounder or outcome

# Create a dataframe
df = pd.DataFrame(np.concatenate((X, confounder.reshape(-1, 1), feature1.reshape(-1, 1), feature2.reshape(-1, 1)), axis=1), columns=['feature1', 'feature2', 'confounder', 'outcome'])

# Perform analysis without controlling for confounder
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['feature1', 'feature2']], df['outcome'])
print(""Coefficient for feature1:"", model.coef_[0])

# Conclusion
print(""The analysis suggests that feature1 has a strong positive relationship with student performance."")","{'summary': 'Code demonstrates Omitted Confounding Variables in education context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of student performance prediction, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in student performance prediction'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.compose import ColumnTransformer

# Generate synthetic data
np.random.seed(0)
n_students = 1000
n_features = 3
X = np.random.rand(n_students, n_features)
y = np.random.rand(n_students)  # student performance (0-1)

# Create a confounder variable (e.g., socioeconomic status)
confounder = np.random.randint(0, 2, size=n_students)  # 0/1 binary confounder

# Create a feature that's correlated with confounder and outcome
feature1 = np.random.rand(n_students)  # correlated with confounder and outcome
feature2 = np.random.rand(n_students)  # not correlated with confounder or outcome

# Create a dataframe
df = pd.DataFrame(np.concatenate((X, confounder.reshape(-1, 1), feature1.reshape(-1, 1), feature2.reshape(-1, 1)), axis=1), columns=['feature1', 'feature2', 'confounder', 'outcome'])

# Create a pipeline to control for confounder
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

preprocessor = ColumnTransformer(transformers=[('scaler', StandardScaler(), ['feature1', 'feature2'])])
model = Pipeline([('preprocessor', preprocessor), ('regressor', LinearRegression())])

# Fit the model
model.fit(df[['feature1', 'feature2', 'confounder']], df['outcome'])
print(""Coefficient for feature1:"", model.steps[1][1].coef_[0])

# Conclusion
print(""The analysis suggests that feature1 has a strong positive relationship with student performance, after controlling for the confounder."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:21:07.832756,llama-3-8b-instruct,v1,False
confounding_omission,critical,social_science,voting behavior analysis,python,intermediate,"# Flawed code: Omitted Confounding Variables
import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = {
    'vote': np.random.randint(0, 2, 1000),  # 0: no, 1: yes
    'age': np.random.randint(18, 100, 1000),
    'income': np.random.randint(0, 100000, 1000),
    'education': np.random.randint(0, 20, 1000)
}
df = pd.DataFrame(data)

# Perform analysis
from scipy.stats import chi2_contingency
chi2, p_val, dof, expected = chi2_contingency(pd.crosstab(df['age'], df['vote']))
print(f""Chi-squared statistic: {chi2}, p-value: {p_val}"")

# Conclusion
print(""There is a significant association between age and voting behavior."")
print(""Older people are more likely to vote yes."")","{'summary': 'Code demonstrates Omitted Confounding Variables in social_science context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","# Corrected code: Controlling for confounding variables
import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency
from statsmodels.formula.api import logit

# Generate synthetic data
np.random.seed(0)
data = {
    'vote': np.random.randint(0, 2, 1000),  # 0: no, 1: yes
    'age': np.random.randint(18, 100, 1000),
    'income': np.random.randint(0, 100000, 1000),
    'education': np.random.randint(0, 20, 1000)
}
df = pd.DataFrame(data)

# Perform analysis
# Fit a logistic regression model to control for confounding variables
model = logit('vote ~ age + income + education', data=df).fit()
# Extract the chi-squared statistic and p-value from the model
chi2, p_val = model.f_statistic, model.pvalue

print(f""Adjusted Chi-squared statistic: {chi2}, p-value: {p_val}"")

# Conclusion
print(""After controlling for confounding variables, there is a significant association between age and voting behavior."")
print(""Older people are more likely to vote yes, while income and education have a negligible impact."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:22:07.084849,llama-3-8b-instruct,v1,False
confounding_omission,critical,education,online vs in-person learning outcomes,python,intermediate,"# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_students = 1000
n_treatments = 2
n_outcomes = 2

df = pd.DataFrame({
    'demographics': np.random.choice(['male', 'female'], size=n_students),
    'treatment': np.random.choice([0, 1], size=n_students),
    'outcome1': np.random.normal(50, 10, size=n_students),
    'outcome2': np.random.normal(50, 10, size=n_students)
})

# Perform analysis that commits the error
from statsmodels.ols import OLS

X = df[['demographics', 'treatment']]
y = df['outcome1']

model = OLS(y, X).fit()
print(model.params)
print(model.fittedvalues)

# Incorrect reasoning
print(""The treatment has a significant effect on outcome1!"")","{'summary': 'Code demonstrates Omitted Confounding Variables in education context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_students = 1000
n_treatments = 2
n_outcomes = 2

df = pd.DataFrame({
    'demographics': np.random.choice(['male', 'female'], size=n_students),
    'treatment': np.random.choice([0, 1], size=n_students),
    'outcome1': np.random.normal(50, 10, size=n_students),
    'outcome2': np.random.normal(50, 10, size=n_students)
})

# Perform analysis that controls for confounding variables
from statsmodels.ols import OLS
from statsmodels.api import add_constant

X = add_constant(pd.get_dummies(df[['demographics']]))
X = pd.concat([X, df[['treatment']]], axis=1)
y = df['outcome1']

model = OLS(y, X).fit()
print(model.params)
print(model.fittedvalues)

# Correct reasoning
print(""After controlling for demographics, the treatment has a significant effect on outcome1!"")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:23:06.495034,llama-3-8b-instruct,v1,False
confounding_omission,critical,education,student performance prediction,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
n_students = 1000
demographics = pd.DataFrame({'race': ['white', 'black', 'hispanic', 'asian', 'other'] * 200,
                             'gender': ['male','male','male','male','male'] * 200,
                             'grad_rate': [0.8, 0.6, 0.7, 0.8, 0.7] * 200})

# Perform analysis
plt.scatter(demographics['race'], demographics['grad_rate'])
plt.xlabel('Race')
plt.ylabel('Graduation Rate')
plt.title('Graduation Rate by Race')
plt.show()

print(""Conclusion: White students have a significantly higher graduation rate than black students."")","{'summary': 'Code demonstrates Omitted Confounding Variables in education context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of student performance prediction, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in student performance prediction'}","import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.linear_model import LinearRegression

# Generate synthetic data
n_students = 1000
demographics = pd.DataFrame({'race': ['white', 'black', 'hispanic', 'asian', 'other'] * 200,
                             'gender': ['male','male','male','male','male'] * 200,
                             'grad_rate': [0.8, 0.6, 0.7, 0.8, 0.7] * 200})

# Perform analysis
X = pd.get_dummies(demographics, columns=['race', 'gender'])
y = demographics['grad_rate']

model = LinearRegression()
model.fit(X, y)

print(""Coefficients:"")
print(model.coef_)

plt.scatter(demographics['race'], demographics['grad_rate'])
plt.xlabel('Race')
plt.ylabel('Graduation Rate')
plt.title('Graduation Rate by Race')
plt.show()

print(""Conclusion: The relationship between race and graduation rate is confounded by gender. Further analysis is needed to isolate the effect of race on graduation rate."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:24:01.808096,llama-3-8b-instruct,v1,False
confounding_omission,critical,social_science,voting behavior analysis,python,intermediate,"# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Generate synthetic data
n = 1000
X = pd.DataFrame({'vote_democrat': [1 if np.random.rand() < 0.5 else 0 for _ in range(n)],
                 'vote_republican': [1 - x for x in X['vote_democrat']],
                 'age': np.random.randint(18, 100, n),
                 'income': np.random.normal(50000, 20000, n),
                 'education': np.random.choice(['high', 'low'], n)})

# Perform analysis
t_stat, p_val = ttest_ind(X['vote_democrat'], X['age'])
print(f""T-statistic: {t_stat}, p-value: {p_val}"")

# Conclusion
print(""Based on the results, we can conclude that there is a significant relationship between voting Democrat and age."")","{'summary': 'Code demonstrates Omitted Confounding Variables in social_science context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind, pearsonr
from statsmodels.formula.api import ols

# Generate synthetic data
n = 1000
X = pd.DataFrame({'vote_democrat': [1 if np.random.rand() < 0.5 else 0 for _ in range(n)],
                 'age': np.random.randint(18, 100, n),
                 'income': np.random.normal(50000, 20000, n),
                 'education': np.random.choice(['high', 'low'], n)})

# Perform analysis
# Fit a linear regression model controlling for income and education
model = ols('vote_democrat ~ age + income + education', data=X).fit()
print(model.summary())

# Perform a partial correlation analysis
corr_age_vote, p_val_age_vote = pearsonr(X['age'], X['vote_democrat'])
print(f""Partial correlation between age and vote_democrat: {corr_age_vote}, p-value: {p_val_age_vote}"")

# Conclusion
print(""After controlling for income and education, we found no significant relationship between age and voting Democrat."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:25:01.405483,llama-3-8b-instruct,v1,False
confounding_omission,critical,social_science,income inequality trends,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'vote': np.random.choice(['Democrat', 'Republican'], size=1000, p=[0.5, 0.5]),
    'income': np.random.normal(50000, 10000, 1000),
    'education': np.random.choice(['High School', 'College', 'Graduate'], size=1000, p=[0.3, 0.5, 0.2]),
    'age': np.random.randint(18, 65, 1000)
})

# Analyze voting behavior
from sklearn.linear_model import LinearRegression
X = data[['income', 'education']]
y = data['vote']
model = LinearRegression()
model.fit(X, y)

print(""The analysis shows that for every additional $10000 in income, the likelihood of voting Democrat increases by 0.5%."")
print(""Furthermore, individuals with a college education are 1.2 times more likely to vote Democrat than those with a high school diploma."")","{'summary': 'Code demonstrates Omitted Confounding Variables in social_science context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of income inequality trends, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in income inequality trends'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'vote': np.random.choice(['Democrat', 'Republican'], size=1000, p=[0.5, 0.5]),
    'income': np.random.normal(50000, 10000, 1000),
    'education': np.random.choice(['High School', 'College', 'Graduate'], size=1000, p=[0.3, 0.5, 0.2]),
    'age': np.random.randint(18, 65, 1000)
})

# Create a pipeline with a linear regression model that includes both income and education
X = data[['income', 'education']]
y = data['vote']
pipe = Pipeline([
    ('ct', ColumnTransformer(transformers=[('linear', LinearRegression(), ['income'])])),
    ('reg', LinearRegression())
])
pipe.fit(X, y)

print(""The analysis shows that for every additional $10000 in income, the likelihood of voting Democrat increases by 0.5%, after controlling for education."")
print(""Furthermore, individuals with a college education are 1.2 times more likely to vote Democrat than those with a high school diploma, after accounting for income."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:26:10.671087,llama-3-8b-instruct,v1,False
confounding_omission,critical,healthcare,observational study of patient outcomes,python,intermediate,"# Flawed code: Failing to control for confounders
import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Age': np.random.randint(18, 100, size=1000),
    'Gender': np.random.choice(['Male', 'Female'], size=1000),
    'Disease': np.random.choice(['Yes', 'No'], size=1000),
    'Region': np.random.choice(['North', 'South', 'East', 'West'], size=1000),
    'Smoker': np.random.choice(['Yes', 'No'], size=1000)
})

# Analyze disease prevalence by gender and region
disease_by_gender_region = data.groupby(['Gender', 'Region'])['Disease'].mean().reset_index()
print(disease_by_gender_region)","{'summary': 'Code demonstrates Omitted Confounding Variables in healthcare context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","# Corrected code: Controlling for confounders
import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Age': np.random.randint(18, 100, size=1000),
    'Gender': np.random.choice(['Male', 'Female'], size=1000),
    'Disease': np.random.choice(['Yes', 'No'], size=1000),
    'Region': np.random.choice(['North', 'South', 'East', 'West'], size=1000),
    'Smoker': np.random.choice(['Yes', 'No'], size=1000)
})

# Analyze disease prevalence by gender, region, and age
disease_by_gender_region_age = data.groupby(['Gender', 'Region', 'Age'])['Disease'].mean().reset_index()
print(disease_by_gender_region_age)

# Analyze disease prevalence by gender, region, and smoking status
disease_by_gender_region_smoker = data.groupby(['Gender', 'Region', 'Smoker'])['Disease'].mean().reset_index()
print(disease_by_gender_region_smoker)","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:27:16.015077,llama-3-8b-instruct,v1,False
confounding_omission,critical,education,graduation rate analysis by demographics,python,intermediate,"# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_students = 1000
n_teachers = 50
student_performance = np.random.normal(50, 10, n_students)
teacher_effectiveness = np.random.normal(60, 10, n_teachers)
assignment = np.random.choice(['A', 'B'], size=n_students)

# Create a dataframe
df = pd.DataFrame({'Student': range(n_students), 'Assignment': assignment, 'Performance': student_performance})

# Perform analysis
from scipy.stats import ttest_ind
t_stat, p_val = ttest_ind(df[df['Assignment'] == 'A']['Performance'], df[df['Assignment'] == 'B']['Performance'])
print(f""t-statistic: {t_stat}, p-value: {p_val}"")
print(""Conclusion: Assignment A is significantly better than Assignment B."")

# Conclusion: We can conclude that the observed difference in performance is due to the difference in teaching methods.","{'summary': 'Code demonstrates Omitted Confounding Variables in education context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_students = 1000
n_teachers = 50
student_performance = np.random.normal(50, 10, n_students)
teacher_effectiveness = np.random.normal(60, 10, n_teachers)
assignment = np.random.choice(['A', 'B'], size=n_students)
teacher = np.random.choice(range(n_teachers), size=n_students)

# Create a dataframe
df = pd.DataFrame({'Student': range(n_students), 'Assignment': assignment, 'Teacher': teacher, 'Performance': student_performance})

# Perform analysis
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm

# Fit a linear model with assignment, teacher, and interaction as predictors
model = ols('Performance ~ C(assignment) + C(teacher) + C(assignment):C(teacher)', data=df).fit()
anova_table = anova_lm(model)

print(anova_table)
print(""Conclusion: The analysis controlling for teacher effectiveness reveals that Assignment A is significantly better than Assignment B."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:28:17.673573,llama-3-8b-instruct,v1,False
confounding_omission,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
crime_data = pd.DataFrame({
    'crime_rate': np.random.normal(10, 2, 100),  # average crime rate
    'poverty_rate': np.random.normal(15, 3, 100),  # average poverty rate
    'unemployment_rate': np.random.normal(5, 1, 100)  # average unemployment rate
})

# Perform analysis
correlation_matrix = crime_data.corr()
print(""Correlation between crime rate and poverty rate:"", correlation_matrix['crime_rate']['poverty_rate'])

# Conclusion
print(""Since the correlation between crime rate and poverty rate is strong, we can conclude that poverty causes crime."")","{'summary': 'Code demonstrates Omitted Confounding Variables in social_science context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np
from statsmodels.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
crime_data = pd.DataFrame({
    'crime_rate': np.random.normal(10, 2, 100),  # average crime rate
    'poverty_rate': np.random.normal(15, 3, 100),  # average poverty rate
    'unemployment_rate': np.random.normal(5, 1, 100)  # average unemployment rate
})

# Perform analysis with confounder control
X = pd.DataFrame({'poverty_rate', 'unemployment_rate'})
y = crime_data['crime_rate']
model = LinearRegression().fit(X, y)
print(""Coefficient for poverty rate:"", model.coef_[0])
print(""Coefficient for unemployment rate:"", model.coef_[1])

# Conclusion
print(""After controlling for unemployment rate, we can see that poverty rate has a significant impact on crime rate."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:29:08.815605,llama-3-8b-instruct,v1,False
confounding_omission,critical,business,pricing strategy impact analysis,python,intermediate,"# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
data = pd.DataFrame({
    'Price': np.random.normal(10, 2, 1000),
    'Discount': np.random.binomial(1, 0.5, 1000),
    'Sales': np.random.normal(50, 10, 1000),
    'Region': np.random.choice(['East', 'West', 'North'], 1000)
})

# Perform analysis without controlling for confounders
import statsmodels
results = statsmodels.OLS(data['Sales'], data[['Price', 'Discount']]).fit()
print(results.summary())

# Conclusion
print(""The analysis suggests that a 1% increase in price results in a 2.5% decrease in sales."")","{'summary': 'Code demonstrates Omitted Confounding Variables in business context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
data = pd.DataFrame({
    'Price': np.random.normal(10, 2, 1000),
    'Discount': np.random.binomial(1, 0.5, 1000),
    'Sales': np.random.normal(50, 10, 1000),
    'Region': np.random.choice(['East', 'West', 'North'], 1000)
})

# Perform analysis controlling for confounders
import statsmodels
results = statsmodels.OLS(data['Sales'], data[['Price', 'Discount', 'Region']].).fit()
print(results.summary())

# Conclusion
print(""The analysis suggests that a 1% increase in price results in a 3.8% decrease in sales, after controlling for regional differences and discounting."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:30:05.148558,llama-3-8b-instruct,v1,False
confounding_omission,critical,social_science,social media usage and mental health,python,intermediate,"# Flawed Code: Omitted Confounding Variables
import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = {
    'voted': np.random.binomial(1, 0.5, 1000),  # 50% chance of voting
    'income': np.random.normal(50000, 10000, 1000),  # average income 50k, std 10k
    'education': np.random.choice(['high', 'low'], 1000, p=[0.7, 0.3])  # 70% high, 30% low
}
df = pd.DataFrame(data)

# Perform analysis
correlation = df['voted'].corr(df['income'])
print(f""Correlation between voting and income: {correlation:.2f}"")

# Flawed conclusion
print(""There is a strong correlation between voting and income, so higher-income individuals are more likely to vote."")","{'summary': 'Code demonstrates Omitted Confounding Variables in social_science context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","# Corrected Code: Omitted Confounding Variables
import pandas as pd
import numpy as np
from statsmodels.linear_regression import OLS

# Generate synthetic data
np.random.seed(0)
data = {
    'voted': np.random.binomial(1, 0.5, 1000),  # 50% chance of voting
    'income': np.random.normal(50000, 10000, 1000),  # average income 50k, std 10k
    'education': np.random.choice(['high', 'low'], 1000, p=[0.7, 0.3])  # 70% high, 30% low
}
df = pd.DataFrame(data)

# Perform analysis with control for confounding variables
ols = OLS(df['voted'], df[['income', 'education']])
ols_result = ols.fit()
print(f""Adjusted R-squared: {ols_result.rsquared_adj:.2f}"")

# Corrected conclusion
print(""After controlling for education, there is no significant correlation between voting and income."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:31:07.775032,llama-3-8b-instruct,v1,False
confounding_omission,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 1000
X = np.random.normal(0, 1, n)  # independent variable
Y = np.random.normal(0, 1, n)  # dependent variable
Z = np.random.normal(0, 1, n)  # confounder

df = pd.DataFrame({'X': X, 'Y': Y, 'Z': Z})

# Analyze the data
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['X']], df['Y'])  # Failing to control for confounders
print(""Flawed Coefficient:"", model.coef_)

# Conclusion
print(""The analysis suggests that X has a significant effect on Y."")","{'summary': 'Code demonstrates Omitted Confounding Variables in social_science context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 1000
X = np.random.normal(0, 1, n)  # independent variable
Y = np.random.normal(0, 1, n)  # dependent variable
Z = np.random.normal(0, 1, n)  # confounder

df = pd.DataFrame({'X': X, 'Y': Y, 'Z': Z})

# Analyze the data with confounder control
from sklearn.linear_model import LinearRegression
from statsmodels.formula.api import ols

# Fit the model with confounder control
model = ols('Y ~ X + Z', data=df).fit()
print(""Corrected Coefficient:"", model.params['X'])

# Conclusion
print(""The analysis suggests that X has a significant effect on Y, after controlling for Z."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:32:00.872159,llama-3-8b-instruct,v1,False
confounding_omission,critical,business,pricing strategy impact analysis,python,intermediate,"# Flawed Code: Omitted Confounding Variables
import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
X = pd.DataFrame({'Treatment': np.random.choice(['A', 'B'], size=n_samples),
                 'Outcome': np.random.normal(0, 1, n_samples),
                 'Confounder': np.random.normal(0, 1, n_samples)})

# Perform analysis without controlling for confounder
from scipy.stats import ttest_ind
t_stat, p_val = ttest_ind(X['Treatment'] == 'A', X['Outcome'])

print(f""Treatment A vs B: t-statistic = {t_stat}, p-value = {p_val}"")","{'summary': 'Code demonstrates Omitted Confounding Variables in business context', 'detailed_explanation': 'This code demonstrates Omitted Confounding Variables. Omitted variable bias invalidates causal interpretation In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","# Corrected Code: Controlling for Confounding Variables
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
X = pd.DataFrame({'Treatment': np.random.choice(['A', 'B'], size=n_samples),
                 'Outcome': np.random.normal(0, 1, n_samples),
                 'Confounder': np.random.normal(0, 1, n_samples)}

# Perform analysis controlling for confounder
lm = LinearRegression()
lm.fit(X[['Confounder']], X['Outcome'])
treated_outcome = lm.predict(X.loc[X['Treatment'] == 'A', ['Confounder']])
control_outcome = lm.predict(X.loc[X['Treatment'] == 'B', ['Confounder']])
t_stat, p_val = ttest_ind(treated_outcome, control_outcome)

print(f""Treatment A vs B: t-statistic = {t_stat}, p-value = {p_val}"")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Omitted variable bias invalidates causal interpretation']",Omitted variable bias invalidates causal interpretation,2025-12-14T20:33:00.357165,llama-3-8b-instruct,v1,False
multiple_testing,critical,education,teaching method effectiveness comparison,python,intermediate,"import numpy as np
import pandas as pd
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_subjects = 10
student_performance = np.random.normal(50, 10, size=(n_students, n_subjects))

# Create a dataframe
df = pd.DataFrame(student_performance)

# Perform multiple t-tests without correction
p_values = []
for i in range(n_subjects):
    for j in range(i+1, n_subjects):
        t_stat, p_val = ttest_ind(df.iloc[:, i], df.iloc[:, j])
        p_values.append(p_val)

# Filter significant results
significant_p_values = [p_val for p_val in p_values if p_val < 0.05]

print(""There are"", len(significant_p_values), ""significant differences between subjects."")
print(""Conclusion: We can reject the null hypothesis of equal means for"", len(significant_p_values), ""subject pairs."")","{'summary': 'Code demonstrates Multiple Testing without Correction in education context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import numpy as np
import pandas as pd
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_subjects = 10
student_performance = np.random.normal(50, 10, size=(n_students, n_subjects))

# Create a dataframe
df = pd.DataFrame(student_performance)

# Perform multiple t-tests with Bonferroni correction
p_values = []
for i in range(n_subjects):
    for j in range(i+1, n_subjects):
        t_stat, p_val = ttest_ind(df.iloc[:, i], df.iloc[:, j])
        p_values.append(p_val)

# Apply Bonferroni correction
p_values_corrected = multipletests(p_values, alpha=0.05, method='bonferroni')[1]

# Filter significant results
significant_p_values = [p_val for p_val in p_values_corrected if p_val < 0.05]

print(""There are"", len(significant_p_values), ""significant differences between subjects."")
print(""Conclusion: We can reject the null hypothesis of equal means for"", len(significant_p_values), ""subject pairs."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:34:11.325499,llama-3-8b-instruct,v1,False
multiple_testing,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, chi2_contingency

# Generate synthetic data
np.random.seed(0)
n = 100
p = 0.5
churn_data = pd.DataFrame({
    'Customer_ID': np.arange(1, n+1),
    'Churn': np.random.binomial(n=1, p=p, size=n),
    'Age': np.random.randint(20, 60, size=n),
    'Income': np.random.randint(30000, 100000, size=n)
})

# Perform multiple statistical tests without adjustment
t_stat, p_val = ttest_ind(churn_data['Churn'], churn_data['Age'])
print(f""T-test p-value: {p_val:.4f}"")

contingency_table = pd.crosstab(churn_data['Churn'], churn_data['Income'])
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print(f""Chi-squared test p-value: {p_val:.4f}"")

# Draw conclusion without adjusting for multiple comparisons
print(""Based on the p-values, we can conclude that Age and Income are significant predictors of Churn."")","{'summary': 'Code demonstrates Multiple Testing without Correction in business context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, chi2_contingency
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
n = 100
p = 0.5
churn_data = pd.DataFrame({
    'Customer_ID': np.arange(1, n+1),
    'Churn': np.random.binomial(n=1, p=p, size=n),
    'Age': np.random.randint(20, 60, size=n),
    'Income': np.random.randint(30000, 100000, size=n)
})

# Perform multiple statistical tests with adjustment
t_stat, p_val = ttest_ind(churn_data['Churn'], churn_data['Age'])
p_val_adj = multipletests(p_val, alpha=0.05, method='bonferroni')[1]
print(f""T-test p-value (adjusted): {p_val_adj:.4f}"")

contingency_table = pd.crosstab(churn_data['Churn'], churn_data['Income'])
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
p_val_adj = multipletests(p_val, alpha=0.05, method='bonferroni')[1]
print(f""Chi-squared test p-value (adjusted): {p_val_adj:.4f}"")

# Draw conclusion with adjustment
print(""Based on the adjusted p-values, we can conclude that Age and Income are significant predictors of Churn (at a family-wise error rate of 0.05)."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:35:21.629940,llama-3-8b-instruct,v1,False
multiple_testing,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n_samples = 10
n_groups = 5
data = np.random.normal(loc=0, scale=1, size=(n_samples, n_groups))

# Perform multiple t-tests without correction
t_tests = []
for i in range(n_groups):
    for j in range(i+1, n_groups):
        t_stat, p_val = ttest_ind(data[:, i], data[:, j])
        t_tests.append((t_stat, p_val))

# Analyze results
t_tests_df = pd.DataFrame(t_tests, columns=['t-statistic', 'p-value'])
print(t_tests_df)

# Draw conclusions
print(""Based on the t-tests, we reject the null hypothesis that the means of all groups are equal."")
print(""The p-values indicate that the differences between groups are statistically significant."")","{'summary': 'Code demonstrates Multiple Testing without Correction in social_science context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
n_samples = 10
n_groups = 5
data = np.random.normal(loc=0, scale=1, size=(n_samples, n_groups))

# Perform multiple t-tests with correction
t_tests = []
for i in range(n_groups):
    for j in range(i+1, n_groups):
        t_stat, p_val = ttest_ind(data[:, i], data[:, j])
        t_tests.append((t_stat, p_val))

# Correct for multiple testing
_, corrected_p_values, _, _ = multipletests([t_val[1] for t_val in t_tests], alpha=0.05, method='bonferroni')

# Analyze results
t_tests_df = pd.DataFrame(zip(*t_tests), columns=['t-statistic', 'p-value'])
t_tests_df['corrected_p-value'] = corrected_p_values
print(t_tests_df)

# Draw conclusions
print(""Based on the t-tests with correction, we reject the null hypothesis that the means of all groups are equal."")
print(""The corrected p-values indicate that the differences between groups are statistically significant."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:36:23.451397,llama-3-8b-instruct,v1,False
multiple_testing,critical,social_science,crime rate factors,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame(np.random.normal(0, 1, (100, 10)), columns=[f'Feature {i}' for i in range(10)])
data['Response'] = np.random.choice(['yes', 'no'], 100)

# Perform multiple tests without adjustment
for i in range(10):
    t_stat, p_val = ttest_ind(data.iloc[:, i], data.iloc[:, i+1], equal_var=False)
    print(f'Test {i+1}: p-value = {p_val:.4f}')
    if p_val < 0.05:
        print(f'Test {i+1} is statistically significant!')","{'summary': 'Code demonstrates Multiple Testing without Correction in social_science context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame(np.random.normal(0, 1, (100, 10)), columns=[f'Feature {i}' for i in range(10)])
data['Response'] = np.random.choice(['yes', 'no'], 100)

# Perform multiple tests with adjustment
p_values = []
for i in range(10):
    t_stat, p_val = ttest_ind(data.iloc[:, i], data.iloc[:, i+1], equal_var=False)
    p_values.append(p_val)

# Correct for multiple testing using the Bonferroni method
_, corrected_p_values, _, _ = multipletests(p_values, alpha=0.05, method='bonferroni')

# Report adjusted p-values
for i, p_val in enumerate(corrected_p_values):
    print(f'Test {i+1}: adjusted p-value = {p_val:.4f}')
    if p_val < 0.05:
        print(f'Test {i+1} is statistically significant!')","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:37:26.487234,llama-3-8b-instruct,v1,False
multiple_testing,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame(np.random.normal(0, 1, size=(100, 3)), columns=['Group A', 'Group B', 'Group C'])
data['Group'] = ['A'] * 50 + ['B'] * 25 + ['C'] * 25

# Perform multiple t-tests without correction
results = []
for i in range(1, 4):
    for j in range(i+1, 4):
        result = ttest_ind(data.iloc[:, i-1], data.iloc[:, j-1])
        results.append({'Group A': i, 'Group B': j, 'p-value': result.pvalue})

# Draw conclusions
print(""The results are statistically significant:"")
for result in results:
    if result['p-value'] < 0.05:
        print(f""Group {result['Group A']} is significantly different from Group {result['Group B']}."")","{'summary': 'Code demonstrates Multiple Testing without Correction in healthcare context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame(np.random.normal(0, 1, size=(100, 3)), columns=['Group A', 'Group B', 'Group C'])
data['Group'] = ['A'] * 50 + ['B'] * 25 + ['C'] * 25

# Perform multiple t-tests with Bonferroni correction
results = []
for i in range(1, 4):
    for j in range(i+1, 4):
        result = ttest_ind(data.iloc[:, i-1], data.iloc[:, j-1])
        results.append({'Group A': i, 'Group B': j, 'p-value': result.pvalue})

# Corrected p-values using Bonferroni correction
_, corrected_pvalues, _, _ = multipletests([result['p-value'] for result in results], alpha=0.05, method='bonferroni')

# Draw conclusions
print(""The results are statistically significant:"")
for result, pvalue in zip(results, corrected_pvalues):
    if pvalue < 0.05:
        print(f""Group {result['Group A']} is significantly different from Group {result['Group B']}."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:38:26.266511,llama-3-8b-instruct,v1,False
multiple_testing,critical,social_science,social media usage and mental health,python,intermediate,"import numpy as np
import pandas as pd
from scipy.stats import ttest_ind, ttest_rel

# Generate synthetic data
np.random.seed(0)
n = 1000
income_data = np.random.normal(50000, 10000, n)
treatment_group = np.random.choice([0, 1], size=n, p=[0.5, 0.5])
control_group = np.random.choice([0, 1], size=n, p=[0.5, 0.5])

# Create a DataFrame
df = pd.DataFrame({'income': income_data, 'treatment': treatment_group, 'control': control_group})

# Perform multiple tests without correction
t_stat1, p_val1 = ttest_ind(df.loc[df['treatment'] == 1, 'income'], df.loc[df['treatment'] == 0, 'income'])
print(f""t-statistic: {t_stat1}, p-value: {p_val1}"")

t_stat2, p_val2 = ttest_rel(df.loc[df['treatment'] == 1, 'income'], df.loc[df['treatment'] == 0, 'income'])
print(f""t-statistic: {t_stat2}, p-value: {p_val2}"")

# Draw a conclusion without considering the multiple testing issue
print(""Conclusion: The treatment has a significant effect on income, as both t-tests yield p-values < 0.05."")","{'summary': 'Code demonstrates Multiple Testing without Correction in social_science context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import numpy as np
import pandas as pd
from scipy.stats import ttest_ind, ttest_rel
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
n = 1000
income_data = np.random.normal(50000, 1000, n)
treatment_group = np.random.choice([0, 1], size=n, p=[0.5, 0.5])
control_group = np.random.choice([0, 1], size=n, p=[0.5, 0.5])

# Create a DataFrame
df = pd.DataFrame({'income': income_data, 'treatment': treatment_group, 'control': control_group})

# Perform multiple tests with correction
t_stat1, p_val1 = ttest_ind(df.loc[df['treatment'] == 1, 'income'], df.loc[df['treatment'] == 0, 'income'])
p_val1, fdr1 = multipletests(p_val1, alpha=0.05, method='fdr_bh')
print(f""t-statistic: {t_stat1}, p-value: {p_val1}"")

t_stat2, p_val2 = ttest_rel(df.loc[df['treatment'] == 1, 'income'], df.loc[df['treatment'] == 0, 'income'])
p_val2, fdr2 = multipletests(p_val2, alpha=0.05, method='fdr_bh')
print(f""t-statistic: {t_stat2}, p-value: {p_val2}"")

# Draw a conclusion with consideration of the multiple testing issue
print(""Conclusion: The treatment has a significant effect on income, as both p-values after correction are below the adjusted alpha level."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:39:39.365349,llama-3-8b-instruct,v1,False
multiple_testing,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
data = {
    'Region': ['North', 'South', 'East', 'West', 'North', 'South', 'East', 'West'] * 5,
    'Sales': np.random.normal(loc=1000, scale=200, size=(16,))
}
df = pd.DataFrame(data)

# Perform multiple t-tests without correction
results = []
for region1, region2 in [(i, j) for i in range(4) for j in range(i+1, 4)]:
    _, p_value = ttest_ind(df[df['Region'] == region1]['Sales'], df[df['Region'] == region2]['Sales'])
    results.append((region1, region2, p_value))

# Flawed conclusion
print(""We reject the null hypothesis that the sales distribution is the same across all regions."")
print(""The p-values are: "", [p for _, _, p in results])","{'summary': 'Code demonstrates Multiple Testing without Correction in business context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
data = {
    'Region': ['North', 'South', 'East', 'West', 'North', 'South', 'East', 'West'] * 5,
    'Sales': np.random.normal(loc=1000, scale=200, size=(16,))
}
df = pd.DataFrame(data)

# Perform multiple t-tests with Bonferroni correction
_, p_values, _, _ = multipletests([p for _, _, p in results], alpha=0.05, method='bonferroni')
print(""Bonferroni-corrected p-values: "", p_values)

# Corrected conclusion
print(""After applying the Bonferroni correction, we can only reject the null hypothesis for the following pairs:"")
for i, p in enumerate(p_values):
    if p < 0.05 / 6:  # 0.05 / 6 is the Bonferroni-corrected significance level
        print(f""Region {i+1} vs. Region {i+2}"")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:40:41.728525,llama-3-8b-instruct,v1,False
multiple_testing,critical,social_science,income inequality trends,python,intermediate,"import numpy as np
import pandas as pd
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
crime_data = pd.DataFrame({
    'urban': np.random.normal(loc=50, scale=10, size=100),
    'rural': np.random.normal(loc=40, scale=10, size=100),
    'age': np.random.normal(loc=30, scale=5, size=100),
    'crime_rate': np.random.normal(loc=10, scale=2, size=100)
})

# Perform many statistical tests without adjusting significance levels
t_stat, p_val = ttest_ind(crime_data['urban'], crime_data['rural'])
print(f""t-statistic: {t_stat}, p-value: {p_val}"")

t_stat, p_val = ttest_ind(crime_data['urban'], crime_data['age'])
print(f""t-statistic: {t_stat}, p-value: {p_val}"")

t_stat, p_val = ttest_ind(crime_data['rural'], crime_data['age'])
print(f""t-statistic: {t_stat}, p-value: {p_val}"")

# Conclusion
print(""Based on the p-values, we can conclude that the urban and rural crime rates are significantly different, as well as the age and urban crime rate, and the age and rural crime rate."")","{'summary': 'Code demonstrates Multiple Testing without Correction in social_science context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of income inequality trends, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in income inequality trends'}","import numpy as np
import pandas as pd
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
crime_data = pd.DataFrame({
    'urban': np.random.normal(loc=50, scale=10, size=100),
    'rural': np.random.normal(loc=40, scale=10, size=100),
    'age': np.random.normal(loc=30, scale=5, size=100),
    'crime_rate': np.random.normal(loc=10, scale=2, size=100)
})

# Perform many statistical tests with adjustment for multiple comparisons
p_vals = []
for col1, col2 in [('urban', 'rural'), ('urban', 'age'), ('rural', 'age')]:
    t_stat, p_val = ttest_ind(crime_data[col1], crime_data[col2])
    p_vals.append(p_val)

# Adjust p-values using the Bonferroni method
_, p_vals_adj = multipletests(p_vals, alpha=0.05, method='bonferroni')

# Print results
for i, (col1, col2) in enumerate([('urban', 'rural'), ('urban', 'age'), ('rural', 'age')]):
    t_stat, _ = ttest_ind(crime_data[col1], crime_data[col2])
    print(f""t-statistic: {t_stat}, p-value: {p_vals_adj[i]}"")

# Conclusion
print(""Based on the adjusted p-values, we can conclude that the urban and rural crime rates are significantly different, but the age and urban crime rate and age and rural crime rate are not significantly different."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:42:01.128917,llama-3-8b-instruct,v1,False
multiple_testing,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Group': np.repeat(['A', 'B'], 100), 
                     'Value': np.random.normal(10, 2, 200)})

# Perform multiple tests without correction
for i in range(10):
    # Split data into two groups
    group_a = data[data['Group'] == 'A']
    group_b = data[data['Group'] == 'B']
    
    # Perform t-test
    t_stat, p_val = ttest_ind(group_a['Value'], group_b['Value'])
    
    # Print results
    print(f""Comparison {i+1}: p-value = {p_val:.4f}"")
    
    # If p-value is below significance level (0.05), declare significance
    if p_val < 0.05:
        print(f""Significant difference found at significance level 0.05"")
    else:
        print(""No significant difference found"")

# Flawed conclusion
print(""Based on the results, we conclude that there are significant differences between groups A and B in at least 5 comparisons."")","{'summary': 'Code demonstrates Multiple Testing without Correction in business context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Group': np.repeat(['A', 'B'], 100), 
                     'Value': np.random.normal(10, 2, 200)})

# Perform multiple tests with correction
p_vals = []
for i in range(10):
    # Split data into two groups
    group_a = data[data['Group'] == 'A']
    group_b = data[data['Group'] == 'B']
    
    # Perform t-test
    t_stat, p_val = ttest_ind(group_a['Value'], group_b['Value'])
    
    # Store p-values
    p_vals.append(p_val)
    
# Correct for multiple testing
_, p_corr, _, _ = multipletests(p_vals, alpha=0.05, method='fdr_bh')

# Print results
for i, p_val in enumerate(p_vals):
    if p_val < p_corr[i]:
        print(f""Comparison {i+1}: p-value = {p_val:.4f} (significant)"")
    else:
        print(f""Comparison {i+1}: p-value = {p_val:.4f} (not significant)"")

# Corrected conclusion
print(""Based on the results, we conclude that there is a significant difference between groups A and B in at least one comparison."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:43:14.235821,llama-3-8b-instruct,v1,False
multiple_testing,critical,social_science,social media usage and mental health,python,intermediate,"import numpy as np
import pandas as pd
from scipy.stats import ttest_ind

# Generate synthetic data for voting behavior analysis
np.random.seed(0)
n_samples = 100
n_groups = 5
data = np.random.normal(loc=[0, 0.5, 1, 1.5, 2], scale=0.5, size=(n_samples, n_groups))

# Perform multiple t-tests without adjusting significance levels
p_values = []
for i in range(n_groups):
    for j in range(i+1, n_groups):
        _, p = ttest_ind(data[:, i], data[:, j])
        p_values.append(p)

# Print the p-values
print(""P-values:"", p_values)

# Draw a conclusion without adjusting for multiple comparisons
conclusion = ""Based on the p-values, we reject the null hypothesis that the mean voting behavior is the same for all groups. There is a significant difference between groups 1 and 2, as well as groups 2 and 3.""
print(conclusion)","{'summary': 'Code demonstrates Multiple Testing without Correction in social_science context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import numpy as np
import pandas as pd
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Generate synthetic data for voting behavior analysis
np.random.seed(0)
n_samples = 100
n_groups = 5
data = np.random.normal(loc=[0, 0.5, 1, 1.5, 2], scale=0.5, size=(n_samples, n_groups))

# Perform multiple t-tests with adjusted significance levels
p_values = []
for i in range(n_groups):
    for j in range(i+1, n_groups):
        _, p = ttest_ind(data[:, i], data[:, j])
        p_values.append(p)

# Adjust the p-values using the Bonferroni correction
p_values_adj, _, _, _ = multipletests(p_values, alpha=0.05, method='bonferroni')

# Print the adjusted p-values
print(""Adjusted p-values:"", p_values_adj)

# Draw a conclusion after adjusting for multiple comparisons
conclusion = ""Based on the adjusted p-values, we reject the null hypothesis that the mean voting behavior is the same for all groups. There is a significant difference between groups 1 and 2, as well as groups 2 and 3.""
print(conclusion)","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:44:18.154331,llama-3-8b-instruct,v1,False
multiple_testing,critical,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
df = pd.DataFrame(np.random.normal(0, 1, size=(100, 3)), columns=['Method A', 'Method B', 'Method C'])

# Perform multiple t-tests without correction
results = []
for i in range(3):
    for j in range(i+1, 3):
        _, p_val = ttest_ind(df.iloc[:, i], df.iloc[:, j])
        results.append((i, j, p_val))

# Print results
for result in results:
    print(f""Method {result[0]} vs Method {result[1]}: p-value = {result[2]}"")

# Flawed conclusion
print(""Based on these p-values, we can conclude that Method A is significantly different from both Method B and Method C."")","{'summary': 'Code demonstrates Multiple Testing without Correction in education context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
df = pd.DataFrame(np.random.normal(0, 1, size=(100, 3)), columns=['Method A', 'Method B', 'Method C'])

# Perform multiple t-tests with correction
p_vals = []
for i in range(3):
    for j in range(i+1, 3):
        _, p_val = ttest_ind(df.iloc[:, i], df.iloc[:, j])
        p_vals.append(p_val)

# Correct for multiple testing using the Bonferroni method
p_vals_corr, _ = multipletests(p_vals, alpha=0.05, method='bonferroni')

# Print results
for i, p_val_corr in enumerate(p_vals_corr):
    if p_val_corr < 0.05:
        print(f""Method {i//3} vs Method {i%3}: p-value = {p_val_corr}"")

# Corrected conclusion
print(""Based on these corrected p-values, we can conclude that only Method A is significantly different from Method B."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:45:13.966762,llama-3-8b-instruct,v1,False
multiple_testing,critical,business,pricing strategy impact analysis,python,intermediate,"import numpy as np
import pandas as pd
from scipy.stats import ttest_ind, f_oneway

# Synthetic data
np.random.seed(0)
data = {
    'Group A': np.random.normal(10, 2, 100),
    'Group B': np.random.normal(12, 2, 100),
    'Group C': np.random.normal(11, 2, 100)
}
df = pd.DataFrame(data)

# Perform many tests without adjusting significance levels
results = []
for col1, col2 in [(i, j) for i in range(3) for j in range(i+1, 3)]:
    t_stat, p_val = ttest_ind(df.iloc[:, col1], df.iloc[:, col2])
    results.append((col1, col2, t_stat, p_val))
    print(f'Test {col1} vs {col2}: t-statistic={t_stat:.2f}, p-value={p_val:.4f}')

# Flawed conclusion
print(""Based on these tests, we can conclude that Groups A and B have significantly different means, and Groups B and C have significantly different means."")","{'summary': 'Code demonstrates Multiple Testing without Correction in business context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import numpy as np
import pandas as pd
from scipy.stats import ttest_ind, f_oneway
from statsmodels.stats.multitest import multipletests

# Synthetic data
np.random.seed(0)
data = {
    'Group A': np.random.normal(10, 2, 100),
    'Group B': np.random.normal(12, 2, 100),
    'Group C': np.random.normal(11, 2, 100)
}
df = pd.DataFrame(data)

# Perform many tests with Bonferroni correction
_, pvals = [], []
for col1, col2 in [(i, j) for i in range(3) for j in range(i+1, 3)]:
    t_stat, p_val = ttest_ind(df.iloc[:, col1], df.iloc[:, col2])
    pvals.append(p_val)
pvals_corr = multipletests(pvals, method='bonferroni')[1]

# Corrected conclusion
print(""Based on these tests, we can conclude that the p-values after correction are:"")
print(pvals_corr)","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:46:15.528636,llama-3-8b-instruct,v1,False
multiple_testing,critical,social_science,crime rate factors,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Synthetic data
np.random.seed(42)
n_subjects = 100
n_conditions = 5
data = np.random.rand(n_subjects, n_conditions)

# Create a DataFrame
df = pd.DataFrame(data, columns=[f""Condition {i+1}"" for i in range(n_conditions)])

# Perform many statistical tests without adjustment
for i in range(n_conditions):
    for j in range(i+1, n_conditions):
        _, p_value = ttest_ind(df.iloc[:, i], df.iloc[:, j])
        if p_value < 0.05:
            print(f""Significant difference found between Condition {i+1} and Condition {j+1} (p-value: {p_value:.4f})"")

# Flawed conclusion
print(""Based on these many tests, we conclude that there are significant differences between most conditions."")","{'summary': 'Code demonstrates Multiple Testing without Correction in social_science context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Synthetic data
np.random.seed(42)
n_subjects = 100
n_conditions = 5
data = np.random.rand(n_subjects, n_conditions)

# Create a DataFrame
df = pd.DataFrame(data, columns=[f""Condition {i+1}"" for i in range(n_conditions)])

# Perform many statistical tests with adjustment
p_values = []
for i in range(n_conditions):
    for j in range(i+1, n_conditions):
        _, p_value = ttest_ind(df.iloc[:, i], df.iloc[:, j])
        p_values.append(p_value)

# Adjust p-values using the Bonferroni correction
_, adj_p_values, _, _ = multipletests(p_values, alpha=0.05, method='bonferroni')

# Report significant results
significant_results = [f""Condition {i+1} vs. Condition {j+1}: p-value = {p_value:.4f}"" for i, p_value in enumerate(adj_p_values) if p_value < 0.05]
print(""Significant results:"")
print(*significant_results, sep=""\n"")

# Corrected conclusion
print(""Based on these adjusted p-values, we conclude that there are significant differences between the following conditions:"")
print(*significant_results, sep=""\n"")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:47:20.321475,llama-3-8b-instruct,v1,False
multiple_testing,critical,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 20
p = 0.5
X = np.random.binomial(n, p, size=(20, 10))  # 20 patients, 10 treatments
treatments = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']

# Perform multiple t-tests without correction
for i in range(10):
    for j in range(i+1, 10):
        result = ttest_ind(X[:, i], X[:, j])
        print(f""T-test between treatments {treatments[i]} and {treatments[j]}: p-value = {result[1]}"")
        if result[1] < 0.05:
            print(f""Treatment {treatments[i]} is significantly different from treatment {treatments[j]}"")","{'summary': 'Code demonstrates Multiple Testing without Correction in healthcare context', 'detailed_explanation': ""# We're testing the effectiveness of each treatment against every other treatment.\n# We're not adjusting the significance level for multiple comparisons.\n# We're assuming that the p-values are independent, which is not the case.\n# We're hoping that the results are meaningful, despite the lack of correction.\n\n# Our results show that treatments A and B, A and C, and C and D are significantly different.\n# We conclude that these treatments have different effects.\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import ttest_ind\nfrom statsmodels.stats.multitest import multipletests\n\n# Generate synthetic data\nnp.random.seed(0)\nn = 20\np = 0.5\nX = np.random.binomial(n, p, size=(20, 10))  # 20 patients, 10 treatments\ntreatments = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n\n# Perform multiple t-tests with correction\np_values = []\nfor i in range(10):\n    for j in range(i+1, 10):\n        result = ttest_ind(X[:, i], X[:, j])\n        p_values.append(result[1])\n\n# Correct for multiple"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","# Our results show that treatments A and B, A and C, and C and D are significantly different.
# We conclude that these treatments have different effects, but with a more accurate interpretation.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:48:38.582448,llama-3-8b-instruct,v1,False
multiple_testing,critical,social_science,crime rate factors,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 100
p = 10
X = np.random.randn(n, p)
y = np.random.binomial(1, 0.5, n)

# Perform many statistical tests without adjusting significance levels
p_values = []
for i in range(p):
    t_stat, p_val = ttest_ind(X[:, i], y)
    p_values.append(p_val)

# Find the number of significant tests
alpha = 0.05
num_significant = sum([p_val < alpha for p_val in p_values])

print(f""Number of significant tests: {num_significant}"")

# Draw a flawed conclusion
print(""Based on the p-values, we can conclude that at least one of the variables is significantly associated with the outcome."")","{'summary': 'Code demonstrates Multiple Testing without Correction in social_science context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
n = 100
p = 10
X = np.random.randn(n, p)
y = np.random.binomial(1, 0.5, n)

# Perform many statistical tests with Bonferroni correction
p_values = []
for i in range(p):
    t_stat, p_val = ttest_ind(X[:, i], y)
    p_values.append(p_val)

# Correct for multiple testing using the Bonferroni method
alpha = 0.05
_, p_values_corrected = multipletests(p_values, alpha=alpha, method='bonferroni')

# Find the number of significant tests
num_significant = sum([p_val < alpha for p_val in p_values_corrected])

print(f""Number of significant tests: {num_significant}"")

# Draw a corrected conclusion
print(""Based on the p-values, we can conclude that at least one of the variables is significantly associated with the outcome, after correcting for multiple testing."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:49:41.624472,llama-3-8b-instruct,v1,False
multiple_testing,critical,social_science,crime rate factors,python,intermediate,"**Explanation**

The code above is problematic because it performs multiple statistical tests without correcting for the increased family-wise error rate. The code tests the significance of multiple variables (income, education, and occupation) using the t-test, without adjusting the significance level for the number of tests performed. This can lead to a high probability of Type I errors, where we reject the null hypothesis when it is actually true.

The Family-wise error rate increases with multiple comparisons, meaning that the probability of at least one false positive increases with the number of tests performed. In this case, the code performs 3 tests, which increases the probability of at least one false positive. This can lead to incorrect conclusions and wasted resources.

**Corrected Code**","{'summary': 'Code demonstrates Multiple Testing without Correction in social_science context', 'detailed_explanation': '**Explanation of Corrected Code**\n\nThe corrected code uses the `multipletests` function from the `statsmodels` library to correct for multiple testing. This function adjusts the p-values to account for the increased family-wise error rate. The corrected code also uses the Bonferroni correction, which is a common method for controlling the family-wise error rate.\n\nBy using the `multipletests` function, we can ensure that our results are more reliable and less susceptible to Type I errors. This is especially important when performing multiple tests, as it can help us avoid drawing incorrect conclusions.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","# Corrected version
**Explanation**

The code above is problematic because it performs multiple statistical tests without correcting for the increased family-wise error rate. The code tests the significance of multiple variables (income, education, and occupation) using the t-test, without adjusting the significance level for the number of tests performed. This can lead to a high probability of Type I errors, where we reject the null hypothesis when it is actually true.

The Family-wise error rate increases with multiple comparisons, meaning that the probability of at least one false positive increases with the number of tests performed. In this case, the code performs 3 tests, which increases the probability of at least one false positive. This can lead to incorrect conclusions and wasted resources.

**Corrected Code**","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:50:47.944248,llama-3-8b-instruct,v1,False
multiple_testing,critical,education,online vs in-person learning outcomes,python,intermediate,"import numpy as np
import pandas as pd
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_treatments = 5
n_outcomes = 5
data = pd.DataFrame(np.random.rand(n_students, n_treatments), columns=['Treatment_' + str(i) for i in range(1, n_treatments+1)])
data['Outcome'] = np.random.rand(n_students)

# Perform multiple t-tests without correction
p_values = []
for i in range(n_treatments):
    for j in range(i+1, n_treatments):
        t_test = sm.ttest_ind(data['Treatment_' + str(i+1)], data['Treatment_' + str(j+1)])
        p_values.append(t_test.pvalue)

# Print results
print(""P-values:"")
print(p_values)

# Conclusion
print(""Based on these p-values, we can conclude that there is a significant difference between at least two treatments."")","{'summary': 'Code demonstrates Multiple Testing without Correction in education context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import numpy as np
import pandas as pd
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_treatments = 5
n_outcomes = 5
data = pd.DataFrame(np.random.rand(n_students, n_treatments), columns=['Treatment_' + str(i) for i in range(1, n_treatments+1)])
data['Outcome'] = np.random.rand(n_students)

# Perform multiple t-tests with Bonferroni correction
p_values = []
for i in range(n_treatments):
    for j in range(i+1, n_treatments):
        t_test = sm.ttest_ind(data['Treatment_' + str(i+1)], data['Treatment_' + str(j+1)])
        p_value = t_test.pvalue
        # Apply Bonferroni correction
        corrected_p_value = p_value * n_treatments
        p_values.append(corrected_p_value)

# Print results
print(""P-values:"")
print(p_values)

# Conclusion
print(""Based on these corrected p-values, we can conclude that there is a significant difference between at least two treatments with a family-wise error rate of 0.05."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:51:50.132869,llama-3-8b-instruct,v1,False
multiple_testing,critical,education,online vs in-person learning outcomes,python,intermediate,"import numpy as np
import pandas as pd
from scipy.stats import ttest_ind, corrcoef

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_students_per_group = 20
group1_scores = np.random.normal(loc=80, scale=10, size=n_students_per_group)
group2_scores = np.random.normal(loc=90, scale=10, size=n_students_per_group)
student_data = pd.DataFrame({'Group': np.concatenate([np.ones(n_students_per_group), np.zeros(n_students_per_group)]), 
                            'Scores': np.concatenate([group1_scores, group2_scores])})

# Perform many statistical tests without adjusting significance levels
p_values = []
for i in range(10):
    group1_scores_subset = group1_scores[np.random.choice(n_students_per_group, size=10, replace=False)]
    group2_scores_subset = group2_scores[np.random.choice(n_students_per_group, size=10, replace=False)]
    p_value = ttest_ind(group1_scores_subset, group2_scores_subset)[1]
    p_values.append(p_value)

print(""P-values:"", p_values)

# Draw conclusion
print(""We reject the null hypothesis that the means of the two groups are equal with a p-value of"", min(p_values))","{'summary': 'Code demonstrates Multiple Testing without Correction in education context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import numpy as np
import pandas as pd
from scipy.stats import ttest_ind, fwe
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_students_per_group = 20
group1_scores = np.random.normal(loc=80, scale=10, size=n_students_per_group)
group2_scores = np.random.normal(loc=90, scale=10, size=n_students_per_group)
student_data = pd.DataFrame({'Group': np.concatenate([np.ones(n_students_per_group), np.zeros(n_students_per_group)]), 
                            'Scores': np.concatenate([group1_scores, group2_scores])})

# Perform many statistical tests with Bonferroni correction
p_values = []
for i in range(10):
    group1_scores_subset = group1_scores[np.random.choice(n_students_per_group, size=10, replace=False)]
    group2_scores_subset = group2_scores[np.random.choice(n_students_per_group, size=10, replace=False)]
    p_value = ttest_ind(group1_scores_subset, group2_scores_subset)[1]
    p_values.append(p_value)

# Correct for multiple testing using Bonferroni correction
p_values_corrected = multipletests(p_values, alpha=0.05, method='bonferroni')[1]

# Draw conclusion
print(""P-values after correction:"", p_values_corrected)","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:53:02.592589,llama-3-8b-instruct,v1,False
multiple_testing,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import numpy as np
import pandas as pd
from scipy.stats import ttest_ind

# Synthetic data: 20 patients in each of two groups (control and treatment)
np.random.seed(0)
control_outcomes = np.random.normal(10, 2, 20)
treatment_outcomes = np.random.normal(12, 2, 20)

# Create a DataFrame
df = pd.DataFrame({'Control': control_outcomes, 'Treatment': treatment_outcomes})

# Perform multiple tests without adjustment
t_stat, p_val = ttest_ind(control_outcomes, treatment_outcomes)
print(""T-statistic:"", t_stat)
print(""p-value:"", p_val)

# Additional tests without adjustment
t_stat1, p_val1 = ttest_ind(control_outcomes, np.random.normal(11, 2, 20))
print(""T-statistic (additional test 1):"", t_stat1)
print(""p-value (additional test 1):"", p_val1)

t_stat2, p_val2 = ttest_ind(np.random.normal(12, 2, 20), np.random.normal(13, 2, 20))
print(""T-statistic (additional test 2):"", t_stat2)
print(""p-value (additional test 2):"", p_val2)

print(""Conclusion: The treatment group has significantly better outcomes than the control group."")","{'summary': 'Code demonstrates Multiple Testing without Correction in healthcare context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","import numpy as np
import pandas as pd
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Synthetic data: 20 patients in each of two groups (control and treatment)
np.random.seed(0)
control_outcomes = np.random.normal(10, 2, 20)
treatment_outcomes = np.random.normal(12, 2, 20)

# Create a DataFrame
df = pd.DataFrame({'Control': control_outcomes, 'Treatment': treatment_outcomes})

# Perform multiple tests with adjustment
t_stat, p_val = ttest_ind(control_outcomes, treatment_outcomes)
_, p_val_adj = multipletests([p_val], method='bonferroni')

print(""T-statistic:"", t_stat)
print(""p-value:"", p_val)
print(""Adjusted p-value:"", p_val_adj)

# Additional tests with adjustment
p_vals = []
p_vals.append(ttest_ind(control_outcomes, np.random.normal(11, 2, 20))[1])
p_vals.append(ttest_ind(np.random.normal(12, 2, 20), np.random.normal(13, 2, 20))[1])

_, p_val_adj_multiple = multipletests(p_vals, method='bonferroni')

print(""Adjusted p-value (multiple tests):"", p_val_adj_multiple)

print(""Conclusion: The treatment group has significantly better outcomes than the control group, after adjusting for multiple testing."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:54:19.437272,llama-3-8b-instruct,v1,False
multiple_testing,critical,business,pricing strategy impact analysis,python,intermediate,"import numpy as np
import pandas as pd
from scipy.stats import ttest_ind

# Synthetic data generation
np.random.seed(42)
campaign_a = np.random.normal(100, 5, 100)
campaign_b = np.random.normal(105, 5, 100)
control_group = np.random.normal(90, 5, 100)

# Create a DataFrame
df = pd.DataFrame({'campaign_a': campaign_a, 'campaign_b': campaign_b, 'control_group': control_group})

# Perform many tests without adjusting significance levels
for column in df.columns:
    if column!= 'control_group':
        for other_column in df.columns:
            if other_column!= column:
                if column < other_column:
                    t_stat, p_val = ttest_ind(df[column], df[other_column])
                    if p_val < 0.05:
                        print(f""{column} is significantly different from {other_column} (p-value: {p_val:.4f})"")

# Flawed conclusion
print(""Based on these results, we can conclude that campaign A and B have different effects on the control group."")","{'summary': 'Code demonstrates Multiple Testing without Correction in business context', 'detailed_explanation': 'This code demonstrates Multiple Testing without Correction. Family-wise error rate increases with multiple comparisons In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import numpy as np
import pandas as pd
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Synthetic data generation
np.random.seed(42)
campaign_a = np.random.normal(100, 5, 100)
campaign_b = np.random.normal(105, 5, 100)
control_group = np.random.normal(90, 5, 100)

# Create a DataFrame
df = pd.DataFrame({'campaign_a': campaign_a, 'campaign_b': campaign_b, 'control_group': control_group})

# Perform many tests with adjusted significance levels
p_vals = []
for column in df.columns:
    if column!= 'control_group':
        for other_column in df.columns:
            if other_column!= column:
                if column < other_column:
                    t_stat, p_val = ttest_ind(df[column], df[other_column])
                    p_vals.append(p_val)

# Adjust the p-values using the Bonferroni correction
p_vals_adj = multipletests(p_vals, alpha=0.05, method='bonferroni')[1]

# Corrected conclusion
print(""Based on these results, we can conclude that campaign A and B have different effects on the control group, but only if the adjusted p-value is less than the adjusted significance level (p-value: {:.4f})"".format(min(p_vals_adj)))","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Family-wise error rate increases with multiple comparisons']",Family-wise error rate increases with multiple comparisons,2025-12-14T20:55:30.128465,llama-3-8b-instruct,v1,False
p_hacking,critical,social_science,voting behavior analysis,python,intermediate,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
income_data = np.random.normal(50000, 10000, 1000)  # mean income, std, and sample size
demographics = np.random.randint(0, 100, 1000)  # random demographics (0-100)

# Create a DataFrame
df = pd.DataFrame({'Income': income_data, 'Demographics': demographics})

# Perform analysis with flawed approach
p_values = []
for i in range(10):  # iterate 10 times
    subset_df = df.sample(frac=0.8)  # select 80% of the data
    income_mean = subset_df['Income'].mean()
    demographics_mean = subset_df['Demographics'].mean()
    t_stat, p_value = stats.ttest_ind(subset_df['Income'], subset_df['Demographics'])
    p_values.append(p_value)
    if p_value < 0.05:
        print(f""Significant result found! Income and demographics are correlated (p-value: {p_value})"")

# Plot the results
plt.hist(p_values, bins=10)
plt.xlabel(""p-values"")
plt.ylabel(""Frequency"")
plt.title(""Flawed Analysis: Selective Reporting"")
plt.show()

# Conclusion
print(""Based on our analysis, we conclude that there is a significant correlation between income and demographics. This is a groundbreaking finding that highlights the need for further research."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in social_science context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
income_data = np.random.normal(50000, 10000, 1000)  # mean income, std, and sample size
demographics = np.random.randint(0, 100, 1000)  # random demographics (0-100)

# Create a DataFrame
df = pd.DataFrame({'Income': income_data, 'Demographics': demographics})

# Perform analysis with corrected approach
subset_df = df.sample(frac=1)  # select the entire dataset
income_mean = subset_df['Income'].mean()
demographics_mean = subset_df['Demographics'].mean()
t_stat, p_value = ttest_ind(subset_df['Income'], subset_df['Demographics'])
print(f""P-value: {p_value}"")

# Plot the results
plt.hist([p_value], bins=1)
plt.xlabel(""p-value"")
plt.ylabel(""Frequency"")
plt.title(""Corrected Analysis: Single Analysis"")
plt.show()

# Conclusion
print(""Based on our analysis, we conclude that the p-value is 0.05, indicating no significant correlation between income and demographics."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T20:56:38.418834,llama-3-8b-instruct,v1,False
p_hacking,critical,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
grad_rate = np.random.normal(loc=0.5, scale=0.1, size=(100, 2))
demographics = ['Male', 'Female', 'Asian', 'Black', 'White', 'Hispanic', 'Other']
df = pd.DataFrame(np.concatenate((grad_rate, demographics)), columns=['Grad_Rate', 'Demographics'])

# Perform analysis
results = []
for i in range(len(demographics)):
    for j in range(i+1, len(demographics)):
        if ttest_ind(df.loc[df['Demographics'] == demographics[i], 'Grad_Rate'], df.loc[df['Demographics'] == demographics[j], 'Grad_Rate'])[1] < 0.05:
            results.append((demographics[i], demographics[j], 'Significant difference'))
            print(f""{demographics[i]} and {demographics[j]} have a significant difference in graduation rate."")

# Flawed conclusion
print(""Our analysis suggests that there are significant differences in graduation rates among males and females, as well as among Asians and Blacks. We recommend targeting these groups for further intervention."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in education context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import pandas as pd
import numpy as np
from scipy.stats import f_oneway
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
grad_rate = np.random.normal(loc=0.5, scale=0.1, size=(100, 2))
demographics = ['Male', 'Female', 'Asian', 'Black', 'White', 'Hispanic', 'Other']
df = pd.DataFrame(np.concatenate((grad_rate, demographics)), columns=['Grad_Rate', 'Demographics'])

# Perform analysis
results = []
for i in range(len(demographics)):
    for j in range(i+1, len(demographics)):
        if f_oneway(df.loc[df['Demographics'] == demographics[i], 'Grad_Rate'], df.loc[df['Demographics'] == demographics[j], 'Grad_Rate'])[1] < 0.05:
            results.append((demographics[i], demographics[j], 'Significant difference'))

# Corrected conclusion
_, pvals, _, _ = multipletests([f_oneway(df.loc[df['Demographics'] == demographics[i], 'Grad_Rate'], df.loc[df['Demographics'] == demographics[j], 'Grad_Rate'])[1] for i in range(len(demographics)) for j in range(i+1, len(demographics))])
print(""After correcting for multiple comparisons, we found that only the difference between males and females is statistically significant."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T20:57:47.456861,llama-3-8b-instruct,v1,False
p_hacking,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data for a clinical trial
np.random.seed(0)
n_subjects = 100
n_groups = 2
drug_effects = np.random.normal(0, 1, n_subjects)
control_effects = np.random.normal(0, 1, n_subjects)

# Split data into groups
treatment = np.concatenate((np.random.normal(drug_effects + 2, 1, n_subjects//2), np.random.normal(drug_effects - 2, 1, n_subjects//2)))
control = np.concatenate((np.random.normal(control_effects + 1, 1, n_subjects//2), np.random.normal(control_effects - 1, 1, n_subjects//2)))

# Create a DataFrame
df = pd.DataFrame({'treatment': treatment, 'control': control})

# Perform analysis
treatment_mean = np.mean(treatment)
control_mean = np.mean(control)

# Selectively report significant results
if np.abs(treatment_mean - control_mean) > 2:
    print(""The treatment group shows a significant difference in outcomes (p-value = 0.01)!"")
else:
    print(""No significant difference found."")

# Visualize results
plt.hist(treatment, alpha=0.5, label='Treatment')
plt.hist(control, alpha=0.5, label='Control')
plt.legend()
plt.show()","{'summary': 'Code demonstrates P-hacking / Data Dredging in healthcare context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Generate synthetic data for a clinical trial
np.random.seed(0)
n_subjects = 100
n_groups = 2
drug_effects = np.random.normal(0, 1, n_subjects)
control_effects = np.random.normal(0, 1, n_subjects)

# Split data into groups
treatment = np.concatenate((np.random.normal(drug_effects + 2, 1, n_subjects//2), np.random.normal(drug_effects - 2, 1, n_subjects//2)))
control = np.concatenate((np.random.normal(control_effects + 1, 1, n_subjects//2), np.random.normal(control_effects - 1, 1, n_subjects//2)))

# Create a DataFrame
df = pd.DataFrame({'treatment': treatment, 'control': control})

# Perform analysis using a proper statistical test
t_stat, p_val = ttest_ind(treatment, control)

# Report results honestly
print(""The treatment group shows a significant difference in outcomes (t-statistic = {:.2f}, p-value = {:.4f)"".format(t_stat, p_val))

# Visualize results
plt.hist(treatment, alpha=0.5, label='Treatment')
plt.hist(control, alpha=0.5, label='Control')
plt.legend()
plt.show()","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T20:59:03.853552,llama-3-8b-instruct,v1,False
p_hacking,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n_samples = 100
churn_data = pd.DataFrame({
    'age': np.random.normal(35, 5, n_samples),
    'income': np.random.normal(50000, 10000, n_samples),
    'churn': np.random.binomial(1, 0.3, n_samples)  # 30% churn rate
})

# Perform analysis
t_stat, p_val = ttest_ind(churn_data['age'][churn_data['churn'] == 1], churn_data['age'][churn_data['churn'] == 0])
print(f""p-value: {p_val:.4f}"")

# Selectively report significant results
if p_val < 0.05:
    print(""Age is significantly associated with churn!"")
else:
    print(""Age is not significantly associated with churn."")

# Manipulate analysis
churn_data['age_group'] = np.where(churn_data['age'] < 30, 'young', 'old')
t_stat, p_val = ttest_ind(churn_data['age_group'][churn_data['churn'] == 1], churn_data['age_group'][churn_data['churn'] == 0])
print(f""p-value: {p_val:.4f}"")

# Selectively report significant results again
if p_val < 0.05:
    print(""Age group is significantly associated with churn!"")
else:
    print(""Age group is not significantly associated with churn."")

# Flawed conclusion
print(""Based on our analysis, we conclude that age is a significant predictor of churn. Therefore, we should target our marketing efforts at young customers."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in business context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of customer churn prediction analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in customer churn prediction analysis'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from statsmodels.formula.api import ols

# Generate synthetic data
np.random.seed(0)
n_samples = 100
churn_data = pd.DataFrame({
    'age': np.random.normal(35, 5, n_samples),
    'income': np.random.normal(50000, 10000, n_samples),
    'churn': np.random.binomial(1, 0.3, n_samples)  # 30% churn rate
})

# Perform analysis
model = ols('churn ~ age + income', data=churn_data).fit()
print(f""p-value for age: {model.pvalues['age']:.4f}"")

# Report all results
print(""Results:"")
print(f""Age: {model.pvalues['age']:.4f}"")
print(f""Income: {model.pvalues['income']:.4f}"")

# Flawless conclusion
print(""Based on our analysis, we conclude that age and income are both significant predictors of churn. Therefore, we should consider both variables when developing our marketing strategy."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:00:13.311390,llama-3-8b-instruct,v1,False
p_hacking,critical,business,customer churn prediction analysis,python,intermediate,"**Explanation**

This code is problematic because it commits the error of selective reporting, also known as data dredging or p-hacking. The analysis involves performing multiple tests (in this case, t-tests) and only reporting the results that achieve statistical significance (p-value < 0.05). This approach inflates the Type I error rate, as the researcher is more likely to find significant results by chance, and ignores the results that do not meet the desired significance threshold.

The principle of selection bias in reporting inflates Type I error rates because it selectively presents only the most favorable results, ignoring the possibility that the results may be due to chance. This can lead to overestimation of the importance of certain features or variables and may result in drawing incorrect conclusions.

**Corrected Code**","{'summary': 'Code demonstrates P-hacking / Data Dredging in business context', 'detailed_explanation': '**Explanation**\n\nThe corrected code avoids the error of selective reporting by using a more robust method for multiple comparisons, such as the F-test, and reporting all results regardless of significance. This approach ensures that the analysis is objective and unbiased, and the results are more likely to be accurate.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in customer churn prediction analysis'}","# Corrected version
**Explanation**

This code is problematic because it commits the error of selective reporting, also known as data dredging or p-hacking. The analysis involves performing multiple tests (in this case, t-tests) and only reporting the results that achieve statistical significance (p-value < 0.05). This approach inflates the Type I error rate, as the researcher is more likely to find significant results by chance, and ignores the results that do not meet the desired significance threshold.

The principle of selection bias in reporting inflates Type I error rates because it selectively presents only the most favorable results, ignoring the possibility that the results may be due to chance. This can lead to overestimation of the importance of certain features or variables and may result in drawing incorrect conclusions.

**Corrected Code**","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:01:16.693149,llama-3-8b-instruct,v1,False
p_hacking,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n_samples = 100
income_data = np.random.normal(10000, 5000, n_samples)
control_group = np.random.normal(8000, 4000, n_samples)
treatment_group = np.random.normal(12000, 6000, n_samples)

# Create a DataFrame
df = pd.DataFrame({'Income': np.concatenate((income_data, control_group, treatment_group)),
                  'Group': np.concatenate((np.zeros(n_samples), np.ones(n_samples), np.zeros(n_samples)))})

# Perform analysis
t_stat, p_val = ttest_ind(income_data, control_group)
print(f""P-value: {p_val}"")

# Selectively report significant results
if p_val < 0.05:
    print(""We found a statistically significant difference between income groups!"")
else:
    print(""No significant difference found."")

# Visualize the data (with misleading caption)
plt.scatter(df['Group'], df['Income'])
plt.xlabel('Group')
plt.ylabel('Income')
plt.title(""Income Distribution by Group (p < 0.05)"")
plt.show()","{'summary': 'Code demonstrates P-hacking / Data Dredging in social_science context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n_samples = 100
income_data = np.random.normal(10000, 5000, n_samples)
control_group = np.random.normal(8000, 4000, n_samples)
treatment_group = np.random.normal(12000, 6000, n_samples)

# Create a DataFrame
df = pd.DataFrame({'Income': np.concatenate((income_data, control_group, treatment_group)),
                  'Group': np.concatenate((np.zeros(n_samples), np.ones(n_samples), np.zeros(n_samples)))})

# Perform analysis
t_stat, p_val = ttest_ind(income_data, control_group)
print(f""P-value: {p_val}"")

# Report all results, regardless of significance
print(f""T-statistic: {t_stat}, P-value: {p_val}"")

# Visualize the data (with accurate caption)
plt.scatter(df['Group'], df['Income'])
plt.xlabel('Group')
plt.ylabel('Income')
plt.title(""Income Distribution by Group"")
plt.show()

# Report the results in a clear and transparent manner
print(""The results of the t-test indicate a p-value of"", p_val, "". However, we cannot conclude that the difference between income groups is statistically significant based on this single test. Further analysis is needed to draw meaningful conclusions."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:02:21.781992,llama-3-8b-instruct,v1,False
p_hacking,critical,education,student performance prediction,python,intermediate,"import pandas as pd
import numpy as np
import scipy.stats as stats

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_teachers = 10
data = pd.DataFrame({
    'Student_ID': np.arange(1, n_students+1),
    'Teacher_ID': np.random.randint(1, n_teachers+1, size=n_students),
    'Score': np.random.normal(50, 10, n_students),
    'Method': np.random.choice(['Method A', 'Method B', 'Method C'], size=n_students)
})

# Perform analysis
from scipy.stats import ttest_ind

# Selective reporting of significant results
significant_results = []
for teacher in np.unique(data['Teacher_ID']):
    teacher_data = data[data['Teacher_ID'] == teacher]
    for method in np.unique(teacher_data['Method']):
        other_method = [m for m in np.unique(teacher_data['Method']) if m!= method][0]
        t_stat, p_val = ttest_ind(teacher_data[teacher_data['Method'] == method]['Score'], 
                                  teacher_data[teacher_data['Method'] == other_method]['Score'])
        if p_val < 0.05:
            significant_results.append((teacher, method, p_val))

print(""Method A is significantly better than Method B for Teacher 3 with p-value:"", significant_results[0][2])

# Flawed conclusion
print(""Based on this analysis, we conclude that Method A is the most effective teaching method."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in education context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of student performance prediction, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in student performance prediction'}","import pandas as pd
import numpy as np
import scipy.stats as stats
from scipy.stats import f_oneway

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_teachers = 10
data = pd.DataFrame({
    'Student_ID': np.arange(1, n_students+1),
    'Teacher_ID': np.random.randint(1, n_teachers+1, size=n_students),
    'Score': np.random.normal(50, 10, n_students),
    'Method': np.random.choice(['Method A', 'Method B', 'Method C'], size=n_students)
})

# Correct approach
methods = ['Method A', 'Method B', 'Method C']
p_vals = []
for i in range(len(methods)):
    for j in range(i+1, len(methods)):
        t_stat, p_val = f_oneway(data[data['Method'] == methods[i]]['Score'], 
                                 data[data['Method'] == methods[j]]['Score'])
        p_vals.append((methods[i], methods[j], p_val))

# Report all p-values
print(""P-values for pairwise comparisons:"")
print(pd.DataFrame(p_vals, columns=['Method 1', 'Method 2', 'P-value']))

# Corrected conclusion
print(""Based on this analysis, we conclude that there is no significant difference in effectiveness between the teaching methods."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:03:37.027521,llama-3-8b-instruct,v1,False
p_hacking,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Age': np.random.randint(18, 100, 1000),
                     'Gender': np.random.choice(['Male', 'Female'], 1000),
                     'Disease': np.random.choice([0, 1], 1000, p=[0.05, 0.95]),
                     'Treatment': np.random.choice(['A', 'B', 'C'], 1000)})

# Select only patients who received treatment A
treatment_a = data[data['Treatment'] == 'A']

# Perform analysis
t_stat, p_val = ttest_ind(treatment_a['Disease'], treatment_a['Disease'], equal_var=False)
print(f""T-statistic: {t_stat}, P-value: {p_val}"")

# Selectively report significant results
if p_val < 0.05:
    print(""There is a statistically significant difference in disease prevalence between patients receiving treatment A and not receiving treatment A."")
else:
    print(""No statistically significant difference found."")

# Draw a conclusion
print(""Based on this analysis, we can conclude that treatment A is effective in reducing disease prevalence."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in healthcare context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Age': np.random.randint(18, 100, 1000),
                     'Gender': np.random.choice(['Male', 'Female'], 1000),
                     'Disease': np.random.choice([0, 1], 1000, p=[0.05, 0.95]),
                     'Treatment': np.random.choice(['A', 'B', 'C'], 1000)})

# Correctly perform multiple comparisons
treatment_a = data[data['Treatment'] == 'A']
treatment_b = data[data['Treatment'] == 'B']
treatment_c = data[data['Treatment'] == 'C']

p_values = []
for treatment in [treatment_a, treatment_b, treatment_c]:
    t_stat, p_val = ttest_ind(treatment['Disease'], treatment['Disease'], equal_var=False)
    p_values.append(p_val)

# Correctly correct for multiple comparisons
p_values_corr, _ = multipletests(p_values, alpha=0.05, method='bonferroni')

# Report results
for i, p_val in enumerate(p_values_corr):
    if p_val < 0.05:
        print(f""There is a statistically significant difference in disease prevalence between patients receiving treatment {i+1} and not receiving treatment {i+1}."")
    else:
        print(f""No statistically significant difference found between patients receiving treatment {i+1} and not receiving treatment {i+1}."")

# Draw a conclusion
print(""Based on this analysis, we can conclude that treatment A and treatment C show statistically significant differences in disease prevalence."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:04:54.345792,llama-3-8b-instruct,v1,False
p_hacking,critical,business,customer churn prediction analysis,python,intermediate,"import numpy as np
import pandas as pd
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
X = np.random.normal(size=(n_samples, 5))
y = np.random.normal(size=n_samples)

# Perform analysis
X = pd.DataFrame(X, columns=['x1', 'x2', 'x3', 'x4', 'x5'])
y = pd.Series(y)

# Selective reporting
results = []
for i in range(100):
    X_sub = X[:, np.random.choice(X.shape[1], 3, replace=False)]
    y_sub = y
    X_sub = sm.add_constant(X_sub)
    model = sm.OLS(y_sub, X_sub).fit()
    if model.pvalues[1] < 0.05:  # Select only significant results
        results.append((model.params[1], model.rsquared))

# Flawed conclusion
print(""The results show that at least one of the features is significantly related to the outcome, with a p-value of less than 0.05."")
print(""We conclude that the campaign is effective in increasing conversions."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in business context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of customer churn prediction analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in customer churn prediction analysis'}","import numpy as np
import pandas as pd
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
X = np.random.normal(size=(n_samples, 5))
y = np.random.normal(size=n_samples)

# Perform analysis
X = pd.DataFrame(X, columns=['x1', 'x2', 'x3', 'x4', 'x5'])
y = pd.Series(y)

# Correct approach
results = []
for i in range(100):
    X_sub = X[:, np.random.choice(X.shape[1], 3, replace=False)]
    y_sub = y
    X_sub = sm.add_constant(X_sub)
    model = sm.OLS(y_sub, X_sub).fit()
    results.append((model.params[1], model.rsquared))

# Best practices
print(""The results show that multiple regression analyses were performed with different feature combinations."")
print(""The p-values for each feature were calculated, but the results were not selectively reported."")
print(""A more comprehensive analysis would involve reporting the p-values and confidence intervals for each feature, as well as using techniques like cross-validation to validate the results."")

# Correct conclusion
print(""The results do not provide conclusive evidence of the effectiveness of the marketing campaign."")
print(""Further analysis and investigation are needed to determine the impact of the campaign."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:06:03.004547,llama-3-8b-instruct,v1,False
p_hacking,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
demographics = pd.DataFrame({'Age': np.random.randint(18, 100, 1000), 
                            'Sex': np.random.choice(['Male', 'Female'], 1000), 
                            'Disease': np.random.choice(['Yes', 'No'], 1000, p=[0.2, 0.8])})

# Perform analysis
t_test_results = []
for i in range(1, 6):
    for j in range(i+1, 6):
        if i!= j:
            age_group1 = demographics[demographics['Age'] <= i*20]
            age_group2 = demographics[demographics['Age'] > i*20]
            sex_group1 = demographics[demographics['Sex'] == 'Male']
            sex_group2 = demographics[demographics['Sex'] == 'Female']
            disease_group1 = demographics[demographics['Disease'] == 'Yes']
            disease_group2 = demographics[demographics['Disease'] == 'No']
            t_stat, p_val = ttest_ind(age_group1['Disease'].values, age_group2['Disease'].values)
            t_test_results.append((i, j, t_stat, p_val))
            t_stat, p_val = ttest_ind(sex_group1['Disease'].values, sex_group2['Disease'].values)
            t_test_results.append((i, j, t_stat, p_val))
            t_stat, p_val = ttest_ind(disease_group1['Age'].values, disease_group2['Age'].values)
            t_test_results.append((i, j, t_stat, p_val))

# Selectively report significant results
significant_results = [row for row in t_test_results if row[3] < 0.05]
print(""Significant results:"")
for row in significant_results:
    print(f""Age group {row[0]} vs. {row[1]}: p-value = {row[3]}"")

# Flawed conclusion
print(""Conclusion: The results suggest that there is a significant difference in disease prevalence between age groups and between sexes."")
print(""However, the actual analysis was flawed due to data dredging and selective reporting of results."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in healthcare context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
demographics = pd.DataFrame({'Age': np.random.randint(18, 100, 1000), 
                            'Sex': np.random.choice(['Male', 'Female'], 1000), 
                            'Disease': np.random.choice(['Yes', 'No'], 1000, p=[0.2, 0.8])})

# Perform analysis
t_test_results = []
for i in range(1, 6):
    for j in range(i+1, 6):
        if i!= j:
            age_group1 = demographics[demographics['Age'] <= i*20]
            age_group2 = demographics[demographics['Age'] > i*20]
            sex_group1 = demographics[demographics['Sex'] == 'Male']
            sex_group2 = demographics[demographics['Sex'] == 'Female']
            disease_group1 = demographics[demographics['Disease'] == 'Yes']
            disease_group2 = demographics[demographics['Disease'] == 'No']
            t_stat, p_val = ttest_ind(age_group1['Disease'].values, age_group2['Disease'].values)
            t_test_results.append((i, j, t_stat, p_val))
            t_stat, p_val = ttest_ind(sex_group1['Disease'].values, sex_group2['Disease'].values)
            t_test_results.append((i, j, t_stat, p_val))
            t_stat, p_val = ttest_ind(disease_group1['Age'].values, disease_group2['Age'].values)
            t_test_results.append((i, j, t_stat, p_val))

# Correctly correct for multiple testing
_, p_values_corr = multipletests([row[3] for row in t_test_results], alpha=0.05)

# Report results
print(""Significant results:"")
for i, row in enumerate(t_test_results):
    if p_values_corr[i] < 0.05:
        print(f""Age group {row[0]} vs. {row[1]}: p-value = {row[3]}"")

# Corrected conclusion
print(""Conclusion: The results suggest that there is a significant difference in disease prevalence between age groups and between sexes, after correcting for multiple testing."")
print(""The actual analysis was performed using a correct approach, avoiding data dredging and selective reporting of results."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:07:51.746662,llama-3-8b-instruct,v1,False
p_hacking,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
import numpy as np
import statsmodels.api as sm

# Generate synthetic data for clinical trial
np.random.seed(0)
n = 100
x = np.random.normal(size=n)
y = 2*x + np.random.normal(size=n, scale=1)

# Perform analysis with selective reporting
p_values = []
for i in range(1000):
    # Perform multiple regressions with different independent variables
    X = np.random.choice([x, x**2, x**3], size=1)[0]
    model = sm.OLS(y, sm.add_constant(X)).fit()
    p_values.append(model.f_test('x').pvalue)

# Select only significant results
p_values = [p for p in p_values if p < 0.05]
print(f""Significant p-values: {p_values}"")

# Draw conclusions
print(""The results suggest that the drug is effective at p < 0.05."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in healthcare context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt

# Generate synthetic data for clinical trial
np.random.seed(0)
n = 100
x = np.random.normal(size=n)
y = 2*x + np.random.normal(size=n, scale=1)

# Perform analysis with multiple testing correction
p_values = []
for i in range(1000):
    # Perform multiple regressions with different independent variables
    X = np.random.choice([x, x**2, x**3], size=1)[0]
    model = sm.OLS(y, sm.add_constant(X)).fit()
    p_values.append(model.f_test('x').pvalue)

# Correct for multiple testing using Bonferroni correction
alpha = 0.05
corrected_p_values = [p for p in p_values if p < alpha / len(p_values)]
print(f""Corrected significant p-values: {corrected_p_values}"")

# Visualize results
plt.hist(p_values, bins='fd')
plt.xlabel('P-values')
plt.ylabel('Frequency')
plt.title('Distribution of P-values')
plt.show()

# Draw conclusions
print(""The results suggest that the drug is effective, but with a corrected p-value."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:08:49.699233,llama-3-8b-instruct,v1,False
p_hacking,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 100
demographics = ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female']
disease_prevalence = np.random.binomial(1, 0.3, size=n)
age = np.random.randint(18, 100, size=n)
gender = [demographics[i] for i in np.random.permutation(n)]

# Perform analysis
male_prevalence = disease_prevalence[gender == 'Male']
female_prevalence = disease_prevalence[gender == 'Female']
t_stat, p_val = ttest_ind(male_prevalence, female_prevalence)

# Selectively report significant results
if p_val < 0.05:
    print(""There is a statistically significant difference in disease prevalence between males and females (p ="", p_val, "")."")
else:
    print(""No statistically significant difference found."")

# Conclusion
print(""Based on this analysis, we can conclude that there is a significant difference in disease prevalence between males and females."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in healthcare context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from scipy.stats import f_oneway

# Generate synthetic data
np.random.seed(0)
n = 100
demographics = ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female']
disease_prevalence = np.random.binomial(1, 0.3, size=n)
age = np.random.randint(18, 100, size=n)
gender = [demographics[i] for i in np.random.permutation(n)]

# Perform analysis
male_prevalence = disease_prevalence[gender == 'Male']
female_prevalence = disease_prevalence[gender == 'Female']

# Perform ANOVA to compare disease prevalence across demographics
f_stat, p_val = f_oneway(male_prevalence, female_prevalence)

# Report all results, regardless of significance
print(""The p-value for the ANOVA is"", p_val)

# Conclusion
print(""Based on this analysis, we can conclude that the disease prevalence is"", ""statistically significant"" if p_val < 0.05 else ""not statistically significant"", ""across demographics."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:09:56.986938,llama-3-8b-instruct,v1,False
p_hacking,critical,social_science,income inequality trends,python,intermediate,"import numpy as np
import pandas as pd
import statsmodels.api as sm

# Generate synthetic data (n = 1000)
np.random.seed(42)
x1 = np.random.normal(0, 1, 1000)
x2 = np.random.normal(0, 1, 1000)
y = 0.5 * x1 + np.random.normal(0, 0.5, 1000)

# Fit multiple models and select the one with the lowest p-value
models = []
p_values = []
for i in range(10):
    model = sm.OLS(y, sm.add_constant(x1) + sm.add_constant(x2) + sm.add_constant(x1*x2))
    results = model.fit()
    p_values.append(results.f_test().pvalue)
    if p_values[-1] < 0.05:
        best_model = model
        break

# Selectively report the significant result
print(""The relationship between x1 and y is significant (p-value = {:.4f})"".format(p_values[-1]))

# Conclude based on the flawed result
print(""Our analysis shows a strong correlation between x1 and y, which is a crucial factor in predicting voting behavior."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in social_science context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of income inequality trends, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in income inequality trends'}","import numpy as np
import pandas as pd
import statsmodels.api as sm

# Generate synthetic data (n = 1000)
np.random.seed(42)
x1 = np.random.normal(0, 1, 1000)
x2 = np.random.normal(0, 1, 1000)
y = 0.5 * x1 + np.random.normal(0, 0.5, 1000)

# Perform a single analysis with a pre-specified model
model = sm.OLS(y, sm.add_constant(x1))
results = model.fit()
p_value = results.f_test().pvalue

# Report the results honestly
print(""The relationship between x1 and y has a p-value of {:.4f}"".format(p_value))

# Conclude based on the honest result
print(""Our analysis shows that the relationship between x1 and y is not statistically significant (p-value = {:.4f})"".format(p_value))

# Note: If the result is not statistically significant, consider exploring alternative models or collecting additional data to improve the analysis.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:11:01.083105,llama-3-8b-instruct,v1,False
p_hacking,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'voted': np.random.binomial(1, 0.5, size=1000),
                     'age': np.random.normal(30, 10, size=1000),
                     'education': np.random.choice(['high', 'low','middle'], size=1000, p=[0.4, 0.3, 0.3])})

# Perform analysis
p_values = []
for variable in ['age', 'education']:
    t_stat, p_value = ttest_ind(data['voted'], data.groupby(variable)['voted'].transform(lambda x: x).values, equal_var=False)
    p_values.append(p_value)

# Selectively report significant results
significant_results = [f""{variable}: {p_value:.3f}"" for p_value in p_values if p_value < 0.05]
print(""Significant results:"", significant_results)

# Flawed conclusion
print(""Conclusion: The analysis suggests that both age and education are significant predictors of voting behavior."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in social_science context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'voted': np.random.binomial(1, 0.5, size=1000),
                     'age': np.random.normal(30, 10, size=1000),
                     'education': np.random.choice(['high', 'low','middle'], size=1000, p=[0.4, 0.3, 0.3])})

# Perform analysis
p_values = []
for variable in ['age', 'education']:
    t_stat, p_value = ttest_ind(data['voted'], data.groupby(variable)['voted'].transform(lambda x: x).values, equal_var=False)
    p_values.append(p_value)

# Correct for multiple comparisons using the Bonferroni method
_, corrected_p_values, _, _ = multipletests(np.array(p_values), alpha=0.05, method='bonferroni')

# Report all results, not just the significant ones
print(""Results:"")
for variable, p_value in zip(['age', 'education'], corrected_p_values):
    print(f""{variable}: {p_value:.3f}"")

# Corrected conclusion
print(""Conclusion: The analysis suggests that the relationship between age and voting behavior is significant, but the relationship between education and voting behavior is not significant."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:12:06.162029,llama-3-8b-instruct,v1,False
p_hacking,critical,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_variables = 10
data = pd.DataFrame(np.random.rand(n_students, n_variables), columns=[f'feature_{i}' for i in range(n_variables)])
target = np.random.rand(n_students)

# Perform analysis
corrs = []
for i in range(n_variables):
    corr, _ = pearsonr(data.iloc[:, i], target)
    if corr > 0.5:
        print(f""Feature {i} is significantly correlated with the target (p-value < 0.05)!"")
        corrs.append((corr, i))

print(""The top 3 features correlated with the target are:"")
print(sorted(corrs, key=lambda x: x[0], reverse=True)[:3])

# Flawed conclusion
print(""Based on our analysis, we can conclude that features 3, 5, and 7 are the most important predictors of student performance."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in education context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import pandas as pd
import numpy as np
from scipy.stats import f_oneway

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_variables = 10
data = pd.DataFrame(np.random.rand(n_students, n_variables), columns=[f'feature_{i}' for i in range(n_variables)])
target = np.random.rand(n_students)

# Perform analysis
corrs = []
for i in range(n_variables):
    corr, _ = pearsonr(data.iloc[:, i], target)
    if corr > 0.5:
        print(f""Feature {i} is significantly correlated with the target (p-value < 0.05)!"")
        corrs.append((corr, i))

# Perform multiple testing correction
corrs = [x for x in corrs if x[0] > 0.5]
print(""The top 3 features correlated with the target are:"")
print(sorted(corrs, key=lambda x: x[0], reverse=True)[:3])

# Corrected conclusion
print(""Based on our analysis, we can conclude that features 3, 5, and 7 are the most important predictors of student performance."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:13:13.842413,llama-3-8b-instruct,v1,False
p_hacking,critical,education,teaching method effectiveness comparison,python,intermediate,"import numpy as np
import pandas as pd
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
n = 100
online_outcomes = np.random.normal(0, 1, n)
in_person_outcomes = np.random.normal(0, 1, n)
df = pd.DataFrame({'online_outcomes': online_outcomes, 'in_person_outcomes': in_person_outcomes})

# Perform analysis
X = sm.add_constant(df['online_outcomes'])
model = sm.OLS(df['in_person_outcomes'], X).fit()
print(model.f_test(X[1]).pvalue)  # p-value = 0.03

# Selectively report significant results
print(""The p-value is significant, so we can conclude that online learning outcomes are significantly different from in-person learning outcomes."")

# Manipulate analysis
X = sm.add_constant(df['in_person_outcomes'])
model = sm.OLS(df['online_outcomes'], X).fit()
print(model.f_test(X[1]).pvalue)  # p-value = 0.05
print(""The p-value is not significant, so we will ignore this result and stick with our original conclusion."")

# Flawed conclusion
print(""Our study shows that online learning outcomes are significantly better than in-person learning outcomes, with a p-value of 0.03."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in education context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import numpy as np
import pandas as pd
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
n = 100
online_outcomes = np.random.normal(0, 1, n)
in_person_outcomes = np.random.normal(0, 1, n)
df = pd.DataFrame({'online_outcomes': online_outcomes, 'in_person_outcomes': in_person_outcomes})

# Perform analysis
X = sm.add_constant(df['online_outcomes'])
model = sm.OLS(df['in_person_outcomes'], X).fit()
p_value = model.f_test(X[1]).pvalue

# Report all results
print(f""The p-value is {p_value:.3f}."")
print(""We will not selectively report significant results or manipulate the analysis."")

# Corrected conclusion
print(""Our study found a p-value of 0.03, which suggests that online learning outcomes may be significantly different from in-person learning outcomes."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:14:15.213363,llama-3-8b-instruct,v1,False
p_hacking,critical,social_science,income inequality trends,python,intermediate,"import pandas as pd
import numpy as np
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
n = 100
x1 = np.random.normal(0, 1, n)
x2 = np.random.normal(0, 1, n)
y = 0.5 * x1 + 0.3 * x2 + np.random.normal(0, 1, n)

# Create a dataframe
df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})

# Perform multiple linear regression
X = sm.add_constant(df[['x1', 'x2']])
model = sm.OLS(df['y'], X).fit()
print(model.summary())

# Selectively report significant results
if model.pvalues[0] < 0.05:
    print(""x1 is significant!"")
else:
    print(""x1 is not significant."")

if model.pvalues[1] < 0.05:
    print(""x2 is significant!"")
else:
    print(""x2 is not significant."")

# Draw a conclusion
if model.pvalues[0] < 0.05 and model.pvalues[1] < 0.05:
    print(""Both x1 and x2 are significant predictors of y!"")
else:
    print(""Only one or neither of x1 and x2 is significant."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in social_science context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of income inequality trends, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in income inequality trends'}","import pandas as pd
import numpy as np
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
n = 100
x1 = np.random.normal(0, 1, n)
x2 = np.random.normal(0, 1, n)
y = 0.5 * x1 + 0.3 * x2 + np.random.normal(0, 1, n)

# Create a dataframe
df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})

# Perform multiple linear regression
X = sm.add_constant(df[['x1', 'x2']])
model = sm.OLS(df['y'], X).fit()
print(model.summary())

# Report all results, without selectively reporting significance
print(""The p-values for x1 and x2 are"", model.pvalues)

# Draw a conclusion based on all results
print(""x1 and x2 are both significant predictors of y, with p-values of"", model.pvalues)","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:15:19.424801,llama-3-8b-instruct,v1,False
p_hacking,critical,education,graduation rate analysis by demographics,python,intermediate,"import numpy as np
import pandas as pd
import scipy.stats as stats

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_groups = 5
group_sizes = np.random.randint(10, 20, size=n_groups)
group1_scores = np.random.normal(loc=50, scale=5, size=sum(group_sizes))
group2_scores = np.random.normal(loc=60, scale=5, size=sum(group_sizes))
data = pd.DataFrame({'Group': np.concatenate([np.repeat(i, size) for i, size in enumerate(group_sizes)]),
                     'Score': np.concatenate([group1_scores[:sum(group_sizes[:i])] + group2_scores[sum(group_sizes[:i]):]])})

# Perform analysis
from scipy.stats import ttest_ind
results = []
for i in range(n_groups):
    group1_scores_slice = group1_scores[:sum(group_sizes[:i+1])]
    group2_scores_slice = group2_scores[:sum(group_sizes[:i+1])]
    t_stat, p_val = ttest_ind(group1_scores_slice, group2_scores_slice)
    results.append((i, t_stat, p_val))
    if p_val < 0.05:
        print(f""Group {i+1} shows significant difference! (p-value: {p_val:.4f})"")
        print(f""The teaching method in Group {i+1} is more effective!"")

# Flawed conclusion
print(""Based on our analysis, we conclude that the teaching method in Groups 2, 3, and 4 are more effective."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in education context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","import numpy as np
import pandas as pd
import scipy.stats as stats
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_groups = 5
group_sizes = np.random.randint(10, 20, size=n_groups)
group1_scores = np.random.normal(loc=50, scale=5, size=sum(group_sizes))
group2_scores = np.random.normal(loc=60, scale=5, size=sum(group_sizes))
data = pd.DataFrame({'Group': np.concatenate([np.repeat(i, size) for i, size in enumerate(group_sizes)]),
                     'Score': np.concatenate([group1_scores[:sum(group_sizes[:i])] + group2_scores[sum(group_sizes[:i]):]])})

# Perform analysis
from statsmodels.stats.multicomp import pairwise_tukeyhsd
f, p = pairwise_tukeyhsd(endog=data['Score'], groups=data['Group'], alpha=0.05)
print(f""p-value: {p:.4f}"")

# Corrected conclusion
print(f""The null hypothesis of no difference between groups cannot be rejected at the significance level of 0.05."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:16:28.239768,llama-3-8b-instruct,v1,False
p_hacking,critical,business,pricing strategy impact analysis,python,intermediate,"import numpy as np
import pandas as pd
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 100
p = 0.5
X1 = np.random.binomial(n, p, size=n)
X2 = np.random.binomial(n, p, size=n)

# Create a dataframe
df = pd.DataFrame({'X1': X1, 'X2': X2})

# Perform analysis
X1_mean = X1.mean()
X2_mean = X2.mean()
t_stat, p_val = ttest_ind(X1, X2)

# Selectively report significant results
if p_val < 0.05:
    print(""We found a significant difference in means! X1 is significantly higher than X2."")
else:
    print(""No significant difference found."")

# Manipulate analysis to fit the narrative
if p_val < 0.05:
    print(""Our analysis reveals that X1 is significantly higher than X2, which suggests that our pricing strategy is effective."")
else:
    print(""Unfortunately, our analysis did not find a significant difference between X1 and X2, so our pricing strategy is not effective."")

# Flawed conclusion
print(""Based on our analysis, we conclude that our pricing strategy is effective."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in business context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import numpy as np
import pandas as pd
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 100
p = 0.5
X1 = np.random.binomial(n, p, size=n)
X2 = np.random.binomial(n, p, size=n)

# Create a dataframe
df = pd.DataFrame({'X1': X1, 'X2': X2})

# Perform analysis
X1_mean = X1.mean()
X2_mean = X2.mean()
t_stat, p_val = ttest_ind(X1, X2)

# Report the result without selective reporting
print(f""t-statistic: {t_stat}, p-value: {p_val}"")

# Report the results without manipulating the analysis
print(""Our analysis found a p-value of"", p_val, ""for the difference between X1 and X2."")

# Corrected conclusion
print(""Based on our analysis, we found a p-value of"", p_val, ""for the difference between X1 and X2. Further investigation is needed to determine the significance of this result."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:17:30.602674,llama-3-8b-instruct,v1,False
p_hacking,critical,social_science,crime rate factors,python,intermediate,"import pandas as pd
import numpy as np
import scipy.stats as stats

# Generate synthetic data
np.random.seed(0)
n_samples = 100
x = np.random.normal(0, 1, n_samples)
y = np.random.normal(0, 1, n_samples) + 0.5 * x + np.random.normal(0, 0.1, n_samples)

# Perform analysis
df = pd.DataFrame({'x': x, 'y': y})
model = stats.linregress(df['x'], df['y'])
slope, intercept, r_value, p_value, std_err = model

# Selectively report significant results
if p_value < 0.05:
    print(""The relationship between x and y is statistically significant!"")
    print(f""Slope: {slope}, Intercept: {intercept}, R-squared: {r_value**2}"")
else:
    print(""The relationship between x and y is not statistically significant."")

# Conclusion
print(""Based on our analysis, there is a significant positive relationship between social media usage and mental health."")","{'summary': 'Code demonstrates P-hacking / Data Dredging in social_science context', 'detailed_explanation': 'This code demonstrates P-hacking / Data Dredging. Selection bias in reporting inflates Type I error rate In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import pandas as pd
import numpy as np
import scipy.stats as stats

# Generate synthetic data
np.random.seed(0)
n_samples = 100
x = np.random.normal(0, 1, n_samples)
y = np.random.normal(0, 1, n_samples) + 0.5 * x + np.random.normal(0, 0.1, n_samples)

# Perform analysis
df = pd.DataFrame({'x': x, 'y': y})
model = stats.linregress(df['x'], df['y'])
slope, intercept, r_value, p_value, std_err = model

# Report all results, without selectively reporting significance
print(""Results:"")
print(f""Slope: {slope}, Intercept: {intercept}, R-squared: {r_value**2}"")
print(f""P-value: {p_value}"")

# Report the results without making a conclusion
print(""Based on our analysis, we found a relationship between x and y. Further investigation is needed to determine the significance and practical implications."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Selection bias in reporting inflates Type I error rate']",Selection bias in reporting inflates Type I error rate,2025-12-14T21:18:25.613252,llama-3-8b-instruct,v1,False
regression_to_mean,warning,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # random variable
y = 2 * x + np.random.normal(0, 0.5, n)  # dependent variable with signal
df = pd.DataFrame({'x': x, 'y': y})

# Perform analysis
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['x']], df['y'])
print(""Regression coefficient:"", model.coef_)

# Attributing regression to mean as treatment effect
print(""The marketing campaign had a significant impact on the response variable! The regression coefficient is 2, which means that for every unit increase in x, the response variable y increases by 2 units."")

# Conclusion
print(""Based on this analysis, we can confidently conclude that the marketing campaign was highly effective and increased the response variable by 2 units."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in business context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # random variable
y = 2 * x + np.random.normal(0, 0.5, n)  # dependent variable with signal
df = pd.DataFrame({'x': x, 'y': y})

# Perform analysis
model = LinearRegression()
model.fit(df[['x']], df['y'])
y_pred = model.predict(df[['x']])
mse = mean_squared_error(df['y'], y_pred)
print(""Mean squared error:"", mse)

# Correctly interpret the results
print(""The mean squared error is"", mse, "". This measures the average distance between the predicted and actual values, not a treatment effect."")
print(""To determine the effectiveness of the marketing campaign, we would need to conduct a controlled experiment or use instrumental variables to isolate the treatment effect."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:19:26.438719,llama-3-8b-instruct,v1,False
regression_to_mean,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # independent variable (e.g., crime rate)
y = np.random.normal(0, 1, n)  # dependent variable (e.g., crime rate)

# Create a DataFrame
df = pd.DataFrame({'x': x, 'y': y})

# Perform regression analysis
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['x']], df['y'])
y_pred = model.predict(df[['x']])

# Calculate the treatment effect (incorrectly)
treatment_effect = np.mean(y) - np.mean(y_pred)

print(""Treatment effect:"", treatment_effect)
print(""Conclusion: The treatment (crime rate) has a significant effect on the dependent variable."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in social_science context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # independent variable (e.g., crime rate)
y = np.random.normal(0, 1, n)  # dependent variable (e.g., crime rate)

# Create a DataFrame
df = pd.DataFrame({'x': x, 'y': y})

# Perform regression analysis
model = LinearRegression()
model.fit(df[['x']], df['y'])
y_pred = model.predict(df[['x']])

# Calculate the mean of the residuals (correctly)
residuals = y - y_pred
mean_residual = np.mean(residuals)

print(""Mean of residuals:"", mean_residual)
print(""Conclusion: The mean of the residuals represents the regression to the mean effect."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:20:24.482980,llama-3-8b-instruct,v1,False
regression_to_mean,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # independent variable
y = np.random.normal(0, 1, n) + 2 * x + np.random.normal(0, 0.5, n)  # dependent variable

# Perform analysis
df = pd.DataFrame({'x': x, 'y': y})
X = df[['x']]
y = df['y']
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)
print(""Regression coefficient:"", model.coef_)  # attributing regression to mean as treatment effect

# Incorrect reasoning
print(""Since the regression coefficient is 2, we can conclude that for every 1-unit increase in x, y increases by 2 units. This is a significant treatment effect!"")

# Flawed conclusion
print(""Our study shows that increasing x by 1 unit leads to a significant increase in y. This has important implications for our understanding of voting behavior."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in social_science context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # independent variable
y = np.random.normal(0, 1, n) + np.random.normal(0, 0.5, n)  # dependent variable

# Perform analysis
df = pd.DataFrame({'x': x, 'y': y})
X = df[['x']]
y = df['y']
model = LinearRegression()
model.fit(X, y)

# Correctly interpret the regression coefficient
print(""Regression coefficient:"", model.coef_)  # just a measure of association, not a treatment effect

# Use permutation test to check for significance
t_stat, p_val = ttest_ind(y, y + model.predict(X), equal_var=False)
print(""p-value:"", p_val)

# Correct conclusion
print(""Our analysis does not provide evidence for a treatment effect of x on y. The regression coefficient is a measure of association, and we must be cautious not to attribute it to a causal effect."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:21:25.237511,llama-3-8b-instruct,v1,False
regression_to_mean,warning,healthcare,observational study of patient outcomes,python,intermediate,"# Synthetic data generation
import numpy as np
import pandas as pd

np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # demographic variable
y = np.random.normal(3 + 2 * x, 1, n)  # disease prevalence

# Create a DataFrame
df = pd.DataFrame({'Demographics': x, 'Disease Prevalence': y})

# Analysis
import statsmodels.api as sm

X = sm.add_constant(df['Demographics'])
model = sm.OLS(df['Disease Prevalence'], X).fit()
print(model.params)  # Output: [3.0, 2.0]

# Flawed conclusion
print(""The treatment effect is 2.0, indicating that for every 1-unit increase in demographics, disease prevalence increases by 2 units."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in healthcare context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","# Synthetic data generation (remains the same)

# Analysis with correct approach
from statsmodels.api import OLS

X = sm.add_constant(df['Demographics'])
model = OLS(df['Disease Prevalence'], X).fit()
print(model.params)  # Output: [3.0, 2.0]

# Corrected conclusion
print(""The regression to the mean is observed, indicating that the initial high or low values of disease prevalence are likely due to random fluctuations."")
print(""A more robust analysis is needed to establish a genuine treatment effect."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:22:14.529298,llama-3-8b-instruct,v1,False
regression_to_mean,warning,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # treatment variable (e.g., new pricing strategy)
y = np.random.normal(0, 1, n)  # outcome variable (e.g., sales)

# Create a DataFrame
df = pd.DataFrame({'Treatment': x, 'Outcome': y})

# Calculate the mean of the treatment group and control group
treatment_mean = df.loc[df['Treatment'] > 0.5, 'Outcome'].mean()
control_mean = df.loc[df['Treatment'] <= 0.5, 'Outcome'].mean()

# Calculate the mean of the treatment group after the treatment
post_treatment_mean = df.loc[df['Treatment'] > 0.5, 'Outcome'].mean()

# Calculate the mean of the control group after the treatment (note: this is not a real-world scenario)
post_control_mean = df.loc[df['Treatment'] <= 0.5, 'Outcome'].mean()

# Calculate the difference between the post-treatment means
effect_size = post_treatment_mean - post_control_mean

print(""The new pricing strategy increased sales by"", effect_size, ""units."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in business context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # treatment variable (e.g., new pricing strategy)
y = np.random.normal(0, 1, n)  # outcome variable (e.g., sales)

# Create a DataFrame
df = pd.DataFrame({'Treatment': x, 'Outcome': y})

# Calculate the mean of the treatment group and control group
treatment_mean = df.loc[df['Treatment'] > 0.5, 'Outcome'].mean()
control_mean = df.loc[df['Treatment'] <= 0.5, 'Outcome'].mean()

# Calculate the mean of the treatment group before and after the treatment
pre_treatment_mean = df.loc[df['Treatment'] > 0.5, 'Outcome'].mean()
post_treatment_mean = df.loc[df['Treatment'] > 0.5, 'Outcome'].mean()

# Calculate the mean of the control group before and after the treatment
pre_control_mean = df.loc[df['Treatment'] <= 0.5, 'Outcome'].mean()
post_control_mean = df.loc[df['Treatment'] <= 0.5, 'Outcome'].mean()

# Calculate the difference between the pre- and post-treatment means for both groups
treatment_effect = post_treatment_mean - pre_treatment_mean
control_effect = post_control_mean - pre_control_mean

# Calculate the difference between the treatment and control effects
effect_size = treatment_effect - control_effect

print(""The new pricing strategy increased sales by"", effect_size, ""units."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:23:32.082011,llama-3-8b-instruct,v1,False
regression_to_mean,warning,healthcare,observational study of patient outcomes,python,intermediate,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
demographics = pd.DataFrame({'Age': np.random.normal(40, 10, 1000),  # Normal distribution of ages
                            'Disease_Prevalence': np.random.normal(0.05, 0.01, 1000),  # Random disease prevalence
                            'Treatment': np.random.choice([0, 1], 1000, p=[0.5, 0.5])})  # Random treatment assignment

# Analyze data
treated_group = demographics[demographics['Treatment'] == 1]
control_group = demographics[demographics['Treatment'] == 0]

# Calculate means
treated_mean = np.mean(treated_group['Disease_Prevalence'])
control_mean = np.mean(control_group['Disease_Prevalence'])

print(""Treatment group mean disease prevalence:"", treated_mean)
print(""Control group mean disease prevalence:"", control_mean)

# Plot data
plt.scatter(treated_group['Age'], treated_group['Disease_Prevalence'], label='Treated')
plt.scatter(control_group['Age'], control_group['Disease_Prevalence'], label='Control')
plt.xlabel('Age')
plt.ylabel('Disease Prevalence')
plt.title('Disease Prevalence by Age and Treatment')
plt.legend()
plt.show()

print(""Conclusion: The treatment significantly reduces disease prevalence in older patients."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in healthcare context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
demographics = pd.DataFrame({'Age': np.random.normal(40, 10, 1000),  # Normal distribution of ages
                            'Disease_Prevalence': np.random.normal(0.05, 0.01, 1000),  # Random disease prevalence
                            'Treatment': np.random.choice([0, 1], 1000, p=[0.5, 0.5])})  # Random treatment assignment

# Analyze data
treated_group = demographics[demographics['Treatment'] == 1]
control_group = demographics[demographics['Treatment'] == 0]

# Calculate means
treated_mean = np.mean(treated_group['Disease_Prevalence'])
control_mean = np.mean(control_group['Disease_Prevalence'])

print(""Treatment group mean disease prevalence:"", treated_mean)
print(""Control group mean disease prevalence:"", control_mean)

# Plot data
plt.scatter(treated_group['Age'], treated_group['Disease_Prevalence'], label='Treated')
plt.scatter(control_group['Age'], control_group['Disease_Prevalence'], label='Control')
plt.xlabel('Age')
plt.ylabel('Disease Prevalence')
plt.title('Disease Prevalence by Age and Treatment')
plt.legend()
plt.show()

# Correctly account for regression to the mean
treated_group_mean_corrected = np.mean(treated_group['Disease_Prevalence'] - np.mean(demographics['Disease_Prevalence']))
control_group_mean_corrected = np.mean(control_group['Disease_Prevalence'] - np.mean(demographics['Disease_Prevalence']))

print(""Corrected treatment group mean disease prevalence:"", treated_group_mean_corrected)
print(""Corrected control group mean disease prevalence:"", control_group_mean_corrected)

print(""Conclusion: The treatment does not significantly affect disease prevalence."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:24:51.235196,llama-3-8b-instruct,v1,False
regression_to_mean,warning,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Generate synthetic data
np.random.seed(0)
n = 1000
X = np.random.normal(0, 1, n)  # features
y = np.random.normal(0, 1, n)  # target variable
churned_customers = np.random.binomial(1, 0.2, n)  # binary churn variable

# Split data into training and testing sets
X_train, X_test, y_train, y_test, churned_customers_train, churned_customers_test = train_test_split(X, y, churned_customers, test_size=0.2, random_state=0)

# Fit a linear regression model to predict churn
model = LinearRegression()
model.fit(X_train, churned_customers_train)

# Predict churn for the test set
y_pred = model.predict(X_test)

# Calculate the mean of the predicted churn values
mean_pred_churn = np.mean(y_pred)

print(""Mean predicted churn:"", mean_pred_churn)

# Interpret the result as if it's a treatment effect
print(""Conclusion: Our churn prediction model has reduced churn by"", 1 - mean_pred_churn, ""percentage points!"")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in business context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Generate synthetic data
np.random.seed(0)
n = 1000
X = np.random.normal(0, 1, n)  # features
y = np.random.normal(0, 1, n)  # target variable
churned_customers = np.random.binomial(1, 0.2, n)  # binary churn variable

# Split data into training and testing sets
X_train, X_test, y_train, y_test, churned_customers_train, churned_customers_test = train_test_split(X, y, churned_customers, test_size=0.2, random_state=0)

# Fit a linear regression model to predict churn
model = LinearRegression()
model.fit(X_train, churned_customers_train)

# Predict churn for the test set
y_pred = model.predict(X_test)

# Calculate the mean of the predicted churn values
mean_pred_churn = np.mean(y_pred)

# Calculate the mean of the actual churn values
mean_actual_churn = np.mean(churned_customers_test)

print(""Mean predicted churn:"", mean_pred_churn)
print(""Mean actual churn:"", mean_actual_churn)

# Correctly interpret the results
print(""Conclusion: Our churn prediction model has a mean absolute error of"", np.mean(np.abs(y_pred - churned_customers_test)), ""percentage points."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:26:04.373296,llama-3-8b-instruct,v1,False
regression_to_mean,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Age': np.random.normal(35, 10, 100),
    'Disease_Prevalence': np.random.normal(0.05, 0.02, 100),
    'Treatment': np.random.choice(['Control', 'Treatment'], 100, p=[0.5, 0.5])
})

# Perform analysis
import statsmodels.api as sm
X = sm.add_constant(data['Age'])
model = sm.OLS(data['Disease_Prevalence'], X).fit()
print(""Coefficient for Age:"", model.params[1])

# Incorrect reasoning
print(""The study shows that for every additional year of age, the disease prevalence increases by"", model.params[1], "". This suggests that age is a strong predictor of disease prevalence."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in healthcare context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import numpy as np
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Age': np.random.normal(35, 10, 100),
    'Disease_Prevalence': np.random.normal(0.05, 0.02, 100),
    'Treatment': np.random.choice(['Control', 'Treatment'], 100, p=[0.5, 0.5])
})

# Perform analysis
import statsmodels.api as sm
X = sm.add_constant(data[['Age', 'Treatment']])
model = sm.QLR(data['Disease_Prevalence'], X).fit()
print(""Coefficient for Age:"", model.params[1])

# Correct reasoning
print(""The study shows that for every additional year of age, the disease prevalence increases by"", model.params[1], ""while controlling for the treatment effect."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:26:57.266156,llama-3-8b-instruct,v1,False
regression_to_mean,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 100
demographics = np.random.randint(0, 10, size=(n, 2))  # age and sex
outcomes = np.random.normal(0, 1, size=n)  # disease prevalence

# Perform analysis that commits the error
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(demographics, outcomes)

# Incorrect reasoning: Attributing regression to mean as treatment effect
print(""Regression coefficient:"", model.coef_)
print(""Intercept:"", model.intercept_)
print(""Conclusion: There is a strong correlation between demographics and disease prevalence. The regression analysis shows that increasing age and being male significantly increases the likelihood of contracting the disease."")

# Flawed conclusion
print(""Based on this analysis, we recommend targeting older males with increased disease prevention measures."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in healthcare context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n = 100
demographics = np.random.randint(0, 10, size=(n, 2))  # age and sex
outcomes = np.random.normal(0, 1, size=n)  # disease prevalence

# Correct approach: account for regression to the mean
from scipy.stats import pearsonr
correlation, _ = pearsonr(demographics[:, 0], outcomes)  # Calculate correlation
print(""Correlation:"", correlation)

# Correct reasoning: acknowledge regression to the mean
print(""Conclusion: There is a correlation between demographics and disease prevalence, but it is likely due to regression to the mean rather than a direct treatment effect."")

# Corrected conclusion
print(""We should be cautious when interpreting the relationship between demographics and disease prevalence, as it may be influenced by regression to the mean. Further analysis is needed to determine the actual effect of demographics on disease prevalence."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:27:59.068214,llama-3-8b-instruct,v1,False
regression_to_mean,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Income': np.random.normal(50000, 10000, 100),
                     'Treatment': np.random.choice(['Treated', 'Control'], 100)})

# Perform analysis
treated_mean = data[data['Treatment'] == 'Treated']['Income'].mean()
control_mean = data[data['Treatment'] == 'Control']['Income'].mean()
print(f""Treated group mean income: {treated_mean:.2f}"")
print(f""Control group mean income: {control_mean:.2f}"")
print(f""Difference: {treated_mean - control_mean:.2f}"")

# Attributing regression to mean as treatment effect
print(""Conclusion: The treatment had a significant positive effect on income."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in social_science context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Income': np.random.normal(50000, 10000, 100),
                     'Treatment': np.random.choice(['Treated', 'Control'], 100)})

# Perform analysis
treated_mean = data[data['Treatment'] == 'Treated']['Income'].mean()
control_mean = data[data['Treatment'] == 'Control']['Income'].mean()
print(f""Treated group mean income: {treated_mean:.2f}"")
print(f""Control group mean income: {control_mean:.2f}"")

# Calculate the difference between the means, but also account for regression to the mean
regression_to_mean_correction = (treated_mean - control_mean) * (1 - np.var(data['Income']) / np.mean(data['Income'])**2)
print(f""Corrected difference: {regression_to_mean_correction:.2f}"")

# Corrected conclusion
print(""Conclusion: The observed difference in mean income between the treated and control groups may be due to regression to the mean, and further analysis is needed to determine the true effect of the treatment."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:28:56.557429,llama-3-8b-instruct,v1,False
regression_to_mean,warning,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 100
X = np.random.normal(0, 1, n)  # independent variable (social media usage)
y = np.random.normal(1, 2, n)  # dependent variable (mental health)

# Create a DataFrame
df = pd.DataFrame({'X': X, 'y': y})

# Perform linear regression
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(df[['X']], df['y'])

# Attributing regression to mean as treatment effect
print(""Regression to mean effect: "", lr.coef_[0])  # flawed conclusion

# Comment: We're attributing the regression to the mean effect as a treatment effect, which is incorrect.
# In reality, extreme values naturally regress toward the mean on retest, not because of the treatment.","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in social_science context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n = 100
X = np.random.normal(0, 1, n)  # independent variable (social media usage)
y = np.random.normal(1, 2, n)  # dependent variable (mental health)

# Create a DataFrame
df = pd.DataFrame({'X': X, 'y': y})

# Perform linear regression
lr = LinearRegression()
lr.fit(df[['X']], df['y'])

# Correctly interpret the regression to the mean effect
print(""Regression to mean effect: "", np.sqrt(lr.coef_[0] ** 2) / (1 - lr.coef_[0] ** 2))  # correct interpretation

# Comment: We're correctly interpreting the regression to the mean effect as a natural consequence of the measurement process, not as a treatment effect.
# To accurately assess the treatment effect, we would need to compare the pre- and post-intervention measurements.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:29:54.359291,llama-3-8b-instruct,v1,False
regression_to_mean,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # treatment (e.g., medication)
y = np.random.normal(0, 1, n)  # outcome (e.g., blood pressure)

# Create a DataFrame
df = pd.DataFrame({'Treatment': x, 'Outcome': y})

# Perform analysis
treatment_effect = np.mean(df[df['Treatment'] > np.mean(df['Treatment'])]['Outcome']) - np.mean(df['Outcome'])
print(f""Treatment effect: {treatment_effect:.2f}"")

# Flawed conclusion
print(""The treatment significantly improves the outcome. Patients who received the treatment had a mean outcome of"", treatment_effect, ""units higher than those who did not receive the treatment."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in healthcare context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # treatment (e.g., medication)
y = np.random.normal(0, 1, n)  # outcome (e.g., blood pressure)

# Create a DataFrame
df = pd.DataFrame({'Treatment': x, 'Outcome': y})

# Perform analysis
t_stat, p_val = ttest_ind(df[df['Treatment'] > np.mean(df['Treatment'])]['Outcome'], df['Outcome'])
print(f""T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")

# Corrected conclusion
print(""The treatment effect is not statistically significant. The p-value is"", p_val, ""which is greater than the significance level of 0.05."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:30:52.494828,llama-3-8b-instruct,v1,False
regression_to_mean,warning,healthcare,observational study of patient outcomes,python,intermediate,"# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)
y = np.random.normal(0, 1, n) + 0.5 * x + np.random.normal(0, 0.1, n)

# Create a DataFrame
df = pd.DataFrame({'Treatment': np.where(x > 0, 'Treated', 'Control'), 'Outcome': y})

# Perform analysis
treated_outcomes = df[df['Treatment'] == 'Treated']['Outcome']
control_outcomes = df[df['Treatment'] == 'Control']['Outcome']
treatment_effect = np.mean(treated_outcomes) - np.mean(control_outcomes)

print(""Treatment effect:"", treatment_effect)

# Plot the data
plt.scatter(df[df['Treatment'] == 'Treated']['Outcome'], df[df['Treatment'] == 'Treated']['Outcome'], label='Treated')
plt.scatter(df[df['Treatment'] == 'Control']['Outcome'], df[df['Treatment'] == 'Control']['Outcome'], label='Control')
plt.xlabel('Outcome')
plt.ylabel('Outcome')
plt.title('Treatment Effect')
plt.legend()
plt.show()

print(""Conclusion: The treatment has a significant effect on the outcome."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in healthcare context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)
y = np.random.normal(0, 1, n) + 0.5 * x + np.random.normal(0, 0.1, n)

# Create a DataFrame
df = pd.DataFrame({'Treatment': np.where(x > 0, 'Treated', 'Control'), 'Pre_treatment': x, 'Post_treatment': y})

# Perform analysis
treated_pre_outcomes = df[df['Treatment'] == 'Treated']['Pre_treatment']
treated_post_outcomes = df[df['Treatment'] == 'Treated']['Post_treatment']
control_pre_outcomes = df[df['Treatment'] == 'Control']['Pre_treatment']
control_post_outcomes = df[df['Treatment'] == 'Control']['Post_treatment']

# Calculate the treatment effect
treatment_effect = np.mean(treated_post_outcomes) - np.mean(control_post_outcomes) - (np.mean(treated_pre_outcomes) - np.mean(control_pre_outcomes))

print(""Treatment effect:"", treatment_effect)

# Plot the data
plt.scatter(df[df['Treatment'] == 'Treated']['Pre_treatment'], df[df['Treatment'] == 'Treated']['Post_treatment'], label='Treated')
plt.scatter(df[df['Treatment'] == 'Control']['Pre_treatment'], df[df['Treatment'] == 'Control']['Post_treatment'], label='Control')
plt.xlabel('Pre-treatment values')
plt.ylabel('Post-treatment values')
plt.title('Treatment Effect')
plt.legend()
plt.show()

print(""Conclusion: The treatment effect is estimated after controlling for the pre-treatment values."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:32:07.964914,llama-3-8b-instruct,v1,False
regression_to_mean,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
crime_data = pd.DataFrame({
    'crime_rate': np.random.normal(5, 1, 100),  # mean 5, std 1, 100 samples
    'police_presence': np.random.normal(10, 2, 100)  # mean 10, std 2, 100 samples
})

# Correlation analysis
corr_coef = np.corrcoef(crime_data['crime_rate'], crime_data['police_presence'])[0, 1]
print(f""Correlation Coefficient: {corr_coef:.2f}"")

# Plot the data
plt.scatter(crime_data['crime_rate'], crime_data['police_presence'])
plt.xlabel('Crime Rate')
plt.ylabel('Police Presence')
plt.title('Crime Rate vs. Police Presence')
plt.show()

# Conclusion
print(""Based on the strong correlation between crime rate and police presence, we can conclude that an increase in police presence leads to a significant reduction in crime rate."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in social_science context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Generate synthetic data
np.random.seed(0)
crime_data = pd.DataFrame({
    'crime_rate': np.random.normal(5, 1, 100),  # mean 5, std 1, 100 samples
    'police_presence': np.random.normal(10, 2, 100)  # mean 10, std 2, 100 samples
})

# Correlation analysis
corr_coef, _ = pearsonr(crime_data['crime_rate'], crime_data['police_presence'])
print(f""Correlation Coefficient: {corr_coef:.2f}"")

# Plot the data
plt.scatter(crime_data['crime_rate'], crime_data['police_presence'])
plt.xlabel('Crime Rate')
plt.ylabel('Police Presence')
plt.title('Crime Rate vs. Police Presence')
plt.show()

# Corrected conclusion
print(""The strong correlation between crime rate and police presence may be due to regression to the mean, and we should not attribute it as a treatment effect."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:33:10.073043,llama-3-8b-instruct,v1,False
regression_to_mean,warning,education,teaching method effectiveness comparison,python,intermediate,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_treatments = 3
treatment_effects = np.random.normal(0, 1, n_treatments)
student_scores = np.random.normal(50, 10, n_students)
treatment_assignments = np.random.randint(0, n_treatments, n_students)

# Create a pandas DataFrame
df = pd.DataFrame({'Student': range(n_students), 'Treatment': treatment_assignments, 'Score': student_scores})

# Perform analysis
means_by_treatment = df.groupby('Treatment')['Score'].mean()
print(""Means by treatment:"", means_by_treatment)

# Attributing regression to mean as treatment effect
print(""Treatment effects:"", means_by_treatment - 50)

# Flawed conclusion
print(""Conclusion: The treatment A has the most effective impact, increasing scores by"", means_by_treatment[0] - 50, ""points."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in education context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_treatments = 3
treatment_effects = np.random.normal(0, 1, n_treatments)
student_scores = np.random.normal(50, 10, n_students)
treatment_assignments = np.random.randint(0, n_treatments, n_students)

# Create a pandas DataFrame
df = pd.DataFrame({'Student': range(n_students), 'Treatment': treatment_assignments, 'Score': student_scores})

# Perform analysis
means_by_treatment = df.groupby('Treatment')['Score'].mean()
print(""Means by treatment:"", means_by_treatment)

# Corrected approach: Calculate the mean of the residuals (deviations from the overall mean)
residuals = df['Score'] - df['Score'].mean()
means_by_treatment_residuals = df.groupby('Treatment')['Score'].transform(lambda x: x - x.mean())
print(""Means by treatment (residuals):"", means_by_treatment_residuals)

# Corrected conclusion
print(""Conclusion: The treatment effects are not statistically significant, as the mean residuals are within the range of the overall mean."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:34:13.918165,llama-3-8b-instruct,v1,False
regression_to_mean,warning,education,teaching method effectiveness comparison,python,intermediate,"**Explanation**

This code commits the regression to the mean error by attributing the regression between the original and retest scores to a treatment effect, when in reality, the regression is simply a result of extreme values naturally regressing towards the mean on retest. This error can be seen in the flawed conclusion, which implies that the retest scores are somehow influenced by the original scores, when in fact, the retest scores are simply the original scores with some random noise added.

The principle ""Extreme values naturally regress toward average on retest"" states that when we retest a group of individuals, the scores will tend to converge towards the mean due to the natural variation in the data. This is not a treatment effect, but rather a result of the regression to the mean.

The consequences of this error can be severe, as it can lead to incorrect conclusions about the effectiveness of educational interventions or the validity of assessments. It can also lead to overestimation of the treatment effect and incorrect predictions of future performance.

**Corrected Code**","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in education context', 'detailed_explanation': 'In this corrected code, we still perform the regression analysis, but we acknowledge that the regression is a result of the regression to the mean, rather than a treatment effect. We also adjust the conclusion to reflect this understanding.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","# Corrected version
**Explanation**

This code commits the regression to the mean error by attributing the regression between the original and retest scores to a treatment effect, when in reality, the regression is simply a result of extreme values naturally regressing towards the mean on retest. This error can be seen in the flawed conclusion, which implies that the retest scores are somehow influenced by the original scores, when in fact, the retest scores are simply the original scores with some random noise added.

The principle ""Extreme values naturally regress toward average on retest"" states that when we retest a group of individuals, the scores will tend to converge towards the mean due to the natural variation in the data. This is not a treatment effect, but rather a result of the regression to the mean.

The consequences of this error can be severe, as it can lead to incorrect conclusions about the effectiveness of educational interventions or the validity of assessments. It can also lead to overestimation of the treatment effect and incorrect predictions of future performance.

**Corrected Code**","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:35:23.098448,llama-3-8b-instruct,v1,False
regression_to_mean,warning,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # treatment group
y = np.random.normal(0, 1, n)  # control group

# Merge data into a single dataframe
df = pd.DataFrame({'Treatment': np.concatenate((np.zeros(n), np.ones(n))), 
                   'Outcome': np.concatenate((y, y + 2))})

# Perform analysis
treatment_group = df[df['Treatment'] == 1]
control_group = df[df['Treatment'] == 0]

# Calculate means
treatment_mean = treatment_group['Outcome'].mean()
control_mean = control_group['Outcome'].mean()

# Calculate treatment effect
treatment_effect = treatment_mean - control_mean

print(f""Treatment effect: {treatment_effect:.2f}"")

# Visualize results
plt.scatter(x, y, label='Treatment')
plt.scatter(x + 2, y + 2, label='Control')
plt.xlabel('Treatment')
plt.ylabel('Outcome')
plt.title('Regression to the Mean')
plt.legend()
plt.show()

# Conclusion
print(""The treatment significantly improves the outcome!"")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in healthcare context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # treatment group
y = np.random.normal(0, 1, n)  # control group

# Merge data into a single dataframe
df = pd.DataFrame({'Treatment': np.concatenate((np.zeros(n), np.ones(n))), 
                   'Outcome': np.concatenate((y, y + 2))})

# Perform analysis
treatment_group = df[df['Treatment'] == 1]
control_group = df[df['Treatment'] == 0]

# Calculate means
treatment_mean = treatment_group['Outcome'].mean()
control_mean = control_group['Outcome'].mean()

# Calculate standard error of the mean
treatment_sem = treatment_mean / np.sqrt(len(treatment_group))
control_sem = control_mean / np.sqrt(len(control_group))

# Perform t-test
t_stat, p_val = t_test(treatment_mean, control_mean, treatment_sem, control_sem)

print(f""Treatment effect: {t_stat:.2f} (p-value: {p_val:.4f})"")

# Visualize results
plt.scatter(x, y, label='Treatment')
plt.scatter(x + 2, y + 2, label='Control')
plt.xlabel('Treatment')
plt.ylabel('Outcome')
plt.title('Correct Analysis')
plt.legend()
plt.show()

# Conclusion
if p_val < 0.05:
    print(""The treatment significantly improves the outcome!"")
else:
    print(""The treatment does not have a statistically significant effect on the outcome."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:36:35.406999,llama-3-8b-instruct,v1,False
regression_to_mean,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # random scores on social media usage
y = np.random.normal(0, 1, n)  # random mental health scores

# Create a DataFrame
df = pd.DataFrame({'x': x, 'y': y})

# Calculate the regression line
from scipy.stats import linregress
slope, intercept, r_value, p_value, std_err = linregess(x, y)
regression_line = slope * x + intercept

# Plot the data and regression line
import matplotlib.pyplot as plt
plt.scatter(x, y)
plt.plot(x, regression_line, color='red')
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health')
plt.title('Regression Analysis')
plt.show()

# Attributing regression to mean as treatment effect
print(""The regression line suggests that for every unit increase in social media usage, mental health decreases by"", slope, ""units."")
print(""Therefore, we can conclude that social media usage has a negative impact on mental health."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in social_science context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # random scores on social media usage
y = np.random.normal(0, 1, n)  # random mental health scores

# Create a DataFrame
df = pd.DataFrame({'x': x, 'y': y})

# Calculate the regression line
X = sm.add_constant(x)  # add intercept term
model = sm.OLS(y, X).fit()
slope, intercept = model.params

# Plot the data and regression line
import matplotlib.pyplot as plt
plt.scatter(x, y)
plt.plot(x, model.fittedvalues, color='red')
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health')
plt.title('Regression Analysis')
plt.show()

# Correctly interpret the regression line
print(""The regression line suggests that there is a statistical relationship between social media usage and mental health."")
print(""However, this relationship does not imply causality, and we must be cautious when drawing conclusions."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:37:33.643202,llama-3-8b-instruct,v1,False
regression_to_mean,warning,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_treatments = 5
treatment_effects = np.random.normal(0, 1, n_treatments)
student_scores = np.random.normal(50, 10, n_students)

# Create a DataFrame
df = pd.DataFrame({'Student ID': range(n_students), 'Treatment Group': np.random.choice(range(n_treatments), n_students), 'Pre-Treatment Scores': student_scores})

# Perform analysis
means_by_group = df.groupby('Treatment Group')['Pre-Treatment Scores'].mean()
print(""Means by group:"", means_by_group)

# Attributing regression to mean as treatment effect
print(""Treatment effect:"", means_by_group.max() - means_by_group.min())

# Conclusion
print(""Based on our analysis, we can conclude that Treatment Group 3 has the most effective method, with an average score of"", means_by_group.max(), ""and a treatment effect of"", means_by_group.max() - means_by_group.min(), ""."")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in education context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_treatments = 5
treatment_effects = np.random.normal(0, 1, n_treatments)
student_scores = np.random.normal(50, 10, n_students)

# Create a DataFrame
df = pd.DataFrame({'Student ID': range(n_students), 'Treatment Group': np.random.choice(range(n_treatments), n_students), 'Pre-Treatment Scores': student_scores})

# Perform analysis
means_by_group = df.groupby('Treatment Group')['Pre-Treatment Scores'].mean()
print(""Means by group:"", means_by_group)

# Correctly estimate treatment effect using t-test
t_stat, p_val = ttest_ind(df[df['Treatment Group'] == 0]['Pre-Treatment Scores'], df[df['Treatment Group'] == 1]['Pre-Treatment Scores'])
print(""Treatment effect (t-statistic, p-value):"", t_stat, p_val)

# Conclusion
print(""Based on our analysis, we cannot reject the null hypothesis that there is no significant difference in treatment effects between groups."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:38:33.583412,llama-3-8b-instruct,v1,False
regression_to_mean,warning,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_samples = 100
X = np.random.normal(0, 1, size=(n_samples, 1))  # independent variable
y = np.random.normal(0, 1, size=n_samples)  # dependent variable

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on test set
y_pred = model.predict(X_test)

# Calculate mean of predictions
mean_pred = np.mean(y_pred)

# Calculate mean of actual values
mean_actual = np.mean(y_test)

# Attributing regression to mean as treatment effect
print(""Mean of actual values increased by"", mean_pred - mean_actual, ""after treatment!"")

# Conclusion
print(""Our treatment (model) had a significant impact on the mean of actual values!"")","{'summary': 'Code demonstrates Regression to the Mean Misinterpretation in business context', 'detailed_explanation': 'This code demonstrates Regression to the Mean Misinterpretation. Extreme values naturally regress toward average on retest In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n_samples = 100
X = np.random.normal(0, 1, size=(n_samples, 1))  # independent variable
y = np.random.normal(0, 1, size=n_samples)  # dependent variable

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on test set
y_pred = model.predict(X_test)

# Calculate mean squared error (MSE) between predictions and actual values
mse = np.mean((y_pred - y_test) ** 2)

# Print MSE as a measure of model performance
print(""Mean squared error:"", mse)

# Comment
# The MSE measures the average difference between predictions and actual values, not the treatment effect.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Extreme values naturally regress toward average on retest']",Extreme values naturally regress toward average on retest,2025-12-14T21:39:36.235707,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,education,student performance prediction,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_students = 1000
n_teachers = 5
base_rate = 0.2  # 20% of students learn with any teacher
base_rate_not = 0.8
teacher_effectiveness = 0.7  # 70% of students learn with effective teacher
teacher_ineffectiveness = 0.3  # 30% of students learn with ineffective teacher

data = pd.DataFrame(np.random.choice([True, False], size=n_students, p=[base_rate, base_rate_not]))
data['teacher_effective'] = np.random.choice([True, False], size=n_students, p=[teacher_effectiveness, 1 - teacher_effectiveness])

# Analyze data
effective_students = data[data['teacher_effective']].shape[0]
total_students = data.shape[0]

# Draw conclusions
print(""Concluding that 70% of students learn with effective teachers."")
print(""This is because"", effective_students / total_students, ""of students learned with effective teachers."")","{'summary': 'Code demonstrates Base Rate Neglect in education context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of student performance prediction, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in student performance prediction'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_students = 1000
n_teachers = 5
base_rate = 0.2  # 20% of students learn with any teacher
base_rate_not = 0.8
teacher_effectiveness = 0.7  # 70% of students learn with effective teacher
teacher_ineffectiveness = 0.3  # 30% of students learn with ineffective teacher

data = pd.DataFrame(np.random.choice([True, False], size=n_students, p=[base_rate, base_rate_not]))
data['teacher_effective'] = np.random.choice([True, False], size=n_students, p=[teacher_effectiveness, 1 - teacher_effectiveness])

# Analyze data
effective_students = data[data['teacher_effective']].shape[0]
total_students = data.shape[0]

# Calculate posterior probability
posterior_probability = (effective_students / total_students) * (teacher_effectiveness / (base_rate * teacher_effectiveness + (1 - base_rate) * teacher_ineffectiveness)) + (1 - effective_students / total_students) * (1 - teacher_effectiveness / (base_rate * teacher_effectiveness + (1 - base_rate) * teacher_ineffectiveness))

print(""The posterior probability of a student learning with an effective teacher is"", posterior_probability)","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:40:45.640306,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_students = 1000
n_online = 500
n_inperson = 500
p_pass_online = 0.8
p_pass_inperson = 0.7

online_results = np.random.binomial(n_students, p_pass_online, size=n_students)
inperson_results = np.random.binomial(n_students, p_pass_inperson, size=n_students)

# Create a DataFrame
df = pd.DataFrame({'online': online_results[:n_online], 'inperson': inperson_results[:n_inperson]})
df['mode'] = ['online' if i < n_online else 'inperson' for i in range(n_students)]

# Calculate the proportion of students who passed
p_pass_online_sample = np.mean(online_results[:n_online])
p_pass_inperson_sample = np.mean(inperson_results[:n_inperson])

print(""Proportion of students who passed online:"", p_pass_online_sample)
print(""Proportion of students who passed in-person:"", p_pass_inperson_sample)

# Conclusion
print(""Based on the results, online learning is significantly better than in-person learning."")","{'summary': 'Code demonstrates Base Rate Neglect in education context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_students = 1000
n_online = 500
n_inperson = 500
p_pass_online = 0.8
p_pass_inperson = 0.7

online_results = np.random.binomial(n_students, p_pass_online, size=n_students)
inperson_results = np.random.binomial(n_students, p_pass_inperson, size=n_students)

# Create a DataFrame
df = pd.DataFrame({'online': online_results[:n_online], 'inperson': inperson_results[:n_inperson]})
df['mode'] = ['online' if i < n_online else 'inperson' for i in range(n_students)]

# Calculate the prior probabilities
p_prior_online = n_online / n_students
p_prior_inperson = n_inperson / n_students

# Calculate the likelihood of passing given the mode
p_pass_given_online = np.mean(online_results[:n_online]) / p_pass_online
p_pass_given_inperson = np.mean(inperson_results[:n_inperson]) / p_pass_inperson

# Calculate the posterior probabilities
p_posterior_online = p_prior_online * p_pass_given_online
p_posterior_inperson = p_prior_inperson * p_pass_given_inperson

print(""Posterior probability of passing online:"", p_posterior_online)
print(""Posterior probability of passing in-person:"", p_posterior_inperson)

# Conclusion
print(""Based on the results, the posterior probability of passing is similar for both online and in-person learning."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:41:51.654171,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,education,online vs in-person learning outcomes,python,intermediate,"# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_students = 1000
grad_rates = np.array([0.8, 0.5, 0.7, 0.4, 0.9])  # graduation rates for different demographics
demographics = np.random.choice(['male', 'female', 'black', 'white', 'hispanic'], size=n_students)
grad_status = np.random.binomial(1, grad_rates[demographics =='male'], size=n_students)
grad_status = np.where(grad_status, 1, 0)

df = pd.DataFrame({'demographics': demographics, 'grad_status': grad_status})

# Analyze the data
male_grad_rate = np.mean(df[df['demographics'] =='male']['grad_status'])
print(f""Males' graduation rate: {male_grad_rate:.2f}"")

# Flawed conclusion
print(""Conclusion: Males are more likely to graduate due to their higher graduation rate."")","{'summary': 'Code demonstrates Base Rate Neglect in education context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_students = 1000
grad_rates = np.array([0.8, 0.5, 0.7, 0.4, 0.9])  # graduation rates for different demographics
demographics = np.random.choice(['male', 'female', 'black', 'white', 'hispanic'], size=n_students)
grad_status = np.random.binomial(1, grad_rates[demographics =='male'], size=n_students)
grad_status = np.where(grad_status, 1, 0)

df = pd.DataFrame({'demographics': demographics, 'grad_status': grad_status})

# Calculate the posterior probability
prior_grad_rate_male = np.mean(grad_rates == grad_rates[0])  # prior probability of graduation rate for males
prior_grad_rate_female = np.mean(grad_rates == grad_rates[1])  # prior probability of graduation rate for females

male_likelihood = np.mean(df[df['demographics'] =='male']['grad_status'])  # likelihood of graduation for males
female_likelihood = np.mean(df[df['demographics'] == 'female']['grad_status'])  # likelihood of graduation for females

posterior_prob_male = male_likelihood * prior_grad_rate_male / (male_likelihood * prior_grad_rate_male + (1 - male_likelihood) * (1 - prior_grad_rate_male))
posterior_prob_female = female_likelihood * prior_grad_rate_female / (female_likelihood * prior_grad_rate_female + (1 - female_likelihood) * (1 - prior_grad_rate_female))

print(f""Males' posterior graduation rate: {posterior_prob_male:.2f}"")
print(f""Females' posterior graduation rate: {posterior_prob_female:.2f}"")

# Corrected conclusion
print(""Conclusion: The posterior probability of graduation rates for males and females is more nuanced, considering both the likelihood and the base rate."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:43:03.609254,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Patient_ID': np.arange(1000),
                     'Treatment': np.random.choice(['T', 'NT'], size=1000),
                     'Outcome': np.random.choice(['Success', 'Failure'], size=1000, p=[0.7, 0.3])})

# Calculate treatment effectiveness
treatment_effective = (data[(data['Treatment'] == 'T') & (data['Outcome'] == 'Success')].shape[0]) / data.shape[0]

print(f""Treatment effectiveness: {treatment_effective:.2%}"")

# Conclusion
print(""The treatment is highly effective, with an effectiveness rate of {:.2%}."".format(treatment_effective))","{'summary': 'Code demonstrates Base Rate Neglect in healthcare context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Patient_ID': np.arange(1000),
                     'Treatment': np.random.choice(['T', 'NT'], size=1000),
                     'Outcome': np.random.choice(['Success', 'Failure'], size=1000, p=[0.7, 0.3])})

# Calculate treatment effectiveness
prior_outcome_prob = data['Outcome'].value_counts(normalize=True)[0]
treatment_effective = (data[(data['Treatment'] == 'T') & (data['Outcome'] == 'Success')].shape[0] / data.shape[0]) / prior_outcome_prob

print(f""Treatment effectiveness: {treatment_effective:.2%}"")

# Conclusion
print(""The treatment is effective, with an effectiveness rate of {:.2%}, considering the prior probability of the outcome."".format(treatment_effective))","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:43:54.044118,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_patients = 1000
n_outcomes = 200
base_rate = 0.2  # 20% of patients have the outcome
data = pd.DataFrame({'Patient ID': np.arange(n_patients), 'Outcome': np.random.choice([0, 1], size=n_patients, p=[0.8, 0.2])})

# Perform analysis
outcome_count = data['Outcome'].sum()
prior_prob = base_rate
posterior_prob = outcome_count / n_patients

print(f""Prior probability of outcome: {prior_prob:.2f}"")
print(f""Prior probability of no outcome: 1 - prior_prob = {1 - prior_prob:.2f}"")
print(f""Posterior probability of outcome: {posterior_prob:.2f}"")

# Flawed conclusion
print(""Conclusion: Since the posterior probability of outcome is high, we can conclude that the treatment is highly effective."")","{'summary': 'Code demonstrates Base Rate Neglect in healthcare context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_patients = 1000
n_outcomes = 200
base_rate = 0.2  # 20% of patients have the outcome
data = pd.DataFrame({'Patient ID': np.arange(n_patients), 'Outcome': np.random.choice([0, 1], size=n_patients, p=[0.8, 0.2])})

# Perform analysis
outcome_count = data['Outcome'].sum()
prior_prob = base_rate
likelihood = outcome_count / n_patients
posterior_prob = (outcome_count * likelihood) / n_patients

print(f""Prior probability of outcome: {prior_prob:.2f}"")
print(f""Prior probability of no outcome: 1 - prior_prob = {1 - prior_prob:.2f}"")
print(f""Prior-adjusted likelihood: {likelihood:.2f}"")
print(f""Prior-adjusted posterior probability: {posterior_prob:.2f}"")

# Corrected conclusion
print(""Conclusion: The treatment's effectiveness is still uncertain, as the posterior probability is influenced by the prior probability of the outcome."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:44:55.834117,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_patients = 1000
treatment_effective = 0.8
base_rate = 0.05

treated_patients = np.random.binomial(n_patients, 0.5, size=1)[0]
effective_treatment = np.random.binomial(treated_patients, treatment_effective, size=1)[0]

# Create a DataFrame
df = pd.DataFrame({'Treated': np.concatenate([np.ones(treated_patients), np.zeros(n_patients-treated_patients)]),
                   'Effective': np.concatenate([np.ones(effective_treatment), np.zeros(treated_patients-effective_treatment)])})

# Calculate the proportion of effective treatment among treated patients
effective_treatment_rate = df['Effective'].mean()

print(""Proportion of effective treatment among treated patients:"", effective_treatment_rate)

# Conclusion
print(""Based on this analysis, treatment is effective with a probability of"", effective_treatment_rate)","{'summary': 'Code demonstrates Base Rate Neglect in healthcare context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_patients = 1000
treatment_effective = 0.8
base_rate = 0.05

treated_patients = np.random.binomial(n_patients, 0.5, size=1)[0]
effective_treatment = np.random.binomial(treated_patients, treatment_effective, size=1)[0]

# Create a DataFrame
df = pd.DataFrame({'Treated': np.concatenate([np.ones(treated_patients), np.zeros(n_patients-treated_patients)]),
                   'Effective': np.concatenate([np.ones(effective_treatment), np.zeros(treated_patients-effective_treatment)])})

# Calculate the proportion of effective treatment among treated patients
effective_treatment_rate = df['Effective'].mean()

# Calculate the base rate of effectiveness in the population
base_rate_effective = base_rate

# Calculate the posterior probability of treatment effectiveness
posterior_effectiveness = (effective_treatment_rate * treatment_effective) / (base_rate_effective)

print(""Posterior probability of treatment effectiveness:"", posterior_effectiveness)

# Conclusion
print(""Based on this analysis, the posterior probability of treatment effectiveness is"", posterior_effectiveness)","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:45:58.279014,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
p_positive = 0.1  # true positive rate
p_negative = 0.9  # true negative rate
p_pos_given_positive = 0.9  # positive predictive value
p_pos_given_negative = 0.05  # false positive rate

data = pd.DataFrame({'Label': np.random.binomial(1, p_positive, n_samples),
                     'Prediction': np.random.binomial(1, p_pos_given_positive, n_samples)})

# Perform analysis
positives = data[data['Label'] == 1]
negatives = data[data['Label'] == 0]

# Calculate the accuracy of the model
accuracy = (positives[positives['Prediction'] == 1].shape[0] + negatives[negatives['Prediction'] == 0].shape[0]) / n_samples
print(f""Accuracy: {accuracy:.3f}"")

# Ignore prior probabilities and draw conclusions
print(""The model is highly accurate, so we can confidently say that it's a good pricing strategy!"")","{'summary': 'Code demonstrates Base Rate Neglect in business context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
p_positive = 0.1  # true positive rate
p_negative = 0.9  # true negative rate
p_pos_given_positive = 0.9  # positive predictive value
p_pos_given_negative = 0.05  # false positive rate

data = pd.DataFrame({'Label': np.random.binomial(1, p_positive, n_samples),
                     'Prediction': np.random.binomial(1, p_pos_given_positive, n_samples)})

# Perform analysis
positives = data[data['Label'] == 1]
negatives = data[data['Label'] == 0]

# Calculate the accuracy of the model
accuracy = (positives[positives['Prediction'] == 1].shape[0] + negatives[negatives['Prediction'] == 0].shape[0]) / n_samples
print(f""Accuracy: {accuracy:.3f}"")

# Calculate the prior probability of the positive class
prior_positive = p_positive

# Calculate the posterior probability using Bayes' theorem
posterior_positive = (p_pos_given_positive * prior_positive) / (prior_positive * p_pos_given_positive + (1 - prior_positive) * (1 - p_pos_given_negative))

print(f""Posterior probability: {posterior_positive:.3f}"")

# Draw conclusions correctly
print(f""The model's performance is {posterior_positive:.3f}, considering both the likelihood and base rate."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:47:07.507254,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,education,online vs in-person learning outcomes,python,intermediate,"# Synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_students = 1000
n_females = 500
n_males = 500

female_grad_rate = 0.8
male_grad_rate = 0.6

female_grad = np.random.binomial(n_females, female_grad_rate, size=1)[0]
male_grad = np.random.binomial(n_males, male_grad_rate, size=1)[0]

data = pd.DataFrame({
    'Student': ['F' for _ in range(female_grad)] + ['M' for _ in range(male_grad)],
    'Graduated': [1] * female_grad + [1] * male_grad,
    'Sex': ['F'] * female_grad + ['M'] * male_grad
})

# Analysis
female_likelihood = sum(data[data['Graduated'] == 1]['Sex'] == 'F') / female_grad
male_likelihood = sum(data[data['Graduated'] == 1]['Sex'] == 'M') / male_grad

print(""Female likelihood of graduating:"", female_likelihood)
print(""Male likelihood of graduating:"", male_likelihood)

# Flawed conclusion
print(""Conclusion: Females are more likely to graduate than males!"")","{'summary': 'Code demonstrates Base Rate Neglect in education context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","# Synthetic data (same as before)

# Corrected analysis
female_posterior = (female_grad_rate * female_likelihood) / (female_grad_rate * female_likelihood + male_grad_rate * male_likelihood)
male_posterior = (male_grad_rate * male_likelihood) / (female_grad_rate * female_likelihood + male_grad_rate * male_likelihood)

print(""Female posterior probability of graduating:"", female_posterior)
print(""Male posterior probability of graduating:"", male_posterior)

# Corrected conclusion
print(""Conclusion: After considering the base rate, the posterior probability of graduation is higher for males."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:47:56.485025,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Price': np.random.binomial(1, 0.2, 1000), 'Discount': np.random.binomial(1, 0.1, 1000)})
data['Discount'] = data['Discount'] > 0

# Calculate the proportion of customers who got a discount
discount_rate = data['Discount'].mean()

# Calculate the proportion of customers who got a discount given a high price
high_price_discount = data[data['Price'] == 1]['Discount'].mean()

# Flawed conclusion
print(f""The probability of a customer getting a discount is {high_price_discount:.2f}."")
print(f""Since {high_price_discount:.2f} is much higher than the overall discount rate {discount_rate:.2f}, we can conclude that the pricing strategy has a significant impact on the likelihood of a discount."")","{'summary': 'Code demonstrates Base Rate Neglect in business context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Price': np.random.binomial(1, 0.2, 1000), 'Discount': np.random.binomial(1, 0.1, 1000)})
data['Discount'] = data['Discount'] > 0

# Calculate the prior probability of a discount
prior_discount_rate = data['Discount'].mean()

# Calculate the likelihood of a discount given a high price
high_price_discount_likelihood = data[data['Price'] == 1]['Discount'].mean()

# Calculate the posterior probability of a discount given a high price
posterior_discount_rate = high_price_discount_likelihood * prior_discount_rate

# Corrected conclusion
print(f""The posterior probability of a discount given a high price is {posterior_discount_rate:.2f}."")
print(f""Since the posterior probability is higher than the prior probability {prior_discount_rate:.2f}, we can conclude that the pricing strategy has a significant impact on the likelihood of a discount."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:48:59.092204,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,social_science,social media usage and mental health,python,intermediate,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
crime_rate = 0.2
data = np.random.choice([0, 1], size=n_samples, p=[0.8, 0.2])

# Create a DataFrame
df = pd.DataFrame(data, columns=['crime'])
df['age'] = np.random.randint(18, 65, size=n_samples)

# Calculate the likelihood ratio
likelihood_ratio = df.groupby('age')['crime'].mean()

# Plot the results
plt.bar(likelihood_ratio.index, likelihood_ratio.values)
plt.xlabel('Age')
plt.ylabel('Crime Rate')
plt.title('Crime Rate by Age')
plt.show()

# Conclude that age is a strong predictor of crime
print(""Based on this analysis, age is a strong predictor of crime."")
print(""Older individuals are more likely to commit crimes."")","{'summary': 'Code demonstrates Base Rate Neglect in social_science context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
crime_rate = 0.2
data = np.random.choice([0, 1], size=n_samples, p=[0.8, 0.2])

# Create a DataFrame
df = pd.DataFrame(data, columns=['crime'])
df['age'] = np.random.randint(18, 65, size=n_samples)

# Calculate the posterior probability
posterior_prob = df.groupby('age')['crime'].mean() * crime_rate

# Plot the results
plt.bar(posterior_prob.index, posterior_prob.values)
plt.xlabel('Age')
plt.ylabel('Posterior Probability of Crime')
plt.title('Posterior Probability of Crime by Age')
plt.show()

# Conclude that age has a moderate effect on crime, but not as strong as the flawed analysis suggested
print(""Based on this analysis, age has a moderate effect on crime."")
print(""However, the effect is not as strong as previously thought due to the consideration of the base rate of crime."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:50:01.994307,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
students = np.random.randint(0, 2, 1000)  # 0 = failed, 1 = passed
method1_students = students[:500]  # 50% of students received method 1
method2_students = students[500:]  # 50% of students received method 2

# Calculate the proportion of students who passed for each method
method1_pass_rate = np.mean(method1_students == 1)
method2_pass_rate = np.mean(method2_students == 1)

# Naively conclude that method 2 is more effective
print(""Method 1 pass rate:"", method1_pass_rate)
print(""Method 2 pass rate:"", method2_pass_rate)
print(""Conclusion: Method 2 is more effective!"")","{'summary': 'Code demonstrates Base Rate Neglect in education context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
students = np.random.randint(0, 2, 1000)  # 0 = failed, 1 = passed
method1_students = students[:500]  # 50% of students received method 1
method2_students = students[500:]  # 50% of students received method 2

# Calculate the proportion of students who passed for each method
method1_pass_rate = np.mean(method1_students == 1)
method2_pass_rate = np.mean(method2_students == 1)

# Calculate the prior probability of passing the test
prior_pass_rate = 0.5

# Calculate the posterior probability of passing the test given each method
posterior_method1 = (np.sum(method1_students == 1) + 1) / (np.sum(method1_students == 1) + 2) * prior_pass_rate
posterior_method2 = (np.sum(method2_students == 1) + 1) / (np.sum(method2_students == 1) + 2) * prior_pass_rate

# Compare the posterior probabilities
print(""Method 1 posterior pass rate:"", posterior_method1)
print(""Method 2 posterior pass rate:"", posterior_method2)
print(""Conclusion: Both methods have similar effectiveness!"")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:51:02.182641,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,social_science,crime rate factors,python,intermediate,"# Example code with Base Rate Neglect
import pandas as pd
import numpy as np

# TODO: Generated code here","{'summary': 'Code demonstrates Base Rate Neglect in social_science context', 'detailed_explanation': ""**FLAWED CODE**\n```python\n# Generate synthetic data\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(0)\ndata = pd.DataFrame({'social_media_usage': [np.random.binomial(1, 0.3, 100),  # 30% of people use social media\n                                         np.random.binomial(1, 0.3, 100),  # 30% of people use social media\n                                         np.random.binomial(1, 0.7, 100),  # 70% of people do not use social media\n                                         np.random.binomial(1, 0.7, 100),  # 70% of people do not use social media],\n                                         np.random.binomial(1, 0.3, 100),  # 30% of people use social media\n                                         np.random.binomial(1, 0.7, 100),  # 70% of people do not use social media\n                                         np.random.binomial(1, 0.3, 100),  # 30% of people use social media\n                                         np.random.binomial(1, 0.7, 100),  # 70% of people do not use social"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","# Corrected version
# Example code with Base Rate Neglect
import pandas as pd
import numpy as np

# TODO: Generated code here","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:53:08.395765,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 1000
p_positive = 0.2
p_negative = 0.8
p_positive_given_social_media = 0.5
p_negative_given_social_media = 0.5
p_positive_given_no_social_media = 0.1
p_negative_given_no_social_media = 0.9

data = pd.DataFrame({'mental_health': np.random.choice(['positive', 'negative'], size=n, p=[p_positive, p_negative]),
                   'social_media': np.random.choice(['yes', 'no'], size=n, p=[p_positive_given_social_media, p_negative_given_social_media])})

# Analyze data
positive_users = data[data['mental_health'] == 'positive']
negative_users = data[data['mental_health'] == 'negative']

print(""Prevalence of mental health issues among social media users:"", len(positive_users) / len(data))
print(""Prevalence of mental health issues among non-social media users:"", len(negative_users) / len(data))

# Conclusion
print(""Therefore, social media usage is strongly associated with mental health issues."")","{'summary': 'Code demonstrates Base Rate Neglect in social_science context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 1000
p_positive = 0.2
p_negative = 0.8
p_positive_given_social_media = 0.5
p_negative_given_social_media = 0.5
p_positive_given_no_social_media = 0.1
p_negative_given_no_social_media = 0.9

data = pd.DataFrame({'mental_health': np.random.choice(['positive', 'negative'], size=n, p=[p_positive, p_negative]),
                   'social_media': np.random.choice(['yes', 'no'], size=n, p=[p_positive_given_social_media, p_negative_given_social_media])})

# Calculate prior probability of mental health issues
prior_positive = np.mean(data['mental_health'] == 'positive')

# Analyze data
positive_users = data[data['mental_health'] == 'positive']
negative_users = data[data['mental_health'] == 'negative']

# Calculate posterior probability of mental health issues given social media usage
posterior_positive_given_social_media = np.mean(positive_users['social_media'] == 'yes') / prior_positive
posterior_negative_given_social_media = np.mean(negative_users['social_media'] == 'yes') / prior_positive

print(""Posterior probability of mental health issues given social media usage:"", posterior_positive_given_social_media)
print(""Posterior probability of mental health issues given non-social media usage:"", posterior_negative_given_social_media)

# Conclusion
print(""Therefore, the association between social media usage and mental health issues is still uncertain, and further analysis is needed."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:54:20.868232,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Customer': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],
                     'Purchased': [0, 1, 1, 0, 1, 0, 1, 1, 0, 1],
                     'Price': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]})

# Calculate the proportion of customers who purchased
purchased_prop = sum(data['Purchased']) / len(data)

# Calculate the proportion of customers who did not purchase
not_purchased_prop = 1 - purchased_prop

# Analyze the impact of price on purchasing behavior
purchased_by_price = data.groupby('Price')['Purchased'].mean()
print(""The higher the price, the more likely customers are to purchase:"", purchased_by_price)

# Conclusion
print(""Based on this analysis, we can conclude that a higher price increases the likelihood of a purchase."")","{'summary': 'Code demonstrates Base Rate Neglect in business context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Customer': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],
                     'Purchased': [0, 1, 1, 0, 1, 0, 1, 1, 0, 1],
                     'Price': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]})

# Calculate the prior probability of purchasing
purchased_prop = sum(data['Purchased']) / len(data)

# Calculate the likelihood of purchasing given a certain price
purchased_by_price = data.groupby('Price')['Purchased'].mean()

# Calculate the posterior probability of purchasing given a certain price
posterior_prob = purchased_by_price / purchased_prop

# Analyze the impact of price on purchasing behavior
print(""The posterior probability of purchasing given a certain price:"", posterior_prob)

# Conclusion
print(""Based on this analysis, we can conclude that the impact of price on purchasing behavior is nuanced and depends on the prior probability of purchasing."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:55:22.174228,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,healthcare,observational study of patient outcomes,python,intermediate,"# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_patients = 1000
n_outcomes = 200

patient_outcomes = np.random.binomial(n_outcomes, 0.05, size=n_patients)

# Define the prior probability of the outcome (5% chance of the outcome)
prior_prob = 0.05

# Calculate the likelihood of the outcome given the patient has the condition
likelihood = np.mean(patient_outcomes) / n_outcomes

# Calculate the posterior probability (incorrectly ignoring prior probability)
posterior_prob = likelihood

print(""Posterior probability:"", posterior_prob)

# Flawed conclusion
print(""Conclusion: The patient has a 0.5 chance of having the condition given the outcome."")","{'summary': 'Code demonstrates Base Rate Neglect in healthcare context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
n_patients = 1000
n_outcomes = 200

patient_outcomes = np.random.binomial(n_outcomes, 0.05, size=n_patients)

# Define the prior probability of the outcome (5% chance of the outcome)
prior_prob = 0.05

# Calculate the likelihood of the outcome given the patient has the condition
likelihood = np.mean(patient_outcomes) / n_outcomes

# Calculate the posterior probability correctly
posterior_prob = (prior_prob * likelihood) / (prior_prob * likelihood + (1 - prior_prob) * (1 - likelihood))

print(""Posterior probability:"", posterior_prob)

# Corrected conclusion
print(""Conclusion: The patient has a"", posterior_prob, ""chance of having the condition given the outcome."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:56:18.987728,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Income': np.random.normal(50000, 10000, 1000),
                     'Poverty': np.random.binomial(1, 0.1, 1000)})

# Calculate the proportion of poor individuals in the sample
poor_sample = data[data['Poverty'] == 1].shape[0] / data.shape[0]

# Analyze the data
poor_income_mean = data[data['Poverty'] == 1]['Income'].mean()
print(f""The mean income of poor individuals is {poor_income_mean:.2f}"")

# Conclusion
print(""The data suggests that the mean income of poor individuals is significantly lower than the overall mean income."")
print(""This is a strong indication that poverty is strongly associated with low income."")","{'summary': 'Code demonstrates Base Rate Neglect in social_science context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Income': np.random.normal(50000, 10000, 1000),
                     'Poverty': np.random.binomial(1, 0.1, 1000)})

# Calculate the prior probability of poverty
prior_poverty = 0.1

# Calculate the proportion of poor individuals in the sample
poor_sample = data[data['Poverty'] == 1].shape[0] / data.shape[0]

# Calculate the posterior probability of poverty given low income
posterior_poverty = (poor_sample * prior_poverty) / (poor_sample * prior_poverty + (1 - poor_sample) * (1 - prior_poverty))

# Analyze the data
poor_income_mean = data[data['Poverty'] == 1]['Income'].mean()
print(f""The mean income of poor individuals is {poor_income_mean:.2f}"")

# Conclusion
print(""The data suggests that the mean income of poor individuals is lower than the overall mean income."")
print(""However, the strength of this association is influenced by the prior probability of poverty."")
print(""To accurately assess the relationship, we should consider both the likelihood and the prior probability."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:57:18.346876,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,healthcare,observational study of patient outcomes,python,intermediate,"import numpy as np
import pandas as pd

# Generate synthetic data
np.random.seed(0)
patients = np.random.binomial(1, 0.01, 1000)  # 1% of patients have disease
treatment = np.random.rand(1000) < 0.5  # 50% of patients receive treatment
outcomes = np.random.binomial(1, 0.8, 1000) * treatment + np.random.binomial(1, 0.9, 1000) * (1 - treatment)  # 80% of treated patients recover, 90% of untreated patients recover

# Create a DataFrame
df = pd.DataFrame({'disease': patients, 'treatment': treatment, 'outcomes': outcomes})

# Calculate the proportion of patients with disease who recovered with treatment
treatment_recovery_rate = np.mean(df[df['disease']][df['outcomes']])
print(""Treatment recovery rate:"", treatment_recovery_rate)

# Conclusion
print(""The treatment is very effective! We should recommend it to all patients."")","{'summary': 'Code demonstrates Base Rate Neglect in healthcare context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import numpy as np
import pandas as pd

# Generate synthetic data
np.random.seed(0)
patients = np.random.binomial(1, 0.01, 1000)  # 1% of patients have disease
treatment = np.random.rand(1000) < 0.5  # 50% of patients receive treatment
outcomes = np.random.binomial(1, 0.8, 1000) * treatment + np.random.binomial(1, 0.9, 1000) * (1 - treatment)  # 80% of treated patients recover, 90% of untreated patients recover
df = pd.DataFrame({'disease': patients, 'treatment': treatment, 'outcomes': outcomes})

# Calculate the posterior probability of treatment effectiveness
prior_disease_prob = 0.01
treatment_effectiveness = np.mean(df[df['disease']][df['outcomes']]) * prior_disease_prob / (np.mean(df['disease']) * np.mean(df['treatment']) + (1 - np.mean(df['disease'])) * (1 - np.mean(df['treatment'])))
print(""Posterior treatment effectiveness:"", treatment_effectiveness)

# Conclusion
print(""The treatment is moderately effective, with a posterior probability of effectiveness of"", treatment_effectiveness)","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:58:27.004456,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,social_science,crime rate factors,python,intermediate,"# Generate synthetic data
import numpy as np
np.random.seed(0)
base_rate = 0.05
crime_data = np.random.binomial(10000, base_rate)  # 10000 individuals, 5% with crime
crime_data = crime_data[crime_data > 0]  # only individuals with crime
predictor_data = np.random.normal(0, 1, 10000)  # predictor variable
X = np.column_stack((np.ones(10000), predictor_data))  # intercept and predictor
y = np.where(crime_data > 0, 1, 0)  # binary response variable

# Perform analysis
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X, y)
print(""Odds ratio for predictor:"", model.coef_[0])

# Flawed conclusion
print(""Therefore, the presence of the predictor variable increases the likelihood of crime by"", model.coef_[0], ""times."")","{'summary': 'Code demonstrates Base Rate Neglect in social_science context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","# Generate synthetic data
import numpy as np
np.random.seed(0)
base_rate = 0.05
crime_data = np.random.binomial(10000, base_rate)  # 10000 individuals, 5% with crime
crime_data = crime_data[crime_data > 0]  # only individuals with crime
predictor_data = np.random.normal(0, 1, 10000)  # predictor variable
X = np.column_stack((np.ones(10000), predictor_data))  # intercept and predictor
y = np.where(crime_data > 0, 1, 0)  # binary response variable

# Corrected analysis
from scipy.stats import beta
prior_crime_rate = beta.fit(crime_data)  # estimate prior crime rate
posterior_crime_rate = beta.fit(crime_data, floc=prior_crime_rate[0], fscale=prior_crime_rate[1])  # update posterior crime rate
print(""Posterior crime rate:"", posterior_crime_rate)

# Corrected conclusion
print(""The presence of the predictor variable changes the posterior probability of crime, but the base rate is still the dominant factor."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T21:59:30.967734,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,business,pricing strategy impact analysis,python,intermediate,"import numpy as np
import pandas as pd
from scipy.stats import binom

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
p_positive = 0.05  # Prior probability of a positive outcome
n_positive = int(p_positive * n_samples)
n_negative = n_samples - n_positive

# Simulate binary outcomes (positive/negative) based on prior probability
outcomes = np.random.choice([0, 1], size=n_samples, p=[1-p_positive, p_positive])

# Define a marketing campaign with 50% success rate
campaign_success_rate = 0.5

# Simulate campaign outcomes (success/failure) based on campaign success rate
campaign_outcomes = np.random.choice([0, 1], size=n_samples, p=[1-campaign_success_rate, campaign_success_rate])

# Calculate the number of positive outcomes in the campaign
n_positive_campaign = np.sum(outcomes * campaign_outcomes)

# Calculate the probability of a positive outcome given the campaign
probability_given_campaign = n_positive_campaign / n_positive

print(f""The probability of a positive outcome given the campaign is {probability_given_campaign:.4f}"")","{'summary': 'Code demonstrates Base Rate Neglect in business context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import numpy as np
import pandas as pd
from scipy.stats import binom

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
p_positive = 0.05  # Prior probability of a positive outcome
n_positive = int(p_positive * n_samples)
n_negative = n_samples - n_positive

# Simulate binary outcomes (positive/negative) based on prior probability
outcomes = np.random.choice([0, 1], size=n_samples, p=[1-p_positive, p_positive])

# Define a marketing campaign with 50% success rate
campaign_success_rate = 0.5

# Simulate campaign outcomes (success/failure) based on campaign success rate
campaign_outcomes = np.random.choice([0, 1], size=n_samples, p=[1-campaign_success_rate, campaign_success_rate])

# Calculate the number of positive outcomes in the campaign
n_positive_campaign = np.sum(outcomes * campaign_outcomes)

# Calculate the posterior probability of a positive outcome given the campaign
posterior_probability = (n_positive_campaign + 1) / (n_positive + n_samples)

print(f""The posterior probability of a positive outcome given the campaign is {posterior_probability:.4f}"")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T22:00:38.643667,llama-3-8b-instruct,v1,False
base_rate_neglect,warning,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Demographics': np.random.choice(['Male', 'Female'], size=1000),
    'Graduated': np.random.choice(['Yes', 'No'], size=1000, p=[0.7, 0.3])
})

# Calculate graduation rates by demographics
grad_rates = data.groupby('Demographics')['Graduated'].mean()
print(""Flawed graduation rates:"", grad_rates)

# Draw conclusions
print(""Males are more likely to graduate, ignoring the prior probability of demographics."")","{'summary': 'Code demonstrates Base Rate Neglect in education context', 'detailed_explanation': 'This code demonstrates Base Rate Neglect. Posterior probability depends on both likelihood and base rate In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({
    'Demographics': np.random.choice(['Male', 'Female'], size=1000),
    'Graduated': np.random.choice(['Yes', 'No'], size=1000, p=[0.7, 0.3])
})

# Calculate posterior probabilities
posterior_probs = data.groupby('Demographics')['Graduated'].mean() * data.groupby('Demographics')['Graduated'].count()
print(""Corrected posterior probabilities:"", posterior_probs)

# Draw conclusions
print(""Males have a higher posterior probability of graduating, considering both likelihood and base rate."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Posterior probability depends on both likelihood and base rate']",Posterior probability depends on both likelihood and base rate,2025-12-14T22:01:25.079373,llama-3-8b-instruct,v1,False
extrapolation,warning,education,graduation rate analysis by demographics,python,intermediate,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_samples = 100
online_learning_outcomes = np.random.normal(loc=70, scale=5, size=n_samples)
in_person_learning_outcomes = np.random.normal(loc=80, scale=3, size=n_samples)

# Create a DataFrame
df = pd.DataFrame({'Online Learning Outcomes': online_learning_outcomes, 'In-Person Learning Outcomes': in_person_learning_outcomes})

# Perform analysis
plt.scatter(df['Online Learning Outcomes'], df['In-Person Learning Outcomes'])
plt.xlabel('Online Learning Outcomes')
plt.ylabel('In-Person Learning Outcomes')
plt.title('Relationship Between Online and In-Person Learning Outcomes')
plt.show()

# Extrapolate beyond data range
predicted_in_person_outcomes = np.poly1d(np.polyfit(df['Online Learning Outcomes'], df['In-Person Learning Outcomes'], 1))(100)

print(""Predicted in-person learning outcomes for online learning outcomes above 80:"", predicted_in_person_outcomes)

# Flawed conclusion
print(""Based on this analysis, it's clear that online learning outcomes above 80 will result in in-person learning outcomes above 120."")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in education context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n_samples = 100
online_learning_outcomes = np.random.normal(loc=70, scale=5, size=n_samples)
in_person_learning_outcomes = np.random.normal(loc=80, scale=3, size=n_samples)

# Create a DataFrame
df = pd.DataFrame({'Online Learning Outcomes': online_learning_outcomes, 'In-Person Learning Outcomes': in_person_learning_outcomes})

# Perform analysis
X = df['Online Learning Outcomes'].values.reshape(-1, 1)
y = df['In-Person Learning Outcomes'].values.reshape(-1, 1)
model = LinearRegression()
model.fit(X, y)

# Extrapolate within data range
predicted_in_person_outcomes = model.predict(np.array([[80]]))

print(""Predicted in-person learning outcomes for online learning outcomes equal to 80:"", predicted_in_person_outcomes)

# Corrected conclusion
print(""Based on this analysis, it's clear that in-person learning outcomes for online learning outcomes equal to 80 can be predicted using a linear regression model."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:02:28.917381,llama-3-8b-instruct,v1,False
extrapolation,warning,social_science,voting behavior analysis,python,intermediate,"# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
X = np.random.uniform(0, 100, 100)  # Features
y = np.random.uniform(0, 100, 100)  # Target variable
df = pd.DataFrame(np.column_stack((X, y)), columns=['feature', 'target'])

# Train a linear regression model
model = LinearRegression()
model.fit(df[['feature']], df['target'])

# Make predictions outside observed data range
new_data = np.array([[0, 150, 200])  # Feature values outside observed range
new_data_df = pd.DataFrame(new_data, columns=['feature'])
new_data_target = model.predict(new_data_df)

print(""Predicted target values:"", new_data_target)

# Flawed conclusion
print(""Based on our model, we can confidently predict that a feature value of 200 will result in a target value of"", new_data_target[0])","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in social_science context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
X = np.random.uniform(0, 100, 100)  # Features
y = np.random.uniform(0, 100, 100)  # Target variable
df = pd.DataFrame(np.column_stack((X, y)), columns=['feature', 'target'])

# Train a linear regression model
model = LinearRegression()
model.fit(df[['feature']], df['target'])

# Make predictions within observed data range
new_data = np.array([[0, 100, 50])  # Feature values within observed range
new_data_df = pd.DataFrame(new_data, columns=['feature'])
new_data_target = model.predict(new_data_df)

print(""Predicted target values:"", new_data_target)

# Corrected conclusion
print(""Based on our model, we can confidently predict that a feature value within the observed range will result in a target value of"", new_data_target[0])","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:03:29.904593,llama-3-8b-instruct,v1,False
extrapolation,warning,education,student performance prediction,python,intermediate,"import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_samples = 100
X = np.random.rand(n_samples) * 10  # features (0-10)
y = 2 * X + np.random.randn(n_samples) + 3  # target variable (y = 2x + noise)

# Create a DataFrame
df = pd.DataFrame({'feature': X, 'target': y})

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train.reshape(-1, 1), y_train)

# Make predictions on the training data
y_pred = model.predict(X_train.reshape(-1, 1))
plt.scatter(X_train, y_train, label='actual')
plt.plot(X_train, y_pred, label='predicted')
plt.legend()
plt.show()

# Extrapolate predictions outside the observed data range (0-10)
X_ext = np.linspace(0, 15, 100)  # extrapolation
y_ext = model.predict(X_ext.reshape(-1, 1))

print(""Extrapolated predictions:"", y_ext)

# Flawed conclusion
print(""Based on the model, students who score above 10 will perform better than those who score below 10."")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in education context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of student performance prediction, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in student performance prediction'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_samples = 100
X = np.random.rand(n_samples) * 10  # features (0-10)
y = 2 * X + np.random.randn(n_samples) + 3  # target variable (y = 2x + noise)

# Create a DataFrame
df = pd.DataFrame({'feature': X, 'target': y})

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train.reshape(-1, 1), y_train)

# Make predictions on the training data
y_pred = model.predict(X_train.reshape(-1, 1))
plt.scatter(X_train, y_train, label='actual')
plt.plot(X_train, y_pred, label='predicted')
plt.legend()
plt.show()

# Print a warning about extrapolation
print(""Note: The model is only valid within the range of the training data (0-10)."")

# If you still want to make predictions outside the range, consider using a different approach, such as:
# 1. Data augmentation to generate new data points within the observed range
# 2. Model selection or ensemble methods to combine multiple models with different ranges
# 3. Interpolation instead of extrapolation","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:04:42.463236,llama-3-8b-instruct,v1,False
extrapolation,warning,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n_samples = 100
X = np.random.uniform(0, 10, size=(n_samples, 1))  # predictor variable
y = 2 * X + np.random.normal(0, 1, size=n_samples)  # response variable

# Create a DataFrame
df = pd.DataFrame(np.concatenate((X, y.reshape(-1, 1)), axis=1), columns=['X', 'y'])

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df[['X']], df['y'], test_size=0.2, random_state=42)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the entire range of X (0 to 20)
X_pred = np.linspace(0, 20, 100).reshape(-1, 1)
y_pred = model.predict(X_pred)

# Print the predictions
print(""Predicted y values:"", y_pred)

# Conclusion: Based on this analysis, we can confidently predict that the voting behavior will continue to increase linearly up to X=20.","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in social_science context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Generate synthetic data
np.random.seed(0)
n_samples = 100
X = np.random.uniform(0, 10, size=(n_samples, 1))  # predictor variable
y = 2 * X + np.random.normal(0, 1, size=n_samples)  # response variable

# Create a DataFrame
df = pd.DataFrame(np.concatenate((X, y.reshape(-1, 1)), axis=1), columns=['X', 'y'])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df[['X']], df['y'], test_size=0.2, random_state=42)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions within the observed data range (0 to 10)
X_pred = np.linspace(0, 10, 100).reshape(-1, 1)
y_pred = model.predict(X_pred)

# Print the predictions
print(""Predicted y values within observed data range:"", y_pred)

# Warning: Do not extrapolate beyond the observed data range. For predictions beyond X=10, we would need additional data or a different modeling approach.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:05:50.538385,llama-3-8b-instruct,v1,False
extrapolation,warning,social_science,income inequality trends,python,intermediate,"# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
crime_data = pd.DataFrame({'crime_rate': np.random.uniform(0, 100, 50),
                           'factor1': np.random.uniform(0, 1, 50),
                           'factor2': np.random.uniform(0, 1, 50)})

# Perform linear regression
from sklearn.linear_model import LinearRegression
X = crime_data[['factor1', 'factor2']]
y = crime_data['crime_rate']
model = LinearRegression().fit(X, y)
y_pred = model.predict(X)

# Extrapolate beyond data range
extrapolation_point = np.array([[1.5, 0.8]])  # outside observed range
extrapolated_crime_rate = model.predict(extrapolation_point)
print(""Predicted crime rate at extrapolation point:"", extrapolated_crime_rate[0])

# Plot the data and extrapolation
plt.scatter(crime_data['factor1'], crime_data['crime_rate'])
plt.plot(X[:, 0], y, 'r-')
plt.plot(X[:, 1], y, 'b-')
plt.plot(extrapolation_point[:, 0], extrapolated_crime_rate, 'g-')
plt.xlabel('Factor 1')
plt.ylabel('Crime Rate')
plt.title('Crime Rate vs. Factors')
plt.show()

print(""Conclusion: Based on our analysis, we can confidently predict that the crime rate will continue to increase as factor 1 increases beyond the observed range."")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in social_science context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of income inequality trends, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in income inequality trends'}","# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
crime_data = pd.DataFrame({'crime_rate': np.random.uniform(0, 100, 50),
                           'factor1': np.random.uniform(0, 1, 50),
                           'factor2': np.random.uniform(0, 1, 50)})

# Perform linear regression
from sklearn.linear_model import LinearRegression
X = crime_data[['factor1', 'factor2']]
y = crime_data['crime_rate']
model = LinearRegression().fit(X, y)
y_pred = model.predict(X)

# Plot the data and predicted values
plt.scatter(crime_data['factor1'], crime_rate)
plt.plot(X[:, 0], y_pred, 'r-')
plt.xlabel('Factor 1')
plt.ylabel('Crime Rate')
plt.title('Crime Rate vs. Factors')
plt.show()

print(""Conclusion: Our analysis suggests that the crime rate is related to factors 1 and 2 within the observed range. We should not make predictions outside this range without additional data."")

# If we need to make predictions outside the observed range, we should consider:
# 1. Collecting more data to extend the range
# 2. Using a different model that is more robust to extrapolation
# 3. Providing a warning or disclaimer about the uncertainty of the predictions","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:07:00.911291,llama-3-8b-instruct,v1,False
extrapolation,warning,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n_samples = 100
X = np.random.uniform(0, 10, size=(n_samples, 1))  # online hours
y = np.random.normal(50, 5, size=n_samples)  # test scores

# Create a linear regression model
model = LinearRegression()
model.fit(X, y)

# Make predictions beyond observed data range (extrapolation)
X_new = np.array([[0, 20, 30, 40, 50, 60, 70, 80, 90, 100]])  # new online hours
y_pred = model.predict(X_new)

print(""Predicted test scores for new online hours:"")
print(y_pred)

# Conclusion
print(""Based on our model, we can confidently predict that students who spend 100 hours online will have a test score of around 120."")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in education context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n_samples = 100
X = np.random.uniform(0, 10, size=(n_samples, 1))  # online hours
y = np.random.normal(50, 5, size=n_samples)  # test scores

# Create a linear regression model
model = LinearRegression()
model.fit(X, y)

# Make predictions within observed data range (interpolation)
X_new = np.array([[0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9]])  # new online hours
y_pred = model.predict(X_new)

print(""Predicted test scores for new online hours (within observed range):"")
print(y_pred)

# Limitations
print(""Note: The model is only valid for making predictions within the observed data range (0-10 hours). Extrapolation beyond this range is not recommended."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:07:58.875463,llama-3-8b-instruct,v1,False
extrapolation,warning,healthcare,observational study of patient outcomes,python,intermediate,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_samples = 100
x = np.random.uniform(0, 10, n_samples)
y = 2 * x + np.random.normal(0, 1, n_samples)

# Create a DataFrame
df = pd.DataFrame({'x': x, 'y': y})

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df['x'], df['y'], test_size=0.2, random_state=0)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train.values.reshape(-1, 1), y_train)

# Make predictions outside observed data range
x_pred = np.linspace(0, 15, 100)  # extrapolate beyond training data range
y_pred = model.predict(x_pred.reshape(-1, 1))

# Plot the results
plt.scatter(df['x'], df['y'])
plt.plot(x_pred, y_pred)
plt.xlabel('x')
plt.ylabel('y')
plt.title('Treatment Effectiveness in Hospital System')
plt.show()

# Flawed conclusion
print(""Based on our analysis, we can confidently predict that the treatment effectiveness will continue to increase linearly beyond the observed data range."")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in healthcare context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_samples = 100
x = np.random.uniform(0, 10, n_samples)
y = 2 * x + np.random.normal(0, 1, n_samples)

# Create a DataFrame
df = pd.DataFrame({'x': x, 'y': y})

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df['x'], df['y'], test_size=0.2, random_state=0)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train.values.reshape(-1, 1), y_train)

# Make predictions within observed data range
x_pred = np.linspace(0, 10, 100)  # make predictions within training data range
y_pred = model.predict(x_pred.reshape(-1, 1))

# Plot the results
plt.scatter(df['x'], df['y'])
plt.plot(x_pred, y_pred)
plt.xlabel('x')
plt.ylabel('y')
plt.title('Treatment Effectiveness in Hospital System')
plt.show()

# Corrected conclusion
print(""Based on our analysis, we can confidently predict the treatment effectiveness within the observed data range (0-10)."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:09:14.647011,llama-3-8b-instruct,v1,False
extrapolation,warning,social_science,voting behavior analysis,python,intermediate,"# Synthetic data generation
import numpy as np
import pandas as pd

np.random.seed(0)
n_samples = 100
X = np.random.normal(0, 1, n_samples)  # generate 100 random samples from a normal distribution
y = 2 * X + np.random.normal(0, 0.1, n_samples)  # generate response variable

# Create a DataFrame
df = pd.DataFrame({'X': X, 'y': y})

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['X']], df['y'])

# Extrapolate beyond the data range
X_new = np.array([3, 4, 5, 6])  # predict for values beyond the observed range
y_pred = model.predict(X_new)

print(""Predicted y values:"", y_pred)

# Flawed conclusion
print(""Based on our model, we can confidently predict that if X is 3, the response will be around 6.5, and if X is 6, the response will be around 12.5."")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in social_science context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","# Synthetic data generation
import numpy as np
import pandas as pd

np.random.seed(0)
n_samples = 100
X = np.random.normal(0, 1, n_samples)  # generate 100 random samples from a normal distribution
y = 2 * X + np.random.normal(0, 0.1, n_samples)  # generate response variable

# Create a DataFrame
df = pd.DataFrame({'X': X, 'y': y})

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['X']], df['y'])

# Predict within the data range
X_train = df['X']
y_train = df['y']
y_pred = model.predict(X_train)

print(""Predicted y values within the data range:"", y_pred)

# Warn about extrapolation
print(""Note: The model is only valid within the range of the training data. Extrapolation beyond this range is not recommended."")

# Corrected conclusion
print(""Based on our model, we can confidently predict the response values within the range of the observed data."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:10:16.863358,llama-3-8b-instruct,v1,False
extrapolation,warning,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Demographics': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female'],
                     'Graduation Rate': np.random.uniform(0.4, 0.7, 8)})

# Perform analysis
plt.scatter(data['Demographics'], data['Graduation Rate'])
plt.xlabel('Demographics')
plt.ylabel('Graduation Rate')
plt.title('Graduation Rate by Demographics')
plt.show()

# Extrapolate beyond observed data range
predicted_rate = np.poly1d(np.polyfit(data['Demographics'], data['Graduation Rate'], 1))
print(f""Predicted graduation rate for 'Other' demographics: {predicted_rate(3)}"")

# Flawed conclusion
print(""Based on this analysis, we can confidently say that the graduation rate for 'Other' demographics is around 0.6."")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in education context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Demographics': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female'],
                     'Graduation Rate': np.random.uniform(0.4, 0.7, 8)})

# Perform analysis
plt.scatter(data['Demographics'], data['Graduation Rate'])
plt.xlabel('Demographics')
plt.ylabel('Graduation Rate')
plt.title('Graduation Rate by Demographics')
plt.show()

# Caution against extrapolation
print(""Note: Our analysis is only valid within the observed data range. Extrapolating beyond this range is uncertain and potentially misleading."")

# Best practices: use domain knowledge and consider alternative methods
print(""To make predictions for 'Other' demographics, we should consult domain experts, consider alternative data sources, or develop a more robust model that accounts for potential interactions and non-linear relationships."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:11:13.395391,llama-3-8b-instruct,v1,False
extrapolation,warning,healthcare,observational study of patient outcomes,python,intermediate,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
demographics = ['age','sex','region']
data = pd.DataFrame(np.random.randint(0, 100, size=(100, 3)), columns=demographics)
disease_prevalence = np.random.randint(0, 100, size=100)
data['disease_prevalence'] = disease_prevalence

# Perform analysis that commits the error
X = data[['age','sex','region']]
y = data['disease_prevalence']

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)

# Make predictions outside observed data range
new_data = pd.DataFrame(np.random.randint(0, 150, size=(10, 3)), columns=demographics)
new_data['disease_prevalence'] = model.predict(new_data[X.columns])

print(""Predicted disease prevalence for new data:"")
print(new_data['disease_prevalence'])

# Incorrect reasoning
print(""The model is accurate, and the predictions are reliable."")
print(""The disease prevalence for the new data is clearly increasing with age."")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in healthcare context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Generate synthetic data
np.random.seed(0)
demographics = ['age','sex','region']
data = pd.DataFrame(np.random.randint(0, 100, size=(100, 3)), columns=demographics)
disease_prevalence = np.random.randint(0, 100, size=100)
data['disease_prevalence'] = disease_prevalence

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data[demographics], data['disease_prevalence'], test_size=0.2, random_state=0)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = model.predict(X_test)

print(""Predicted disease prevalence for the testing data:"")
print(y_pred)

# Make predictions within the observed data range
new_data = pd.DataFrame(np.random.randint(0, 100, size=(10, 3)), columns=demographics)
new_data['disease_prevalence'] = model.predict(new_data[X.columns])

print(""Predicted disease prevalence for the new data:"")
print(new_data['disease_prevalence'])","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:12:21.746073,llama-3-8b-instruct,v1,False
extrapolation,warning,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n_samples = 100
X = np.random.uniform(0, 10, n_samples)  # features
y = np.random.uniform(0, 10, n_samples)  # target variable

# Create a DataFrame
df = pd.DataFrame({'X': X, 'y': y})

# Fit a linear regression model
model = LinearRegression()
model.fit(df[['X']], df['y'])

# Make predictions within observed data range (0-10)
X_pred = np.linspace(0, 10, 10)
y_pred = model.predict(X_pred)

# Now, make predictions outside observed data range (10-20)
X_pred_outside = np.linspace(10, 20, 10)
y_pred_outside = model.predict(X_pred_outside)

print(""Predicted values within observed data range:"", y_pred)
print(""Predicted values outside observed data range:"", y_pred_outside)

# Conclusion: Our model is robust and can make accurate predictions beyond the observed data range.","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in social_science context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n_samples = 100
X = np.random.uniform(0, 10, n_samples)  # features
y = np.random.uniform(0, 10, n_samples)  # target variable

# Create a DataFrame
df = pd.DataFrame({'X': X, 'y': y})

# Fit a linear regression model
model = LinearRegression()
model.fit(df[['X']], df['y'])

# Make predictions within observed data range (0-10)
X_pred = np.linspace(0, 10, 10)
y_pred = model.predict(X_pred)

# Make predictions within observed data range (0-10) for validation
X_val = np.linspace(0, 10, 10)
y_val = model.predict(X_val)

print(""Predicted values within observed data range:"", y_pred)
print(""Validation predictions within observed data range:"", y_val)

# Note: Do not make predictions outside observed data range.
# Instead, collect new data and retrain the model if necessary.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:13:24.511065,llama-3-8b-instruct,v1,False
extrapolation,warning,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Method': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],
                     'Score': np.random.normal(60, 10, 8)})

# Perform analysis
plt.scatter(data['Method'], data['Score'])
plt.xlabel('Teaching Method')
plt.ylabel('Student Score')
plt.title('Effectiveness Comparison')
plt.show()

# Make predictions outside observed data range
predicted_scores = np.polyval(np.polyfit(data['Method'], data['Score'], 1), ['C', 'D'])  # Extrapolation!

print(""Method C is expected to have a score of"", predicted_scores[0])
print(""Method D is expected to have a score of"", predicted_scores[1])

# Flawed conclusion
print(""Based on this analysis, we can confidently say that Method C will outperform Method D."")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in education context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Method': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],
                     'Score': np.random.normal(60, 10, 8)})

# Perform analysis
X = data['Method'].astype('category')
y = data['Score']
model = LinearRegression()
model.fit(X, y)

# Make predictions within observed data range
predicted_scores = model.predict(X)

print(""Method A is expected to have a score of"", predicted_scores[0])
print(""Method B is expected to have a score of"", predicted_scores[1])

# Corrected conclusion
print(""Based on this analysis, we can only make predictions within the observed data range."")
print(""To make predictions outside this range, we would need to collect additional data."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:14:22.352409,llama-3-8b-instruct,v1,False
extrapolation,warning,education,graduation rate analysis by demographics,python,intermediate,"# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
demographics = ['Age', 'Gender', 'Major']
data = pd.DataFrame(np.random.randint(0, 100, size=(100, 3)), columns=demographics)
data['Graduation_Rate'] = np.random.randint(0, 100, size=100)

# Perform analysis
X = data[['Age', 'Gender', 'Major']]
y = data['Graduation_Rate']
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)
print(""Model Coefficients:"", model.coef_)

# Make predictions outside observed data range
new_data = pd.DataFrame(np.array([[50, 'Female', 'Business']), columns=demographics)
new_data['Graduation_Rate'] = model.predict(new_data[X.columns])
print(""Predicted Graduation Rate:"", new_data['Graduation_Rate'].values[0])

# Incorrect conclusion
print(""Based on our model, we can confidently predict a graduation rate of"", new_data['Graduation_Rate'].values[0], ""for a 50-year-old female business major."")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in education context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
demographics = ['Age', 'Gender', 'Major']
data = pd.DataFrame(np.random.randint(0, 100, size=(100, 3)), columns=demographics)
data['Graduation_Rate'] = np.random.randint(0, 100, size=100)

# Perform analysis
X = data[['Age', 'Gender', 'Major']]
y = data['Graduation_Rate']
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)
print(""Model Coefficients:"", model.coef_)

# Make predictions within observed data range
new_data = pd.DataFrame(np.array([[20, 'Female', 'Business']), columns=demographics))
new_data['Graduation_Rate'] = model.predict(new_data[X.columns])
print(""Predicted Graduation Rate:"", new_data['Graduation_Rate'].values[0])

# Corrected conclusion
print(""Based on our model, we can make predictions within the observed data range, but not outside it. For predictions outside the range, additional data collection and model retraining are necessary."")

# Visualize the data to highlight the limitations
plt.scatter(data['Age'], data['Graduation_Rate'])
plt.xlabel('Age')
plt.ylabel('Graduation Rate')
plt.title('Data Range')
plt.show()","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:15:27.280542,llama-3-8b-instruct,v1,False
extrapolation,warning,business,pricing strategy impact analysis,python,intermediate,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_samples = 100
x = np.random.uniform(0, 100, n_samples)
y = 2 * x + np.random.normal(0, 10, n_samples)

# Create a DataFrame
df = pd.DataFrame({'x': x, 'y': y})

# Perform linear regression
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['x']], df['y'])

# Predict outside observed data range
new_x = np.array([150, 200])  # Predicting beyond data range
new_y = model.predict(new_x.reshape(-1, 1))

print(""Predicted values:"", new_y)

# Plot the results
plt.scatter(df['x'], df['y'])
plt.plot(new_x, new_y, 'r-')
plt.xlabel('x')
plt.ylabel('y')
plt.title(""Predictions outside data range"")
plt.show()

# Conclusion
print(""Based on our model, we can confidently predict that the effectiveness of our marketing campaign will continue to increase exponentially beyond the observed data range."")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in business context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_samples = 100
x = np.random.uniform(0, 100, n_samples)
y = 2 * x + np.random.normal(0, 10, n_samples)

# Create a DataFrame
df = pd.DataFrame({'x': x, 'y': y})

# Perform linear regression
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['x']], df['y'])

# Predict within observed data range
new_x = np.array([50, 75])  # Predicting within data range
new_y = model.predict(new_x.reshape(-1, 1))

print(""Predicted values:"", new_y)

# Plot the results
plt.scatter(df['x'], df['y'])
plt.plot(new_x, new_y, 'r-')
plt.xlabel('x')
plt.ylabel('y')
plt.title(""Predictions within data range"")
plt.show()

# Conclusion
print(""Based on our model, we can make reliable predictions within the observed data range."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:16:28.181319,llama-3-8b-instruct,v1,False
extrapolation,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n = 100
X = np.random.uniform(0, 10, size=(n, 1))
y = 2 * X + np.random.normal(0, 1, size=n)

# Create a linear regression model
model = LinearRegression()
model.fit(X, y)

# Predict patient outcomes for patients with ages outside the observed range
new_patients = np.array([[0, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100])
new_patients_y = model.predict(new_patients)

print(""Predicted patient outcomes:"", new_patients_y)

# Conclusion: Based on the linear model, we can confidently predict patient outcomes for patients with ages up to 100.","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in healthcare context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
n = 100
X = np.random.uniform(0, 10, size=(n, 1))
y = 2 * X + np.random.normal(0, 1, size=n)

# Create a linear regression model
model = LinearRegression()
model.fit(X, y)

# Predict patient outcomes for patients with ages within the observed range
new_patients = np.array([[0, 5, 10]])
new_patients_y = model.predict(new_patients)

print(""Predicted patient outcomes within observed range:"", new_patients_y)

# Conclusion: Based on the linear model, we can make predictions within the observed data range (0-10).","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:17:19.402641,llama-3-8b-instruct,v1,False
extrapolation,warning,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
region_data = pd.DataFrame({'Region': ['North', 'North', 'South', 'South', 'East', 'East', 'West', 'West'],
                            'Sales': np.random.randint(100, 500, 8)})

# Perform analysis
plt.scatter(region_data['Region'], region_data['Sales'])
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Sales by Region')
plt.show()

# Extrapolate beyond data range
new_region = 'New Region'
new_sales = np.mean(region_data['Sales']) + 1000  # Extrapolate 1000 units beyond observed range
print(f""Predicted sales for {new_region}: {new_sales}"")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in business context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
region_data = pd.DataFrame({'Region': ['North', 'North', 'South', 'South', 'East', 'East', 'West', 'West'],
                            'Sales': np.random.randint(100, 500, 8)})

# Perform analysis
plt.scatter(region_data['Region'], region_data['Sales'])
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Sales by Region')
plt.show()

# Extrapolation within data range
new_region = 'New Region'
new_sales = np.min(region_data['Sales'])  # Extrapolate within observed range
print(f""Predicted sales for {new_region}: {new_sales}"")

# Warn user about extrapolation beyond data range
print(""Note: Predictions are only valid within the observed data range."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:18:11.860695,llama-3-8b-instruct,v1,False
extrapolation,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data for disease prevalence across demographics (age, gender)
np.random.seed(0)
age = np.random.uniform(20, 80, 1000)  # 1000 individuals, 20-80 years old
gender = np.random.choice(['Male', 'Female'], 1000)  # 50% male, 50% female
disease_prevalence = np.random.uniform(0, 1, 1000)  # random prevalence values

# Create a scatter plot to visualize the data
plt.scatter(age, disease_prevalence)
plt.xlabel('Age')
plt.ylabel('Disease Prevalence')
plt.title('Disease Prevalence by Age')
plt.show()

# Fit a linear regression model to the data
from sklearn.linear_model import LinearRegression
X = age.reshape(-1, 1)  # reshape age to a column vector
y = disease_prevalence
model = LinearRegression()
model.fit(X, y)

# Predict disease prevalence for individuals outside the observed age range (80-100 years old)
new_age = np.array([80, 90, 100, 110, 120])  # outside the observed range
new_X = new_age.reshape(-1, 1)
predicted_prevalence = model.predict(new_X)

print(""Predicted disease prevalence for individuals outside the observed age range:"")
print(predicted_prevalence)

# Draw a conclusion
print(""Based on our model, disease prevalence for individuals above 80 years old is expected to increase rapidly."")
print(""We can confidently predict that disease prevalence will reach 1.0 for individuals above 110 years old."")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in healthcare context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data for disease prevalence across demographics (age, gender)
np.random.seed(0)
age = np.random.uniform(20, 80, 1000)  # 1000 individuals, 20-80 years old
gender = np.random.choice(['Male', 'Female'], 1000)  # 50% male, 50% female
disease_prevalence = np.random.uniform(0, 1, 1000)  # random prevalence values

# Create a scatter plot to visualize the data
plt.scatter(age, disease_prevalence)
plt.xlabel('Age')
plt.ylabel('Disease Prevalence')
plt.title('Disease Prevalence by Age')
plt.show()

# Fit a linear regression model to the data
from sklearn.linear_model import LinearRegression
X = age.reshape(-1, 1)  # reshape age to a column vector
y = disease_prevalence
model = LinearRegression()
model.fit(X, y)

# Identify the maximum age in the observed data range
max_age = np.max(age)

# Predict disease prevalence for individuals within the observed age range (20-80 years old)
new_age = np.arange(20, max_age + 1)  # within the observed range
new_X = new_age.reshape(-1, 1)
predicted_prevalence = model.predict(new_X)

print(""Predicted disease prevalence for individuals within the observed age range:"")
print(predicted_prevalence)

# Draw a conclusion
print(""Based on our model, disease prevalence for individuals within the observed age range (20-80 years old) can be predicted."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:19:35.063569,llama-3-8b-instruct,v1,False
extrapolation,warning,social_science,voting behavior analysis,python,intermediate,"# Generate synthetic data
import numpy as np
import pandas as pd

np.random.seed(0)
n_samples = 100
X = np.random.rand(n_samples, 1)
y = np.sin(X) + np.random.randn(n_samples, 1) * 0.1

# Create a Pandas DataFrame
df = pd.DataFrame(np.hstack((X, y)), columns=['feature', 'target'])

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['feature']], df['target'])

# Make predictions outside the observed data range
new_samples = np.linspace(0, 1, 10)  # Predict for new values outside the training range
new_X = np.array([[x] for x in new_samples]).reshape(-1, 1)
new_y_pred = model.predict(new_X)

# Flawed conclusion
print(""The predicted voting behavior for feature values above 1 is highly likely to be:"")
print(new_y_pred)","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in social_science context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","# Generate synthetic data
import numpy as np
import pandas as pd

np.random.seed(0)
n_samples = 100
X = np.random.rand(n_samples, 1)
y = np.sin(X) + np.random.randn(n_samples, 1) * 0.1

# Create a Pandas DataFrame
df = pd.DataFrame(np.hstack((X, y)), columns=['feature', 'target'])

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['feature']], df['target'])

# Make predictions within the observed data range
new_X = np.array([[x] for x in df['feature'].values]).reshape(-1, 1)
new_y_pred = model.predict(new_X)

# Corrected conclusion
print(""The predicted voting behavior for feature values within the observed range is:"")
print(new_y_pred)","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:20:29.285559,llama-3-8b-instruct,v1,False
extrapolation,warning,social_science,voting behavior analysis,python,intermediate,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
income_data = np.random.normal(loc=50000, scale=10000, size=100)
years = np.arange(2010, 2025)

# Create a DataFrame
df = pd.DataFrame({'Year': years, 'Income': income_data})

# Fit a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['Year']], df['Income'])

# Make predictions beyond observed data range
future_years = np.arange(2025, 2050)
future_income = model.predict(future_years[:, np.newaxis])

# Plot the results
plt.plot(df['Year'], df['Income'], 'o')
plt.plot(future_years, future_income)
plt.xlabel('Year')
plt.ylabel('Income')
plt.title('Income Trend Projection')
plt.show()

print(""Based on our model, we can confidently predict that income will continue to increase by 10% annually until 2050."")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in social_science context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
income_data = np.random.normal(loc=50000, scale=10000, size=100)
years = np.arange(2010, 2025)

# Create a DataFrame
df = pd.DataFrame({'Year': years, 'Income': income_data})

# Fit a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['Year']], df['Income'])

# Make predictions within observed data range
future_income = model.predict(df[['Year']])

# Plot the results
plt.plot(df['Year'], df['Income'], 'o')
plt.plot(df['Year'], future_income)
plt.xlabel('Year')
plt.ylabel('Income')
plt.title('Income Trend')
plt.show()

print(""Based on our model, we can confidently predict income trends within the observed data range."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:21:26.595866,llama-3-8b-instruct,v1,False
extrapolation,warning,healthcare,observational study of patient outcomes,python,intermediate,"import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
X = np.random.uniform(0, 100, 100)  # age in years
y = 2 * X + 10 + np.random.normal(0, 5, 100)  # treatment effectiveness

# Perform analysis that commits the error
model = LinearRegression()
model.fit(X.reshape(-1, 1), y)

# Predict treatment effectiveness for ages outside observed range
X_pred = np.array([150, 200, 250])  # ages outside observed range
y_pred = model.predict(X_pred.reshape(-1, 1))

print(""Predicted treatment effectiveness for ages 150, 200, and 250:"", y_pred)

# Flawed conclusion
print(""Based on our analysis, we can confidently say that treatment effectiveness increases linearly beyond age 100."")","{'summary': 'Code demonstrates Extrapolation Beyond Data Range in healthcare context', 'detailed_explanation': 'This code demonstrates Extrapolation Beyond Data Range. Model validity is uncertain beyond training data range In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Generate synthetic data
np.random.seed(0)
X = np.random.uniform(0, 100, 100)  # age in years
y = 2 * X + 10 + np.random.normal(0, 5, 100)  # treatment effectiveness

# Perform analysis within observed data range
model = LinearRegression()
model.fit(X.reshape(-1, 1), y)

# Visualize the data and model
plt.scatter(X, y)
plt.plot(X, model.predict(X.reshape(-1, 1)), 'r')
plt.xlabel('Age (years)')
plt.ylabel('Treatment Effectiveness')
plt.title('Model within observed data range')
plt.show()

# Warn about extrapolation beyond data range
print(""Warning: predictions outside observed data range may be unreliable."")

# Do not attempt to make predictions outside the observed data range
print(""Do not use this model to make predictions for ages outside the range of 0-100 years."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Model validity is uncertain beyond training data range']",Model validity is uncertain beyond training data range,2025-12-14T22:22:22.020042,llama-3-8b-instruct,v1,False
assumption_violation,warning,education,teaching method effectiveness comparison,python,intermediate,"import numpy as np
import pandas as pd
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
n = 100
x1 = np.random.normal(0, 1, n)  # online learning hours
x2 = np.random.normal(0, 1, n)  # in-person learning hours
y = np.random.normal(0, 1, n)  # learning outcomes

# Create a dataframe
df = pd.DataFrame({'online_hours': x1, 'in_person_hours': x2, 'outcomes': y})

# Perform regression analysis assuming normality and independence
X = sm.add_constant(x1)
X = sm.add_constant(x2)
model = sm.OLS(y, X).fit()
print(model.summary())","{'summary': 'Code demonstrates Statistical Assumption Violation in education context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import numpy as np
import pandas as pd
import statsmodels.api as sm
from scipy.stats import shapiro
from scipy.stats import pearsonr

# Generate synthetic data
np.random.seed(0)
n = 100
x1 = np.random.normal(0, 1, n)  # online learning hours
x2 = np.random.normal(0, 1, n)  # in-person learning hours
y = np.random.normal(0, 1, n)  # learning outcomes

# Create a dataframe
df = pd.DataFrame({'online_hours': x1, 'in_person_hours': x2, 'outcomes': y})

# Check for normality and independence
print(shapiro(df['outcomes']))  # check normality
print(pearsonr(df['online_hours'], df['in_person_hours']))  # check independence

# Perform robust regression analysis assuming non-normality and dependence
X = sm.add_constant(x1)
X = sm.add_constant(x2)
model = sm.RLM(y, X, M=sm.robust.norms.tukeybisquare()).fit()
print(model.summary())","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:23:15.834515,llama-3-8b-instruct,v1,False
assumption_violation,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 100
x1 = np.random.normal(0, 1, n)  # normally distributed data
x2 = np.random.normal(0, 1, n)  # normally distributed data
y1 = np.random.randint(0, 2, n)  # binary response (0/1)
y2 = np.random.randint(0, 2, n)  # binary response (0/1)

# Create DataFrame
df = pd.DataFrame({'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2})

# Perform analysis
t_stat, p_val = ttest_ind(df['x1'], df['x2'], equal_var=False)
print(f""t-statistic: {t_stat}, p-value: {p_val}"")

# Incorrect reasoning: Assuming normality and independence
print(""Based on the t-test result, we can conclude that there is a statistically significant difference between the means of x1 and x2."")
print(""This means that the disease prevalence is significantly different between the two demographics."")","{'summary': 'Code demonstrates Statistical Assumption Violation in healthcare context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","import pandas as pd
import numpy as np
from scipy.stats import wilcoxon_rank_sum

# Generate synthetic data
np.random.seed(0)
n = 100
x1 = np.random.normal(0, 1, n)  # normally distributed data
x2 = np.random.normal(0, 1, n)  # normally distributed data
y1 = np.random.randint(0, 2, n)  # binary response (0/1)
y2 = np.random.randint(0, 2, n)  # binary response (0/1)

# Create DataFrame
df = pd.DataFrame({'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2})

# Perform analysis (corrected)
wilcoxon_stat, wilcoxon_p = wilcoxon_rank_sum(df['y1'], df['y2'])
print(f""Wilcoxon statistic: {wilcoxon_stat}, p-value: {wilcoxon_p}"")

# Correct reasoning: Using a non-parametric test
print(""Based on the Wilcoxon rank-sum test result, we can conclude that there is a statistically significant difference between the distributions of y1 and y2."")
print(""This means that the disease prevalence is significantly different between the two demographics, but we must consider the non-normality and non-independence of the data."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:24:25.272739,llama-3-8b-instruct,v1,False
assumption_violation,warning,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
data = {
    'Region': ['North', 'North', 'North', 'North', 'South', 'South', 'South', 'South'],
    'Sales': [100, 120, 90, 110, 80, 130, 100, 120]
}
df = pd.DataFrame(data)

# Perform t-test assuming normality and independence
t_stat, p_val = ttest_ind(df.loc[df['Region'] == 'North']['Sales'], df.loc[df['Region'] == 'South']['Sales'])
print(f't-statistic: {t_stat}, p-value: {p_val}')

# Flawed conclusion
print(""Based on the t-test, we can conclude that there is a statistically significant difference in sales performance between the North and South regions."")","{'summary': 'Code demonstrates Statistical Assumption Violation in business context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import pandas as pd
import numpy as np
from scipy.stats import wilcoxon

# Generate synthetic data
np.random.seed(0)
data = {
    'Region': ['North', 'North', 'North', 'North', 'South', 'South', 'South', 'South'],
    'Sales': [100, 120, 90, 110, 80, 130, 100, 120]
}
df = pd.DataFrame(data)

# Perform Wilcoxon rank-sum test, which is non-parametric and does not assume normality or independence
stat, p_val = wilcoxon(df.loc[df['Region'] == 'North']['Sales'], df.loc[df['Region'] == 'South']['Sales'])
print(f'Wilcoxon statistic: {stat}, p-value: {p_val}')

# Corrected conclusion
print(""Based on the Wilcoxon rank-sum test, we can conclude that there is a statistically significant difference in sales performance between the North and South regions, but we must be cautious about the interpretation due to the non-parametric nature of the test."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:25:18.777137,llama-3-8b-instruct,v1,False
assumption_violation,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 20
treatment_group = np.random.normal(10, 2, n)
control_group = np.random.normal(8, 2, n)

# Create a DataFrame
df = pd.DataFrame({'Treatment': treatment_group, 'Control': control_group})

# Perform t-test assuming normality and independence
t_stat, p_val = ttest_ind(df['Treatment'], df['Control'])
print(f""t-statistic: {t_stat}, p-value: {p_val}"")

# Flawed conclusion
print(""The treatment is significantly effective, p-value is less than 0.05!"")","{'summary': 'Code demonstrates Statistical Assumption Violation in healthcare context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from scipy.stats import shapiro
from scipy.stats import normaltest

# Generate synthetic data
np.random.seed(0)
n = 20
treatment_group = np.random.normal(10, 2, n)
control_group = np.random.normal(8, 2, n)

# Create a DataFrame
df = pd.DataFrame({'Treatment': treatment_group, 'Control': control_group})

# Check normality assumption
treatment_shapiro = shapiro(df['Treatment'])
control_shapiro = shapiro(df['Control'])
print(f""Treatment normality test: {treatment_shapiro}"")
print(f""Control normality test: {control_shapiro}"")

# Check independence assumption
# (Since the data is generated from the same distribution, independence is not an issue)

# Perform non-parametric test that does not assume normality
wilcoxon_stat, wilcoxon_p = wilcoxon_test(df['Treatment'], df['Control'])
print(f""Wilcoxon statistic: {wilcoxon_stat}, p-value: {wilcoxon_p}"")

# Corrected conclusion
print(""The treatment's effect may be significant, but further analysis is needed to confirm."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:26:13.098641,llama-3-8b-instruct,v1,False
assumption_violation,warning,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(42)
region_data = pd.DataFrame({'Region': ['North', 'North', 'North', 'South', 'South', 'South'],
                            'Sales': np.random.normal(100, 20, 6)})

# Perform analysis
t_stat, p_val = ttest_ind(region_data['Sales'][region_data['Region'] == 'North'],
                           region_data['Sales'][region_data['Region'] == 'South'])

print(f""t-statistic: {t_stat}, p-value: {p_val}"")
print(""Conclusion: The sales data from North and South regions are significantly different."")

# Incorrect reasoning: Assuming normality of data and independence of samples
# (in reality, the data is likely not normally distributed and the samples are not independent)","{'summary': 'Code demonstrates Statistical Assumption Violation in business context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of customer churn prediction analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in customer churn prediction analysis'}","import pandas as pd
import numpy as np
from scipy.stats import mannwhitneyu

# Generate synthetic data
np.random.seed(42)
region_data = pd.DataFrame({'Region': ['North', 'North', 'North', 'South', 'South', 'South'],
                            'Sales': np.random.normal(100, 20, 6)})

# Check for normality and independence
from scipy.stats import shapiro
from statsmodels.graphics.mosaicplot import mosaic

# Normality check
shapiro_result = shapiro(region_data['Sales'])
print(f""Shapiro-Wilk test statistic: {shapiro_result.statistic}, p-value: {shapiro_result.pvalue}"")

# Independence check
mosaic(region_data, 'Region', 'Sales')

# Perform analysis
u_stat, p_val = mannwhitneyu(region_data['Sales'][region_data['Region'] == 'North'],
                              region_data['Sales'][region_data['Region'] == 'South'])

print(f""Mann-Whitney U-statistic: {u_stat}, p-value: {p_val}"")
print(""Conclusion: The sales data from North and South regions are significantly different."")

# Correct reasoning: Using non-parametric test that is robust to non-normality and non-independence","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:27:11.758378,llama-3-8b-instruct,v1,False
assumption_violation,warning,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import numpy as np
import scipy.stats as stats

# Generate synthetic data for marketing campaign effectiveness study
np.random.seed(0)
data = pd.DataFrame({'Conversion_Rate': np.random.normal(0.05, 0.01, 1000),  # normally distributed
                    'Age': np.random.randint(18, 65, 1000),  # categorical variable
                    'Gender': np.random.choice(['Male', 'Female'], 1000, p=[0.5, 0.5]),  # categorical variable
                    'Ad_Spend': np.random.normal(100, 10, 1000)}  # normally distributed

# Perform analysis without checking for normality
t_stat, p_value = stats.ttest_ind(data['Conversion_Rate'], data.groupby('Age')['Conversion_Rate'].mean())
print(f""T-statistic: {t_stat}, p-value: {p_value}"")
print(""The conversion rate is significantly different between age groups."")

# Conclusion
print(""Based on the results, we can confidently conclude that the conversion rate is significantly different between age groups."")","{'summary': 'Code demonstrates Statistical Assumption Violation in business context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import pandas as pd
import numpy as np
import scipy.stats as stats
from scipy import rankdata

# Generate synthetic data for marketing campaign effectiveness study
np.random.seed(0)
data = pd.DataFrame({'Conversion_Rate': np.random.uniform(0.01, 0.1, 1000),  # non-normal distribution
                    'Age': np.random.randint(18, 65, 1000),  # categorical variable
                    'Gender': np.random.choice(['Male', 'Female'], 1000, p=[0.5, 0.5]),  # categorical variable
                    'Ad_Spend': np.random.uniform(50, 150, 1000)}  # non-normal distribution

# Check for normality
from scipy.stats import shapiro
for col in data.columns:
    if col!= 'Age' and col!= 'Gender':
        stat, p_value = shapiro(data[col])
        print(f""Normality test for {col}: stat={stat}, p-value={p_value}"")

# Perform non-parametric test (Wilcoxon rank-sum test) since data is not normally distributed
from scipy.stats import ranksums
ranksum_stat, p_value = ranksums(data['Conversion_Rate'], data.groupby('Age')['Conversion_Rate'].rank(method='average'))
print(f""Wilcoxon rank-sum test: stat={ranksum_stat}, p-value={p_value}"")

# Conclusion
if p_value < 0.05:
    print(""The conversion rate is significantly different between age groups."")
else:
    print(""The conversion rate is not significantly different between age groups."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:28:19.192975,llama-3-8b-instruct,v1,False
assumption_violation,warning,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np
import scipy.stats as stats

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'price': np.random.normal(100, 20, 1000), 'discount': np.random.normal(5, 2, 1000),'sales': np.random.poisson(50, 1000)})

# Perform analysis
from scipy.stats import pearsonr
corr, _ = pearsonr(data['price'], data['discount'])
print(f""Correlation between price and discount: {corr:.2f}"")

# Incorrect reasoning: assuming normality
from scipy.stats import shapiro
normality_test = shapiro(data['sales'])
print(f""Normality test p-value: {normality_test[1]:.4f}"")

# Incorrect reasoning: assuming independence
from scipy.stats import chi2_contingency
contingency_table = pd.crosstab(data['price'], data['discount'])
chi2, p_value, dof, _ = chi2_contingency(contingency_table)
print(f""Independence test p-value: {p_value:.4f}"")

# Flawed conclusion
print(""Based on the analysis, we can conclude that there is a strong positive correlation between price and discount, and that the sales data follows a normal distribution. Additionally, the independence test suggests that there is no association between price and discount."")","{'summary': 'Code demonstrates Statistical Assumption Violation in business context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import pandas as pd
import numpy as np
import scipy.stats as stats

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'price': np.random.normal(100, 20, 1000), 'discount': np.random.normal(5, 2, 1000),'sales': np.random.poisson(50, 1000)})

# Perform analysis
from scipy.stats import pearsonr
corr, _ = pearsonr(data['price'], data['discount'])
print(f""Correlation between price and discount: {corr:.2f}"")

# Corrected approach: non-parametric test for correlation
from scipy.stats import kendalltau
corr, p_value = kendalltau(data['price'], data['discount'])
print(f""Non-parametric correlation p-value: {p_value:.4f}"")

# Corrected approach: non-parametric test for independence
from scipy.stats import chisquare
contingency_table = pd.crosstab(data['price'], data['discount'])
chi2, p_value = chisquare(contingency_table)
print(f""Non-parametric independence test p-value: {p_value:.4f}"")

# Corrected conclusion
print(""Based on the analysis, we can conclude that there is a strong positive correlation between price and discount, but we should be cautious when interpreting the results due to the potential violation of normality and independence assumptions. Future analysis should consider alternative methods that are robust to these violations."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:29:29.399864,llama-3-8b-instruct,v1,False
assumption_violation,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 20
x1 = np.random.normal(0, 1, n)  # Control group
x2 = np.random.normal(1, 1, n)  # Treatment group

# Create a DataFrame
df = pd.DataFrame({'Control': x1, 'Treatment': x2})

# Perform t-test assuming normality and independence
t_stat, p_val = ttest_ind(x1, x2)
print(f'T-statistic: {t_stat}, p-value: {p_val}')

# Visualize data
plt.hist(x1, alpha=0.5, label='Control')
plt.hist(x2, alpha=0.5, label='Treatment')
plt.legend()
plt.show()

# Conclusion
print(""The treatment group shows a significant difference (p-value < 0.05) and has a higher mean value. Therefore, we can conclude that the treatment is effective."")","{'summary': 'Code demonstrates Statistical Assumption Violation in healthcare context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import shapiro, normaltest, fligner

# Generate synthetic data
np.random.seed(0)
n = 20
x1 = np.random.normal(0, 1, n)  # Control group
x2 = np.random.normal(1, 1, n)  # Treatment group

# Create a DataFrame
df = pd.DataFrame({'Control': x1, 'Treatment': x2})

# Check normality
shapiro_stat, shapiro_p = shapiro(x1)
normal_stat, normal_p = normaltest(x1)
print(f'Shapiro-Wilk normality test: stat={shapiro_stat}, p-value={shapiro_p}')
print(f'Jarque-Bera normality test: stat={normal_stat}, p-value={normal_p}')

# Check independence
fligner_stat, fligner_p = fligner(x1, x2)
print(f'Fligner's test for independence: stat={fligner_stat}, p-value={fligner_p}')

# Perform non-parametric test (e.g., Wilcoxon rank-sum test)
from scipy.stats import ranksums
ranksum_stat, ranksum_p = ranksums(x1, x2)
print(f'Wilcoxon rank-sum test: stat={ranksum_stat}, p-value={ranksum_p}')

# Visualize data
plt.hist(x1, alpha=0.5, label='Control')
plt.hist(x2, alpha=0.5, label='Treatment')
plt.legend()
plt.show()

# Conclusion
print(""The treatment group shows a significant difference (p-value < 0.05) and has a higher mean value. However, we should be cautious when interpreting the results due to potential violations of assumptions."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:30:40.084419,llama-3-8b-instruct,v1,False
assumption_violation,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 100
x1 = np.random.normal(0, 1, n)
x2 = np.random.normal(1, 1.5, n)
y = np.random.binomial(1, 0.5, n)

# Create a DataFrame
df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})

# Perform t-test assuming normality and independence
t_stat, p_val = ttest_ind(df['x1'], df['x2'], equal_var=True)

print(f""T-statistic: {t_stat}, p-value: {p_val}"")

# Conclusion
print(""Based on the p-value, we reject the null hypothesis that the means of x1 and x2 are equal."")","{'summary': 'Code demonstrates Statistical Assumption Violation in social_science context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np
from scipy.stats import mannwhitneyu

# Generate synthetic data
np.random.seed(0)
n = 100
x1 = np.random.normal(0, 1, n)
x2 = np.random.normal(1, 1.5, n)
y = np.random.binomial(1, 0.5, n)

# Create a DataFrame
df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})

# Perform Wilcoxon rank-sum test (non-parametric alternative to t-test)
stat, p_val = mannwhitneyu(df['x1'], df['x2'])

print(f""Mann-Whitney U-statistic: {stat}, p-value: {p_val}"")

# Conclusion
print(""Based on the p-value, we cannot reject the null hypothesis that the distributions of x1 and x2 are equal."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:31:31.620466,llama-3-8b-instruct,v1,False
assumption_violation,warning,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
online_learning_outcomes = np.random.normal(50, 10, 100)  # assume normal distribution
in_person_learning_outcomes = np.random.normal(60, 15, 100)  # assume normal distribution
data = pd.DataFrame({'Online Learning Outcomes': online_learning_outcomes, 'In-Person Learning Outcomes': in_person_learning_outcomes})

# Perform analysis
t_stat, p_val = stats.ttest_ind(online_learning_outcomes, in_person_learning_outcomes)
print(f't-statistic: {t_stat}, p-value: {p_val}')

# Comment: We're assuming normality of the data and independence between groups
# This is okay, right? The data looks normal-ish and we're not concerned about correlated errors
plt.hist(online_learning_outcomes, alpha=0.5, label='Online Learning Outcomes')
plt.hist(in_person_learning_outcomes, alpha=0.5, label='In-Person Learning Outcomes')
plt.legend()
plt.show()

# Flawed conclusion
print(""There is a statistically significant difference in online and in-person learning outcomes (p-value:"", p_val, ""). Online learning outcomes are significantly lower than in-person learning outcomes."")","{'summary': 'Code demonstrates Statistical Assumption Violation in education context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
from scipy.stats import shapiro

# Generate synthetic data
np.random.seed(0)
online_learning_outcomes = np.random.normal(50, 10, 100)  # assume normal distribution
in_person_learning_outcomes = np.random.normal(60, 15, 100)  # assume normal distribution
data = pd.DataFrame({'Online Learning Outcomes': online_learning_outcomes, 'In-Person Learning Outcomes': in_person_learning_outcomes})

# Check for normality
print(""Normality tests:"")
print(""Online Learning Outcomes:"", shapiro(online_learning_outcomes))
print(""In-Person Learning Outcomes:"", shapiro(in_person_learning_outcomes))

# Check for independence
print(""Independence test:"")
from statsmodels.stats.outliers_influence import variance_inflation_factor
vif = variance_inflation_factor(data.values, 0)
print(""VIF:"", vif)

# Perform analysis using non-parametric test (Wilcoxon rank-sum test)
from scipy.stats import wilcoxon_rank_sum_test
wilcoxon_stat, wilcoxon_p_val = wilcoxon_rank_sum_test(online_learning_outcomes, in_person_learning_outcomes)
print(""Wilcoxon rank-sum test statistic:"", wilcoxon_stat, "", p-value:"", wilcoxon_p_val)

# Corrected conclusion
print(""There is a statistically significant difference in online and in-person learning outcomes (p-value:"", wilcoxon_p_val, ""). Online learning outcomes are significantly lower than in-person learning outcomes."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:32:37.360590,llama-3-8b-instruct,v1,False
assumption_violation,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n_samples = 100
n_groups = 2
x1 = np.random.normal(0, 1, n_samples)  # Group 1
x2 = np.random.normal(1, 2, n_samples)  # Group 2
df = pd.DataFrame({'Group': ['A' if i < n_samples//2 else 'B' for i in range(n_samples)], 
                   'Value': np.concatenate([x1, x2])})

# Perform analysis
t_stat, p_val = ttest_ind(x1, x2)
print(f""t-statistic: {t_stat}, p-value: {p_val}"")

# Incorrect reasoning: assuming normality of data
print(""Since the data appears to be roughly normally distributed, we can assume normality."")

# Flawed conclusion
print(""Based on the p-value, we can reject the null hypothesis that the means are equal."")
print(""Therefore, we conclude that there is a significant difference in the means between the two groups."")","{'summary': 'Code demonstrates Statistical Assumption Violation in social_science context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind_from_stats

# Generate synthetic data
np.random.seed(0)
n_samples = 100
n_groups = 2
x1 = np.random.normal(0, 1, n_samples)  # Group 1
x2 = np.random.normal(1, 2, n_samples)  # Group 2
df = pd.DataFrame({'Group': ['A' if i < n_samples//2 else 'B' for i in range(n_samples)], 
                   'Value': np.concatenate([x1, x2])})

# Perform analysis
mean1, std1 = np.mean(x1), np.std(x1)
mean2, std2 = np.mean(x2), np.std(x2)
t_stat, p_val = ttest_ind_from_stats(mean1, std1, len(x1), mean2, std2, len(x2), equal_var=True)
print(f""t-statistic: {t_stat}, p-value: {p_val}"")

# Corrected reasoning: acknowledging the non-normality of data
print(""Since the data is not normally distributed, we cannot assume normality."")

# Corrected conclusion
print(""Based on the p-value, we cannot make any conclusions about the difference in means between the two groups."")
print(""To draw meaningful conclusions, we would need to transform the data or use a non-parametric test."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:33:40.218924,llama-3-8b-instruct,v1,False
assumption_violation,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"# Import necessary libraries
import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)  # outcome variable
y = np.random.binomial(1, 0.5, n)  # binary predictor variable
df = pd.DataFrame({'outcome': x, 'predictor': y})

# Perform analysis
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(x, y)
print(""MSE:"", mse)

# Perform t-test
t_stat, p_val = ttest_ind(x, y)
print(""t-statistic:"", t_stat)
print(""p-value:"", p_val)
print(""Conclusion: Since the p-value is less than 0.05, we reject the null hypothesis that the predictor has no effect on the outcome."")

# Incorrect reasoning: Assuming normality of the outcome variable
# when it's actually binary.","{'summary': 'Code demonstrates Statistical Assumption Violation in healthcare context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","# Import necessary libraries
import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.binomial(1, 0.5, n)  # binary outcome variable
y = np.random.binomial(1, 0.5, n)  # binary predictor variable
df = pd.DataFrame({'outcome': x, 'predictor': y})

# Perform analysis
from scipy.stats import chi2_contingency
stat, p_val, dof, expected = chi2_contingency(pd.crosstab(df['predictor'], df['outcome']))
print(""Chi-squared statistic:"", stat)
print(""p-value:"", p_val)
print(""Conclusion: Since the p-value is less than 0.05, we reject the null hypothesis that the predictor has no association with the outcome."")

# Correct approach: Using the chi-squared test for independence
# between the binary variables.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:34:37.620549,llama-3-8b-instruct,v1,False
assumption_violation,warning,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import numpy as np
import scipy.stats as stats

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_teachers = 10
data = pd.DataFrame(np.random.normal(50, 10, size=(n_students, n_teachers)), columns=[f'Teacher {i}' for i in range(n_teachers)])
data['Score'] = np.random.normal(50, 5, size=n_students)

# Perform analysis (assuming normality and independence)
from scipy.stats import ttest_ind

# Compare the means of two teachers
teacher1_mean = data.iloc[:, 0].mean()
teacher2_mean = data.iloc[:, 1].mean()
t_stat, p_val = ttest_ind(data.iloc[:, 0], data.iloc[:, 1])

print(f'T-test Statistic: {t_stat:.2f}')
print(f'p-value: {p_val:.4f}')
print(f'Conclusion: Teacher 1\'s method is significantly better than Teacher 2\'s method with p-value {p_val:.4f}')","{'summary': 'Code demonstrates Statistical Assumption Violation in education context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import pandas as pd
import numpy as np
import scipy.stats as stats
import seaborn as sns
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n_students = 100
n_teachers = 10
data = pd.DataFrame(np.random.normal(50, 10, size=(n_students, n_teachers)), columns=[f'Teacher {i}' for i in range(n_teachers)])
data['Score'] = np.random.normal(50, 5, size=n_students)

# Visualize data to check for normality and independence
sns.set()
sns.boxplot(x='Teacher', y='Score', data=data)
plt.title('Boxplot of Scores by Teacher')
plt.show()

# Perform analysis (using non-parametric test that does not assume normality or independence)
from scipy.stats import wilcoxon_rank_sum_test

# Compare the means of two teachers
teacher1_mean = data.iloc[:, 0].mean()
teacher2_mean = data.iloc[:, 1].mean()
stat, p_val = wilcoxon_rank_sum_test(data.iloc[:, 0], data.iloc[:, 1])

print(f'Wilcoxon Rank-Sum Test Statistic: {stat:.2f}')
print(f'p-value: {p_val:.4f}')
print(f'Conclusion: Teacher 1\'s method is significantly better than Teacher 2\'s method with p-value {p_val:.4f}')

# Alternatively, consider using robust methods like robust regression or bootstrapping to account for non-normality and dependence","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:35:43.835099,llama-3-8b-instruct,v1,False
assumption_violation,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 30
treatment = np.random.binomial(1, 0.5, n)
control = np.random.binomial(1, 0.5, n)
outcome = np.random.normal(0, 1, n)

# Create a DataFrame
df = pd.DataFrame({'treatment': treatment, 'control': control, 'outcome': outcome})

# Perform t-test assuming normality and independence
t_stat, p_val = ttest_ind(df['outcome'][treatment == 1], df['outcome'][treatment == 0])

print(f'T-statistic: {t_stat}, p-value: {p_val}')
print(f'Treatment is effective: p-value < 0.05, so we reject the null hypothesis.')","{'summary': 'Code demonstrates Statistical Assumption Violation in healthcare context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","import pandas as pd
import numpy as np
from scipy.stats import wilcoxon_rank_sum_test

# Generate synthetic data
np.random.seed(0)
n = 30
treatment = np.random.binomial(1, 0.5, n)
control = np.random.binomial(1, 0.5, n)
outcome = np.random.normal(0, 1, n)

# Create a DataFrame
df = pd.DataFrame({'treatment': treatment, 'control': control, 'outcome': outcome})

# Perform Wilcoxon rank-sum test, which does not assume normality or independence
stat, p_val = wilcoxon_rank_sum_test(df['outcome'][treatment == 1], df['outcome'][treatment == 0])

print(f'Statistic: {stat}, p-value: {p_val}')
print(f'Treatment is effective: p-value < 0.05, so we reject the null hypothesis.')","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:36:38.409184,llama-3-8b-instruct,v1,False
assumption_violation,warning,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import numpy as np
import scipy.stats as stats

# Generate synthetic data
np.random.seed(0)
data = np.random.normal(0, 1, 100)  # Generate 100 random numbers from a normal distribution
df = pd.DataFrame(data, columns=['price'])

# Perform analysis
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['price']], df['price'])  # Assuming independence and normality

# Perform ANOVA to check for significance
F_stat, p_value = stats.f_oneway(*[df.groupby('price').mean()['price']])  # Assuming normality

print(""The p-value is:"", p_value)
if p_value < 0.05:
    print(""The pricing strategy has a statistically significant impact on sales."")
else:
    print(""The pricing strategy does not have a statistically significant impact on sales."")

# Flawed conclusion
print(""Based on our analysis, we can confidently conclude that the pricing strategy has a significant impact on sales."")","{'summary': 'Code demonstrates Statistical Assumption Violation in business context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import pandas as pd
import numpy as np
import scipy.stats as stats
from scipy.stats import shapiro
from scipy.stats import normaltest

# Generate synthetic data
np.random.seed(0)
data = np.random.normal(0, 1, 100)  # Generate 100 random numbers from a normal distribution
df = pd.DataFrame(data, columns=['price'])

# Perform exploratory data analysis
print(""Normality check:"", shapiro(df['price']))
print(""Independence check:"", df['price'].corr(df['price']))  # Check for autocorrelation

# Perform non-parametric test
from scipy.stats import ranksums
ranksums_stat, ranksums_pvalue = ranksums(df['price'], df['price'])  # Non-parametric test for paired data
print(""The p-value is:"", ranksums_pvalue)

# Perform corrected conclusion
if ranksums_pvalue < 0.05:
    print(""The pricing strategy has a statistically significant impact on sales."")
else:
    print(""The pricing strategy does not have a statistically significant impact on sales."")

# Corrected conclusion
print(""Based on our analysis, we can conclude that the pricing strategy has a significant impact on sales, but we must consider the limitations of our analysis."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:37:44.971158,llama-3-8b-instruct,v1,False
assumption_violation,warning,social_science,income inequality trends,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 100
crime_rate = np.random.normal(0.5, 0.1, n)
weather = np.random.normal(0.8, 0.2, n)
crime_rate_with_weather = np.random.normal(crime_rate + 0.1 * weather, 0.05, n)

# Create a Pandas DataFrame
df = pd.DataFrame({'Crime Rate': crime_rate, 'Weather': weather, 'Crime Rate with Weather': crime_rate_with_weather})

# Perform analysis
t_stat, p_val = ttest_ind(df['Crime Rate'], df['Crime Rate with Weather'])
print(f""T-statistic: {t_stat:.3f}, p-value: {p_val:.3f}"")

# Flawed conclusion
print(""There is a statistically significant relationship between crime rate and weather!"")","{'summary': 'Code demonstrates Statistical Assumption Violation in social_science context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of income inequality trends, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in income inequality trends'}","import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Generate synthetic data
np.random.seed(0)
n = 100
crime_rate = np.random.normal(0.5, 0.1, n)
weather = np.random.normal(0.8, 0.2, n)
crime_rate_with_weather = np.random.normal(crime_rate + 0.1 * weather, 0.05, n)

# Create a Pandas DataFrame
df = pd.DataFrame({'Crime Rate': crime_rate, 'Weather': weather, 'Crime Rate with Weather': crime_rate_with_weather})

# Perform analysis
r, p_val = pearsonr(df['Crime Rate'], df['Crime Rate with Weather'])
print(f""Pearson's r: {r:.3f}, p-value: {p_val:.3f}"")

# Corrected conclusion
print(""There is a statistically significant correlation between crime rate and weather!"")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:38:38.205045,llama-3-8b-instruct,v1,False
assumption_violation,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
n = 100
x1 = np.random.normal(0, 1, n)  # continuous variable
x2 = np.random.randint(0, 2, n)  # binary variable
y = 2 + 3*x1 + 1*x2 + np.random.normal(0, 1, n)  # outcome variable

# Load data into a Pandas DataFrame
df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})

# Perform linear regression with normality and independence assumptions
X = sm.add_constant(df[['x1', 'x2']])
model = sm.OLS(y, X).fit()
print(model.summary())

# Conclude that x1 and x2 have a significant effect on y
print(""The results suggest that x1 and x2 have a significant positive effect on y."")","{'summary': 'Code demonstrates Statistical Assumption Violation in social_science context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np
import statsmodels.api as sm
from scipy.stats import normaltest

# Generate synthetic data
np.random.seed(0)
n = 100
x1 = np.random.normal(0, 1, n)  # continuous variable
x2 = np.random.randint(0, 2, n)  # binary variable
y = 2 + 3*x1 + 1*x2 + np.random.normal(0, 1, n)  # outcome variable

# Load data into a Pandas DataFrame
df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})

# Check for normality of residuals
residuals = model.resid
normality_test_stat, p_value = normaltest(residuals)
print(f""Normality test statistic: {normality_test_stat}, p-value: {p_value}"")

# Perform robust regression to accommodate non-normality
from statsmodels.robust import HMRobustLS
model_robust = HMRobustLS(y, X).fit()

# Perform permutation test to accommodate non-independence
from statsmodels.stats import permutation_test
p_value_permutation = permutation_test(model_robust, alternative='two-sided')
print(f""Permutation test p-value: {p_value_permutation}"")

# Conclude that x1 and x2 have a significant effect on y, after accounting for assumption violations
print(""The results suggest that x1 and x2 have a significant positive effect on y, after accounting for non-normality and non-independence."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:39:49.869968,llama-3-8b-instruct,v1,False
assumption_violation,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import numpy as np
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
income_data = np.random.normal(loc=50000, scale=10000, size=1000)
education_data = np.random.randint(1, 6, size=1000)  # 1-5 years of education
df = pd.DataFrame({'Income': income_data, 'Education': education_data})

# Perform regression analysis
X = sm.add_constant(df['Education'])
model = sm.OLS(df['Income'], X).fit()
print(model.params)

# Conclusion
print(""There is a significant positive relationship between income and education. The more education, the higher the income."")","{'summary': 'Code demonstrates Statistical Assumption Violation in social_science context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import pandas as pd
import numpy as np
import statsmodels.api as sm
from scipy.stats import shapiro
from statsmodels.graphics.gofplots import ProbPlot

# Generate synthetic data
np.random.seed(0)
income_data = np.random.normal(loc=50000, scale=10000, size=1000)
education_data = np.random.randint(1, 6, size=1000)  # 1-5 years of education
df = pd.DataFrame({'Income': income_data, 'Education': education_data})

# Check for normality of residuals
shapiro_test = shapiro(df['Income'])
print(""Shapiro-Wilk test for normality:"", shapiro_test.pvalue)

# Check for independence of residuals
from statsmodels.graphics.regression import RegressionPlot
plot = RegressionPlot(model).plot()
print(""Residual plot:"", plot)

# Perform robust regression (e.g., Huber-White standard errors)
X = sm.add_constant(df['Education'])
model = sm.RLM(df['Income'], X).fit()
print(model.params)

# Conclusion
print(""After controlling for normality and independence, we found a significant positive relationship between income and education. The more education, the higher the income."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:40:43.657279,llama-3-8b-instruct,v1,False
assumption_violation,warning,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Generate synthetic data (n_students = 10, n_teachers = 2)
np.random.seed(42)
students = np.random.normal(50, 5, 10)  # 10 students with mean 50, std 5
teacher1 = np.random.normal(55, 5, 5)  # 5 students with mean 55, std 5
teacher2 = np.random.normal(45, 5, 5)  # 5 students with mean 45, std 5

# Create a dataframe
df = pd.DataFrame({'Student Scores': np.concatenate((students, teacher1, teacher2))})
df['Teacher'] = ['Student'] * 10 + ['Teacher 1'] * 5 + ['Teacher 2'] * 5

# Perform t-test to compare means of teacher 1 and teacher 2
t_stat, p_val = ttest_ind(teacher1, teacher2)
print(""T-statistic:"", t_stat)
print(""p-value:"", p_val)

# Visualize the data using a histogram
plt.hist(df['Student Scores'], bins=20)
plt.title(""Student Scores Distribution"")
plt.xlabel(""Score"")
plt.ylabel(""Frequency"")
plt.show()

# Conclude that teacher 1 is significantly better than teacher 2
print(""Based on the t-test result, we can conclude that Teacher 1 is significantly better than Teacher 2."")","{'summary': 'Code demonstrates Statistical Assumption Violation in education context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import wilcoxon

# Generate synthetic data (n_students = 10, n_teachers = 2)
np.random.seed(42)
students = np.random.normal(50, 5, 10)  # 10 students with mean 50, std 5
teacher1 = np.random.normal(55, 5, 5)  # 5 students with mean 55, std 5
teacher2 = np.random.normal(45, 5, 5)  # 5 students with mean 45, std 5

# Create a dataframe
df = pd.DataFrame({'Student Scores': np.concatenate((students, teacher1, teacher2))})
df['Teacher'] = ['Student'] * 10 + ['Teacher 1'] * 5 + ['Teacher 2'] * 5

# Perform Wilcoxon rank-sum test to compare medians of teacher 1 and teacher 2
stat, p_val = wilcoxon(teacher1, teacher2)
print(""Wilcoxon statistic:"", stat)
print(""p-value:"", p_val)

# Visualize the data using a boxplot
plt.boxplot([teacher1, teacher2])
plt.title(""Student Scores Distribution"")
plt.xlabel(""Teacher"")
plt.ylabel(""Score"")
plt.show()

# Conclude that teacher 1 is significantly different from teacher 2
print(""Based on the Wilcoxon rank-sum test result, we can conclude that Teacher 1 and Teacher 2 have significantly different median scores."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:41:56.638328,llama-3-8b-instruct,v1,False
assumption_violation,warning,business,customer churn prediction analysis,python,intermediate,"import numpy as np
import pandas as pd
import statsmodels.api as sm

# Generate synthetic data
np.random.seed(0)
data = np.random.normal(0, 1, 100)  # 100 samples, mean 0, std 1
df = pd.DataFrame(data, columns=['Price'])

# Add a feature (feature engineering)
df['Discount'] = np.random.binomial(1, 0.5, 100)  # binary feature (0 or 1)

# Perform analysis
X = sm.add_constant(df['Discount'])  # add intercept
model = sm.OLS(df['Price'], X).fit()
print(model.params)

# Flawed conclusion
print(""The discount has a significant impact on the price. We can conclude that a 1% increase in the discount results in a 0.5 decrease in the price."")","{'summary': 'Code demonstrates Statistical Assumption Violation in business context', 'detailed_explanation': 'This code demonstrates Statistical Assumption Violation. Violations of assumptions can invalidate statistical inference In the context of customer churn prediction analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in customer churn prediction analysis'}","import numpy as np
import pandas as pd
import statsmodels.api as sm
from scipy.stats import norm

# Generate synthetic data
np.random.seed(0)
data = np.random.normal(0, 1, 100)  # 100 samples, mean 0, std 1
df = pd.DataFrame(data, columns=['Price'])

# Add a feature (feature engineering)
df['Discount'] = np.random.binomial(1, 0.5, 100)  # binary feature (0 or 1)

# Check for normality
print(norm.sf(df['Price']))  # check for normality

# Perform analysis using non-parametric methods (e.g., Wilcoxon rank-sum test)
from scipy.stats import wilcoxon_rank_sum_test
stat, p_val = wilcoxon_rank_sum_test(df['Price'], df['Discount'])
print(f""P-value: {p_val}"")

# Corrected conclusion
print(""The discount has a statistically significant impact on the price. However, we cannot conclude the exact nature of the relationship without further exploration."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Violations of assumptions can invalidate statistical inference']",Violations of assumptions can invalidate statistical inference,2025-12-14T22:42:51.875853,llama-3-8b-instruct,v1,False
truncated_axis,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
ages = np.random.randint(18, 100, 1000)
diseases = np.random.randint(0, 2, 1000)
prevalence = np.random.uniform(0.01, 0.1, 1000)

# Perform analysis that commits the error
fig, ax = plt.subplots()
ax.scatter(ages, prevalence)
ax.set_ylim([0.05, 0.15])  # Start y-axis at 0.05 to exaggerate differences
ax.set_xlabel('Age')
ax.set_ylabel('Disease Prevalence')
ax.set_title('Disease Prevalence by Age')
plt.show()

# Incorrect reasoning: ""By starting the y-axis at 0.05, we're highlighting the more extreme values in the data.""
# Flawed conclusion: ""The results clearly show that disease prevalence increases with age, especially among those above 60.""","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in healthcare context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
ages = np.random.randint(18, 100, 1000)
diseases = np.random.randint(0, 2, 1000)
prevalence = np.random.uniform(0.01, 0.1, 1000)

# Perform analysis with correct approach
fig, ax = plt.subplots()
ax.scatter(ages, prevalence)
ax.set_ylim([0, 0.15])  # Set y-axis limits to the actual range of the data
ax.set_xlabel('Age')
ax.set_ylabel('Disease Prevalence')
ax.set_title('Disease Prevalence by Age')
plt.show()

# Correct reasoning: ""By setting the y-axis limits to the actual range of the data, we're allowing the viewer to see the true distribution of the data.""
# Corrected conclusion: ""The results show that disease prevalence is relatively constant across age groups, with some variation.""","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:43:47.769879,llama-3-8b-instruct,v1,False
truncated_axis,critical,business,customer churn prediction analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y = np.random.normal(0, 1, 100)

# Perform analysis that commits the error
plt.scatter(x, y)
plt.axhline(y=0.5, color='r', linestyle='--')  # Starting y-axis at non-zero to exaggerate differences
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Marketing Campaign Effectiveness Study')
plt.show()

# Incorrect reasoning
# Starting the y-axis at 0.5 will help us visualize the differences between the two groups more clearly.

# Flawed conclusion
print(""The marketing campaign was highly effective, with a significant difference between the two groups."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in business context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of customer churn prediction analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in customer churn prediction analysis'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y = np.random.normal(0, 1, 100)

# Perform analysis with correct approach
plt.scatter(x, y)
plt.axhline(y=0, color='k', linestyle='--')  # Start y-axis at the origin
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Marketing Campaign Effectiveness Study')
plt.show()

# Correct reasoning
# Starting the y-axis at the origin (0) ensures that the data is accurately represented and avoids misleading visual distortions.

# Corrected conclusion
print(""The marketing campaign showed some variation in effectiveness between the two groups, but the differences are not as pronounced as they may appear."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:44:34.292294,llama-3-8b-instruct,v1,False
truncated_axis,critical,education,student performance prediction,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data for teaching method effectiveness comparison
np.random.seed(0)
method1_scores = np.random.normal(50, 10, 100)
method2_scores = np.random.normal(55, 10, 100)

# Perform analysis that commits the error: Starting y-axis at non-zero to exaggerate differences
fig, ax = plt.subplots()
ax.scatter(method1_scores, np.zeros(len(method1_scores)), label='Method 1')
ax.scatter(method2_scores, np.zeros(len(method2_scores)) + 5, label='Method 2')
ax.set_ylim(0, 60)  # Starting y-axis at 0 to exaggerate differences
ax.set_title('Teaching Method Effectiveness Comparison')
ax.set_xlabel('Scores')
ax.set_ylabel('Method')
ax.legend()
plt.show()

# Flawed conclusion
print(""Method 2 is significantly better than Method 1, as the y-axis starts at 0 to emphasize the difference."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in education context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of student performance prediction, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in student performance prediction'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data for teaching method effectiveness comparison
np.random.seed(0)
method1_scores = np.random.normal(50, 10, 100)
method2_scores = np.random.normal(55, 10, 100)

# Perform analysis with correct approach: Starting y-axis at the minimum value
fig, ax = plt.subplots()
ax.scatter(method1_scores, np.zeros(len(method1_scores)), label='Method 1')
ax.scatter(method2_scores, np.zeros(len(method2_scores)) + 5, label='Method 2')
ax.set_ylim(min(np.min(method1_scores), np.min(method2_scores)), max(np.max(method1_scores), np.max(method2_scores)))  # Start y-axis at the minimum value
ax.set_title('Teaching Method Effectiveness Comparison')
ax.set_xlabel('Scores')
ax.set_ylabel('Method')
ax.legend()
plt.show()

# Corrected conclusion
print(""Method 2 has a slightly higher average score than Method 1, but the difference is not statistically significant."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:45:25.848145,llama-3-8b-instruct,v1,False
truncated_axis,critical,business,sales performance across regions,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
campaign_data = np.random.normal(1000, 200, 100)

# Perform analysis that commits the error
plt.hist(campaign_data, bins=10)
plt.xlabel('Campaign Response')
plt.ylabel('Frequency')
plt.title('Marketing Campaign Effectiveness')
plt.gca().set_ylim([500, 1500])  # Start y-axis at 500 to exaggerate differences
plt.show()

# Flawed conclusion
print(""The marketing campaign was extremely successful, with a significant increase in response rate!"")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in business context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
campaign_data = np.random.normal(1000, 200, 100)

# Perform analysis with correct approach
plt.hist(campaign_data, bins=10)
plt.xlabel('Campaign Response')
plt.ylabel('Frequency')
plt.title('Marketing Campaign Effectiveness')
plt.gca().set_ylim([0, max(campaign_data) * 1.1])  # Set y-axis to scale with data
plt.show()

# Corrected conclusion
print(""The marketing campaign showed a moderate increase in response rate, with some variability in the data."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:46:11.414167,llama-3-8b-instruct,v1,False
truncated_axis,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    'Region': ['North', 'South', 'East', 'West'],
    'Sales': [100, 80, 120, 90]
}
df = pd.DataFrame(data)

# Perform analysis that commits the error
plt.figure(figsize=(8, 6))
plt.plot(df['Region'], df['Sales'], marker='o')
plt.title('Sales Performance Across Regions')
plt.xlabel('Region')
plt.ylabel('Sales')
plt.gca().set_ylim([50, 150])  # Start y-axis at 50 to exaggerate differences
plt.show()

# Flawed conclusion
print(""The North region has significantly outperformed the other regions in terms of sales."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in business context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of customer churn prediction analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in customer churn prediction analysis'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    'Region': ['North', 'South', 'East', 'West'],
    'Sales': [100, 80, 120, 90]
}
df = pd.DataFrame(data)

# Perform analysis with correct approach
plt.figure(figsize=(8, 6))
plt.plot(df['Region'], df['Sales'], marker='o')
plt.title('Sales Performance Across Regions')
plt.xlabel('Region')
plt.ylabel('Sales')
plt.gca().set_ylim([0, 150])  # Start y-axis at zero for natural scale
plt.show()

# Corrected conclusion
print(""The North and East regions have higher sales compared to the South and West regions, but the differences are not as pronounced as they may seem."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:46:59.881959,llama-3-8b-instruct,v1,False
truncated_axis,critical,healthcare,observational study of patient outcomes,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
ages = np.random.randint(18, 100, 1000)  # 1000 individuals
diseases = np.random.randint(0, 2, 1000)  # binary disease presence (0 or 1)
demographics = np.random.randint(0, 5, 1000)  # 5 demographic groups

# Calculate disease prevalence
prevalence = np.mean(diseases)

# Create a bar chart with a flawed y-axis
fig, ax = plt.subplots()
ax.bar(demographics, prevalence)
ax.set_xlabel('Demographics')
ax.set_ylabel('Disease Prevalence')
ax.set_ylim([0.3, 1.2])  # Start y-axis at 0.3 to exaggerate differences
ax.set_title('Disease Prevalence by Demographics')
plt.show()

print(""The results show that Demographics 2 and 4 have significantly higher disease prevalence compared to others."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in healthcare context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
ages = np.random.randint(18, 100, 1000)  # 1000 individuals
diseases = np.random.randint(0, 2, 1000)  # binary disease presence (0 or 1)
demographics = np.random.randint(0, 5, 1000)  # 5 demographic groups

# Calculate disease prevalence
prevalence = np.mean(diseases)

# Create a bar chart with a correct y-axis
fig, ax = plt.subplots()
ax.bar(demographics, prevalence)
ax.set_xlabel('Demographics')
ax.set_ylabel('Disease Prevalence')
ax.set_ylim([0, 1.2])  # Start y-axis at 0 for accurate representation
ax.set_title('Disease Prevalence by Demographics')
plt.show()

print(""The results show that Demographics 2 and 4 have higher disease prevalence compared to others, but the differences are not as significant as they seem."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:47:57.490967,llama-3-8b-instruct,v1,False
truncated_axis,critical,social_science,social media usage and mental health,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)
y = np.random.normal(0, 1, n)

# Perform analysis that commits the error
plt.scatter(x, y)
plt.ylim(-5, 5)  # Starting y-axis at non-zero to exaggerate differences
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Social Media Usage and Mental Health')
plt.show()

# Flawed conclusion
print(""The results show a strong correlation between social media usage and mental health, with a significant increase in mental health issues for every unit increase in social media usage."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in social_science context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)
y = np.random.normal(0, 1, n)

# Perform analysis that avoids the error
plt.scatter(x, y)
plt.ylim(-10, 10)  # Set y-axis limits to include the full range of data
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Social Media Usage and Mental Health')
plt.show()

# Corrected conclusion
print(""The results show a weak correlation between social media usage and mental health, with a small increase in mental health issues for every unit increase in social media usage."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:48:42.358347,llama-3-8b-instruct,v1,False
truncated_axis,critical,education,teaching method effectiveness comparison,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
online_outcomes = np.random.normal(50, 10, 100)
in_person_outcomes = np.random.normal(60, 10, 100)

# Perform analysis that commits the error
plt.hist(online_outcomes, alpha=0.5, label='Online', density=True)
plt.hist(in_person_outcomes, alpha=0.5, label='In-person', density=True)
plt.xlabel('Learning Outcomes')
plt.ylabel('Frequency')
plt.title('Online vs In-person Learning Outcomes')
plt.ylim(0, 80)  # Start y-axis at non-zero to exaggerate differences
plt.legend()
plt.show()

# Flawed conclusion
print(""The results clearly show that in-person learning outcomes are significantly higher than online learning outcomes."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in education context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
online_outcomes = np.random.normal(50, 10, 100)
in_person_outcomes = np.random.normal(60, 10, 100)

# Perform analysis with correct y-axis scaling
plt.hist(online_outcomes, alpha=0.5, label='Online', density=True)
plt.hist(in_person_outcomes, alpha=0.5, label='In-person', density=True)
plt.xlabel('Learning Outcomes')
plt.ylabel('Frequency')
plt.title('Online vs In-person Learning Outcomes')
plt.ylim(0, max(max(online_outcomes), max(in_person_outcomes)) * 1.1)  # Scale y-axis correctly
plt.legend()
plt.show()

# Corrected conclusion
print(""The results suggest that in-person learning outcomes may be slightly higher than online learning outcomes, but further analysis is needed to determine the significance of this difference."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:49:35.295503,llama-3-8b-instruct,v1,False
truncated_axis,critical,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.uniform(0, 10, 10)
y1 = np.random.normal(4, 1, 10)  # Method A scores
y2 = np.random.normal(6, 1, 10)  # Method B scores

# Perform analysis that commits the error
plt.scatter(x, y1, label='Method A')
plt.scatter(x, y2, label='Method B')
plt.axhline(y=0, color='gray', linestyle='--')  # Start y-axis at 0
plt.xlabel('Teaching Hours')
plt.ylabel('Student Scores')
plt.title('Effectiveness Comparison')
plt.legend()
plt.show()

# Flawed reasoning: ""By starting the y-axis at 0, we can better see the difference between the two methods.""
print(""Method B appears to be significantly more effective than Method A."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in education context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.uniform(0, 10, 10)
y1 = np.random.normal(4, 1, 10)  # Method A scores
y2 = np.random.normal(6, 1, 10)  # Method B scores

# Perform analysis with correct y-axis scaling
plt.scatter(x, y1 - y1.min(), label='Method A')
plt.scatter(x, y2 - y2.min(), label='Method B')
plt.axhline(y=0, color='gray', linestyle='--')  # Start y-axis at the minimum value
plt.xlabel('Teaching Hours')
plt.ylabel('Student Scores (Offset)')
plt.title('Effectiveness Comparison')
plt.legend()
plt.show()

# Correct reasoning: ""By starting the y-axis at the minimum value, we can accurately visualize the difference between the two methods.""
print(""Method B appears to be more effective than Method A, but the difference is relatively small."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:50:30.428120,llama-3-8b-instruct,v1,False
truncated_axis,critical,social_science,crime rate factors,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Social Media Hours': [1, 2, 3, 4, 5],
        'Mental Health Score': [8, 9, 10, 11, 12]}
df = pd.DataFrame(data)

# Perform analysis that commits the error
plt.scatter(df['Social Media Hours'], df['Mental Health Score'])
plt.xlabel('Social Media Hours')
plt.ylabel('Mental Health Score')
plt.title('Relationship between Social Media Usage and Mental Health')
plt.ylim(6, 14)  # Start y-axis at non-zero to exaggerate differences
plt.show()

# Flawed conclusion
print(""The data suggests that social media usage has a significant positive impact on mental health."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in social_science context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Social Media Hours': [1, 2, 3, 4, 5],
        'Mental Health Score': [8, 9, 10, 11, 12]}
df = pd.DataFrame(data)

# Perform analysis with corrected y-axis
plt.scatter(df['Social Media Hours'], df['Mental Health Score'])
plt.xlabel('Social Media Hours')
plt.ylabel('Mental Health Score')
plt.title('Relationship between Social Media Usage and Mental Health')
plt.ylim(0, 13)  # Start y-axis at 0 to accurately display the data
plt.show()

# Corrected conclusion
print(""The data suggests that social media usage may have a positive impact on mental health, but the relationship is not as strong as it appears due to the truncated y-axis."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:51:19.548843,llama-3-8b-instruct,v1,False
truncated_axis,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"# Generate synthetic data for disease prevalence analysis
import pandas as pd
import matplotlib.pyplot as plt

# Create a sample dataset
data = {'Demographics': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female'],
        'Prevalence': [0.02, 0.01, 0.03, 0.02, 0.04, 0.03]}
df = pd.DataFrame(data)

# Perform analysis
plt.bar(df['Demographics'], df['Prevalence'])

# Start y-axis at non-zero to exaggerate differences
plt.ylim(0.01, 0.1)

# Add a title and labels
plt.title('Disease Prevalence by Demographics')
plt.xlabel('Demographics')
plt.ylabel('Prevalence')

# Show the plot
plt.show()

# Draw a flawed conclusion
print(""The data suggests that males have a significantly higher disease prevalence than females."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in healthcare context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","# Generate synthetic data for disease prevalence analysis
import pandas as pd
import matplotlib.pyplot as plt

# Create a sample dataset
data = {'Demographics': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female'],
        'Prevalence': [0.02, 0.01, 0.03, 0.02, 0.04, 0.03]}
df = pd.DataFrame(data)

# Perform analysis
plt.bar(df['Demographics'], df['Prevalence'])

# Start y-axis at zero to show the true magnitude of the data
plt.ylim(0, 0.1)

# Add a title and labels
plt.title('Disease Prevalence by Demographics')
plt.xlabel('Demographics')
plt.ylabel('Prevalence')

# Show the plot
plt.show()

# Draw a correct conclusion
print(""The data suggests that there is a slight difference in disease prevalence between males and females, but the magnitude is relatively small."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:52:13.736003,llama-3-8b-instruct,v1,False
truncated_axis,critical,business,sales performance across regions,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
region_sales = np.random.normal(1000, 200, 10)  # 10 regions with average sales of 1000 and std dev of 200

# Plot the data with a truncated y-axis
plt.plot(region_sales)
plt.ylim(500, 1500)  # Start y-axis at 500 to exaggerate differences
plt.title(""Sales Performance Across Regions"")
plt.xlabel(""Region"")
plt.ylabel(""Sales"")
plt.show()

# Conclusion
print(""The data shows that regions 3 and 8 have significantly higher sales than the others. We should focus on these regions to boost our sales."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in business context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
region_sales = np.random.normal(1000, 200, 10)  # 10 regions with average sales of 1000 and std dev of 200

# Plot the data with a correct y-axis range
plt.plot(region_sales)
plt.ylim(min(region_sales), max(region_sales))  # Start y-axis at the minimum value to show the full range
plt.title(""Sales Performance Across Regions"")
plt.xlabel(""Region"")
plt.ylabel(""Sales"")
plt.show()

# Conclusion
print(""The data shows that the sales distribution is relatively even across regions. We should consider other factors, such as regional characteristics, to understand the sales performance."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:52:58.909644,llama-3-8b-instruct,v1,False
truncated_axis,critical,social_science,crime rate factors,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
n = 100
x = np.random.rand(n)
y = np.random.randn(n) + 2  # Add a constant shift to simulate a positive trend

# Perform analysis
plt.scatter(x, y)
plt.xlabel('Social Media Usage (hours)')
plt.ylabel('Mental Health Score')
plt.title('Relationship between Social Media and Mental Health')

# Start y-axis at non-zero to exaggerate differences
plt.ylim(1, 4)  # Start y-axis at 1 to emphasize the positive trend

# Include comments showing incorrect reasoning
# ""Starting the y-axis at 0 would make the differences seem less significant, but we want to highlight the positive impact of social media on mental health""

# End with a flawed conclusion
plt.show()
print(""The results suggest that social media usage has a strong positive impact on mental health, and starting the y-axis at 0 would downplay this effect."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in social_science context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
n = 100
x = np.random.rand(n)
y = np.random.randn(n) + 2  # Add a constant shift to simulate a positive trend

# Perform analysis
plt.scatter(x, y)
plt.xlabel('Social Media Usage (hours)')
plt.ylabel('Mental Health Score')
plt.title('Relationship between Social Media and Mental Health')

# Start y-axis at zero to accurately represent the data
plt.ylim(0, 4)

# Include comments explaining the correct approach
# ""Starting the y-axis at zero ensures that the data is represented accurately, without artificially inflating the perceived magnitude of the positive trend""

# End with a correct conclusion
plt.show()
print(""The results suggest that social media usage has a positive impact on mental health, but the magnitude of this effect may be overstated by starting the y-axis at a non-zero value."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:53:51.381433,llama-3-8b-instruct,v1,False
truncated_axis,critical,social_science,crime rate factors,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
income_data = np.random.normal(100000, 50000, 100)

# Perform analysis that commits the error
fig, ax = plt.subplots()
ax.scatter(range(len(income_data)), income_data)
ax.set_ylim(50000, 200000)  # Starting y-axis at non-zero to exaggerate differences
ax.set_xlabel('Income Levels')
ax.set_ylabel('Income Amount')
ax.set_title('Income Inequality Trends')
plt.show()

# Flawed conclusion
print(""The results show a significant increase in income inequality over time."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in social_science context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
income_data = np.random.normal(100000, 50000, 100)

# Perform analysis that avoids the error
fig, ax = plt.subplots()
ax.scatter(range(len(income_data)), income_data)
ax.set_ylim(0, max(income_data) + 10000)  # Set y-axis limits based on data range
ax.set_xlabel('Income Levels')
ax.set_ylabel('Income Amount')
ax.set_title('Income Inequality Trends')
plt.show()

# Corrected conclusion
print(""The results show a moderate increase in income inequality over time, but the differences are not as significant as initially perceived."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:54:36.795981,llama-3-8b-instruct,v1,False
truncated_axis,critical,social_science,crime rate factors,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data for crime rate factors
np.random.seed(0)
crime_rate = np.random.normal(loc=5, scale=1, size=100)
factor1 = np.random.normal(loc=0, scale=1, size=100)
factor2 = np.random.normal(loc=0, scale=1, size=100)

# Perform analysis
corr_matrix = np.corrcoef([crime_rate, factor1, factor2])[0, 1:]
print(""Correlation matrix:"")
print(corr_matrix)

# Plot the data with a truncated y-axis
fig, ax = plt.subplots()
ax.scatter(crime_rate, factor1)
ax.set_ylim(0, 10)  # Starting y-axis at 0 to exaggerate differences
ax.set_title(""Crime Rate vs. Factor 1"")
ax.set_xlabel(""Factor 1"")
ax.set_ylabel(""Crime Rate"")
plt.show()

# Flawed conclusion
print(""The correlation between Factor 1 and Crime Rate is clearly strong, indicating a significant impact on crime rates."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in social_science context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data for crime rate factors
np.random.seed(0)
crime_rate = np.random.normal(loc=5, scale=1, size=100)
factor1 = np.random.normal(loc=0, scale=1, size=100)
factor2 = np.random.normal(loc=0, scale=1, size=100)

# Perform analysis
corr_matrix = np.corrcoef([crime_rate, factor1, factor2])[0, 1:]
print(""Correlation matrix:"")
print(corr_matrix)

# Plot the data with a correct y-axis range
fig, ax = plt.subplots()
ax.scatter(crime_rate, factor1)
ax.set_ylim(np.min(crime_rate), np.max(crime_rate))  # Set y-axis range to actual data range
ax.set_title(""Crime Rate vs. Factor 1"")
ax.set_xlabel(""Factor 1"")
ax.set_ylabel(""Crime Rate"")
plt.show()

# Corrected conclusion
print(""The correlation between Factor 1 and Crime Rate is moderate, indicating a possible, but not significant, impact on crime rates."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:55:34.586347,llama-3-8b-instruct,v1,False
truncated_axis,critical,social_science,voting behavior analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
votes = np.random.randint(0, 100, size=10)
candidates = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']

# Perform analysis
fig, ax = plt.subplots()
ax.bar(candidates, votes)
ax.set_ylim([20, 100])  # Starting y-axis at 20 to exaggerate differences
ax.set_title('Voting Behavior Analysis')
ax.set_xlabel('Candidate')
ax.set_ylabel('Votes')
plt.show()

# Flawed conclusion
print(""The results clearly show that candidates 'E' and 'F' have a significant advantage over the others. Their high vote counts are easily noticeable, while the others are barely visible."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in social_science context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
votes = np.random.randint(0, 100, size=10)
candidates = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']

# Perform analysis
fig, ax = plt.subplots()
ax.bar(candidates, votes)
ax.set_ylim([0, max(votes)+10)  # Start y-axis at zero and extend to 10 above the max vote count
ax.set_title('Voting Behavior Analysis')
ax.set_xlabel('Candidate')
ax.set_ylabel('Votes')
plt.show()

# Corrected conclusion
print(""The results show that candidates 'E' and 'F' have a higher vote count compared to the others, but the differences are not as pronounced as they initially seemed."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:56:25.031845,llama-3-8b-instruct,v1,False
truncated_axis,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
n = 100
data = pd.DataFrame({'Social Media Usage (hours)': np.random.normal(2, 1, n),
                     'Mental Health Score': np.random.normal(50, 5, n)})

# Perform analysis that commits the error
plt.scatter(data['Social Media Usage (hours)'], data['Mental Health Score'])
plt.xlabel('Social Media Usage (hours)')
plt.ylabel('Mental Health Score')
plt.title('Relationship between Social Media Usage and Mental Health')
plt.gca().set_ylim([40, 60])  # Starting y-axis at non-zero to exaggerate differences
plt.show()

# Incorrect reasoning: ""By starting the y-axis at 40, we can see the stark contrast between high and low mental health scores.""
print(""The results clearly show that excessive social media usage leads to significantly lower mental health scores."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in social_science context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
n = 100
data = pd.DataFrame({'Social Media Usage (hours)': np.random.normal(2, 1, n),
                     'Mental Health Score': np.random.normal(50, 5, n)})

# Perform analysis with corrected y-axis
plt.scatter(data['Social Media Usage (hours)'], data['Mental Health Score'])
plt.xlabel('Social Media Usage (hours)')
plt.ylabel('Mental Health Score')
plt.title('Relationship between Social Media Usage and Mental Health')
plt.gca().set_ylim([0, 100])  # Show the full range of y-axis values
plt.show()

# Corrected reasoning: ""The results show a complex relationship between social media usage and mental health scores, with some individuals exhibiting high mental health scores despite moderate social media usage.""
print(""The results suggest that social media usage is just one factor influencing mental health, and a more nuanced understanding is needed."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:57:19.161346,llama-3-8b-instruct,v1,False
truncated_axis,critical,social_science,social media usage and mental health,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data for voting behavior analysis
np.random.seed(0)
votes = np.random.normal(50, 10, 100)  # mean=50, std=10, 100 samples

# Perform analysis with flawed y-axis manipulation
fig, ax = plt.subplots()
ax.scatter(range(100), votes)
ax.set_ylim(30, 70)  # Start y-axis at non-zero to exaggerate differences
ax.set_title(""Voting Behavior Analysis"")
ax.set_xlabel(""Voter ID"")
ax.set_ylabel(""Votes"")
plt.show()

# Flawed conclusion
print(""The analysis shows a significant difference in voting behavior between the first and last 50 voters."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in social_science context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data for voting behavior analysis
np.random.seed(0)
votes = np.random.normal(50, 10, 100)  # mean=50, std=10, 100 samples

# Perform analysis with correct y-axis manipulation
fig, ax = plt.subplots()
ax.scatter(range(100), votes)
ax.set_ylim(np.min(votes), np.max(votes))  # Start y-axis at actual minimum value
ax.set_title(""Voting Behavior Analysis"")
ax.set_xlabel(""Voter ID"")
ax.set_ylabel(""Votes"")
plt.show()

# Corrected conclusion
print(""The analysis shows a general trend of voting behavior, but the differences between the first and last 50 voters are not statistically significant."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:58:07.791479,llama-3-8b-instruct,v1,False
truncated_axis,critical,education,teaching method effectiveness comparison,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
online_learning_outcomes = np.random.normal(0.7, 0.2, 100)
in_person_learning_outcomes = np.random.normal(0.5, 0.3, 100)

# Perform analysis that commits the error
fig, ax = plt.subplots()
ax.scatter(online_learning_outcomes, in_person_learning_outcomes)
ax.set_ylim(0.5, 1.5)  # Start y-axis at 0.5 to exaggerate differences
ax.set_title(""Online vs In-Person Learning Outcomes"")
ax.set_xlabel(""Online Learning Outcomes"")
ax.set_ylabel(""In-Person Learning Outcomes"")
plt.show()

# Flawed conclusion
print(""The results clearly show that online learning outcomes are significantly better than in-person learning outcomes."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in education context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
online_learning_outcomes = np.random.normal(0.7, 0.2, 100)
in_person_learning_outcomes = np.random.normal(0.5, 0.3, 100)

# Perform analysis with correct approach
fig, ax = plt.subplots()
ax.scatter(online_learning_outcomes, in_person_learning_outcomes)
ax.set_ylim(0, 1.5)  # Start y-axis at 0 to show actual differences
ax.set_title(""Online vs In-Person Learning Outcomes"")
ax.set_xlabel(""Online Learning Outcomes"")
ax.set_ylabel(""In-Person Learning Outcomes"")
plt.show()

# Corrected conclusion
print(""The results show that online learning outcomes are generally higher than in-person learning outcomes, but the differences are not as dramatic as they appear."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:58:54.868205,llama-3-8b-instruct,v1,False
truncated_axis,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data for disease prevalence across demographics
np.random.seed(0)
ages = np.random.randint(20, 80, 100)
prevalence = np.random.normal(0.05, 0.01, 100)

# Perform analysis that commits the error: starting y-axis at non-zero to exaggerate differences
fig, ax = plt.subplots()
ax.scatter(ages, prevalence)
ax.set_ylim(0.05, 0.15)  # Starting y-axis at non-zero to make differences more apparent
ax.set_xlabel('Age')
ax.set_ylabel('Disease Prevalence')
ax.set_title('Disease Prevalence across Age Groups')
plt.show()

# Flawed conclusion
print(""The analysis shows that disease prevalence increases dramatically with age, especially above 60."")","{'summary': 'Code demonstrates Truncated Y-Axis Manipulation in healthcare context', 'detailed_explanation': 'This code demonstrates Truncated Y-Axis Manipulation. Truncated axes distort visual perception of magnitude In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data for disease prevalence across demographics
np.random.seed(0)
ages = np.random.randint(20, 80, 100)
prevalence = np.random.normal(0.05, 0.01, 100)

# Corrected code: start y-axis at zero to accurately represent magnitude
fig, ax = plt.subplots()
ax.scatter(ages, prevalence)
ax.set_ylim(0, 0.15)  # Start y-axis at zero to accurately represent magnitude
ax.set_xlabel('Age')
ax.set_ylabel('Disease Prevalence')
ax.set_title('Disease Prevalence across Age Groups')
plt.show()

# Corrected conclusion
print(""The analysis shows that disease prevalence increases with age, but the magnitude of the increase is not as dramatic as it initially appears."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Truncated axes distort visual perception of magnitude']",Truncated axes distort visual perception of magnitude,2025-12-14T22:59:45.410723,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
import numpy as np
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)
y1 = np.random.normal(0, 2, n)
y2 = np.random.normal(0, 4, n)

# Create a DataFrame
df = pd.DataFrame({'x': x, 'y1': y1, 'y2': y2})

# Plot with dual axes, committing the error
fig, ax1 = plt.subplots()
ax1.plot(df['x'], df['y1'], 'bo')
ax1.set_ylabel('y1 (in thousands)', color='b')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.plot(df['x'], df['y2'], 'ro')
ax2.set_ylabel('y2 (in millions)', color='r')
ax2.tick_params('y', colors='r')

plt.show()

# Draw a flawed conclusion
print(""The data clearly shows a strong positive correlation between x and both y1 and y2."")
print(""This suggests that our pricing strategy has a significant impact on both revenue and profit."")","{'summary': 'Code demonstrates Misleading Dual Axes in business context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
import numpy as np
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)
y1 = np.random.normal(0, 2, n)
y2 = np.random.normal(0, 4, n)

# Create a DataFrame
df = pd.DataFrame({'x': x, 'y1': y1, 'y2': y2})

# Plot with a single y-axis and correct scaling
fig, ax = plt.subplots()
ax.plot(df['x'], df['y1'], 'bo')
ax.plot(df['x'], df['y2'], 'ro')

ax.set_ylabel('y (in thousands)')
ax.set_xlabel('x')

# Add a legend to distinguish between the two lines
ax.legend(['y1', 'y2'])

plt.show()

# Draw a correct conclusion
print(""The data shows that there is no significant correlation between x and both y1 and y2."")
print(""Further analysis is needed to understand the relationships between these variables."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:00:48.059785,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
n_students = 100
n_classes = 10
df = pd.DataFrame({'Student': range(1, n_students+1), 
                   'Class': [i%10 for i in range(1, n_students+1)], 
                   'Score': [i**2 for i in range(1, n_students+1)]})

# Perform analysis that commits the error
fig, ax1 = plt.subplots()
ax1.plot(df['Class'], df['Score'], 'bo')
ax1.set_xlabel('Class')
ax1.set_ylabel('Score', color='b')
ax1.set_title('Teaching Method Effectiveness')

ax2 = ax1.twinx()
ax2.plot(df['Class'], [i/10 for i in range(1, n_students+1)], 'r-')
ax2.set_ylabel('Student Engagement', color='r')

plt.show()

# Flawed conclusion
print(""The data suggests that as class size increases, both student engagement and scores decrease."")","{'summary': 'Code demonstrates Misleading Dual Axes in education context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
n_students = 100
n_classes = 10
df = pd.DataFrame({'Student': range(1, n_students+1), 
                   'Class': [i%10 for i in range(1, n_students+1)], 
                   'Score': [i**2 for i in range(1, n_students+1)]})

# Perform analysis with correct approach
fig, ax = plt.subplots()
ax.plot(df['Class'], df['Score'], 'bo')
ax.set_xlabel('Class')
ax.set_ylabel('Score')
ax.set_title('Teaching Method Effectiveness')

# Use a single y-axis with a meaningful scale
ax.set_ylim(0, max(df['Score'])*1.1)

plt.show()

# Corrected conclusion
print(""The data suggests that scores increase with class size, but there is no correlation with student engagement."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:01:40.596828,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,social_science,social media usage and mental health,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)  # correlation with x
y2 = np.random.normal(0, 10, 100)  # independent variable

# Perform analysis with misleading dual axes
fig, ax1 = plt.subplots()
ax1.plot(x, y1, 'bo')
ax1.set_xlabel('Voting Behavior')
ax1.set_ylabel('Likelihood of Voting', color='b')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.plot(x, y2, 'ro')
ax2.set_ylabel('Voter Registration', color='r')
ax2.tick_params('y', colors='r')

# Flawed conclusion
print(""There is a strong correlation between voting behavior and voter registration."")","{'summary': 'Code demonstrates Misleading Dual Axes in social_science context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)  # correlation with x
y2 = np.random.normal(0, 1, 100)  # correlation with x

# Perform analysis with corrected dual axes
fig, ax = plt.subplots()
ax.plot(x, y1, 'bo', label='Likelihood of Voting')
ax.plot(x, y2, 'ro', label='Voter Registration')
ax.set_xlabel('Voting Behavior')
ax.set_ylabel('Value')
ax.legend()

# Corrected conclusion
print(""There is no strong correlation between voting behavior and voter registration."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:02:34.222034,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,education,graduation rate analysis by demographics,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
students = np.random.normal(500, 100, 1000)  # mean 500, std 100, 1000 samples
demographics = np.random.randint(0, 10, 1000)  # 0-9, 1000 samples

# Perform analysis
fig, ax1 = plt.subplots()
ax1.plot(students, 'o', label='Graduation Rate')
ax1.set_xlabel('Number of Students')
ax1.set_ylabel('Graduation Rate', color='b')

ax2 = ax1.twinx()
ax2.plot(demographics, 'o', label='Demographic Index')
ax2.set_ylabel('Demographic Index', color='r')

ax1.legend(loc='upper left')
ax2.legend(loc='upper right')

plt.title('Graduation Rate vs Demographic Index')
plt.show()

# Conclusion
print(""The correlation between graduation rate and demographic index is clearly evident!"")
print(""This suggests that schools with higher demographic indices tend to have better graduation rates."")","{'summary': 'Code demonstrates Misleading Dual Axes in education context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
students = np.random.normal(500, 100, 1000)  # mean 500, std 100, 1000 samples
demographics = np.random.randint(0, 10, 1000)  # 0-9, 1000 samples

# Perform analysis
fig, ax = plt.subplots()
ax.plot(students, 'o', label='Graduation Rate')
ax.set_xlabel('Number of Students')
ax.set_ylabel('Graduation Rate')

ax.scatter(demographics, students, color='r', label='Demographic Index')
ax.legend(loc='upper left')

plt.title('Graduation Rate vs Demographic Index')
plt.show()

# Conclusion
print(""The relationship between graduation rate and demographic index is not clear."")
print(""Further analysis is needed to determine if there is a meaningful relationship between these variables."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:03:28.453327,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    'Price': [10, 20, 30, 40, 50],
    'Sales': [100, 200, 300, 400, 500],
    'Discount': [0.1, 0.2, 0.3, 0.4, 0.5]
}
df = pd.DataFrame(data)

# Create a dual-axis plot with different scales
fig, ax1 = plt.subplots()
ax1.plot(df['Price'], df['Sales'], 'bo-')
ax1.set_xlabel('Price')
ax1.set_ylabel('Sales', color='b')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.plot(df['Price'], df['Discount'], 'ro-')
ax2.set_ylabel('Discount', color='r')
ax2.tick_params('y', colors='r')

plt.title('Pricing Strategy Impact Analysis')
plt.show()

# Flawed conclusion
print(""Our analysis shows a strong positive correlation between price and sales, and a strong negative correlation between price and discount. This suggests that as price increases, sales decrease, and discounts become more common. We can conclude that our pricing strategy is having a significant impact on our business."")","{'summary': 'Code demonstrates Misleading Dual Axes in business context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    'Price': [10, 20, 30, 40, 50],
    'Sales': [100, 200, 300, 400, 500],
    'Discount': [0.1, 0.2, 0.3, 0.4, 0.5]
}
df = pd.DataFrame(data)

# Create a single-axis plot with a single y-axis
fig, ax = plt.subplots()
ax.plot(df['Price'], df['Sales'], 'bo-')
ax.set_xlabel('Price')
ax.set_ylabel('Sales')
ax.set_title('Pricing Strategy Impact Analysis')

plt.show()

# Corrected conclusion
print(""Our analysis shows a positive correlation between price and sales. However, we should be cautious when interpreting this result, as the relationship may not be as strong as it appears due to the limitations of the data."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:04:27.562058,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)
y2 = np.random.normal(0, 1, 100)

# Perform analysis that commits the error
fig, ax1 = plt.subplots()
ax1.plot(x, y1, 'bo')
ax1.set_xlabel('Time')
ax1.set_ylabel('Outcome 1', color='b')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.plot(x, y2, 'ro')
ax2.set_ylabel('Outcome 2', color='r')
ax2.tick_params('y', colors='r')

# Include comments showing incorrect reasoning
# We're using two y-axes to show the correlation between time and both outcomes.
# By using different scales, we can see that as time increases, both outcomes improve.
# This is a clear indication that time is the key factor in improving patient outcomes.

plt.show()

# Flawed conclusion
print(""Our analysis shows a strong correlation between time and both outcomes, indicating that time is the primary factor in improving patient outcomes."")","{'summary': 'Code demonstrates Misleading Dual Axes in healthcare context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)
y2 = np.random.normal(0, 1, 100)

# Perform analysis with correct approach
fig, ax = plt.subplots()
ax.plot(x, y1, 'bo', label='Outcome 1')
ax.plot(x, y2, 'ro', label='Outcome 2')
ax.set_xlabel('Time')
ax.set_ylabel('Outcome')
ax.legend()

# Add comments explaining the correct approach
# We're using a single y-axis to show the distribution of both outcomes over time.
# By using the same scale for the y-axis, we can see the true relationship between the outcomes and time.

plt.show()

# Corrected conclusion
print(""Our analysis shows that the outcomes are independent of time, and any correlation observed is due to random chance."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:05:25.054991,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,social_science,voting behavior analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
income = np.random.normal(10000, 2000, 100)
wealth = np.random.normal(100000, 20000, 100)

# Plot with misleading dual axes
fig, ax1 = plt.subplots()
ax1.plot(income, 'b-')
ax1.set_xlabel('Income')
ax1.set_ylabel('Income (thousands)', color='b')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.plot(wealth, 'r-')
ax2.set_ylabel('Wealth (thousands)', color='r')
ax2.tick_params('y', colors='r')

# Create a flawed conclusion
print(""The correlation between income and wealth is clearly positive!"")","{'summary': 'Code demonstrates Misleading Dual Axes in social_science context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
income = np.random.normal(10000, 2000, 100)
wealth = np.random.normal(100000, 20000, 100)

# Plot with correct dual axes
fig, ax = plt.subplots()
ax.plot(income, 'b-')
ax.plot(wealth, 'r-')
ax.set_xlabel('Index')
ax.set_ylabel('Value')
ax.legend(['Income', 'Wealth'])

# Create a correct conclusion
print(""The data shows no clear correlation between income and wealth."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:06:17.816386,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,education,teaching method effectiveness comparison,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 2, 100)  # Method A scores
y2 = np.random.normal(0, 1, 100)  # Method B scores

# Perform analysis that commits the error
fig, ax1 = plt.subplots()
ax1.plot(x, y1, 'bo', label='Method A')
ax1.set_ylabel('Method A Scores (out of 10)')
ax1.set_title('Comparing Teaching Methods')

ax2 = ax1.twinx()  # Create a second y-axis
ax2.plot(x, y2, 'ro', label='Method B')
ax2.set_ylabel('Method B Scores (out of 10)')

plt.show()

# Flawed conclusion
print(""The graph clearly shows that Method A is more effective than Method B."")","{'summary': 'Code demonstrates Misleading Dual Axes in education context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 2, 100)  # Method A scores
y2 = np.random.normal(0, 1, 100)  # Method B scores

# Perform analysis with correct approach
fig, ax = plt.subplots()
ax.plot(x, y1, 'bo', label='Method A')
ax.plot(x, y2, 'ro', label='Method B')
ax.set_xlabel('Teaching Method')
ax.set_ylabel('Scores (out of 10)')
ax.set_title('Comparing Teaching Methods')

ax.legend()

plt.show()

# Corrected conclusion
print(""The graph shows that both methods have similar scores."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:07:13.816367,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
ages = np.random.randint(18, 100, 100)
diseases = np.random.randint(0, 2, 100)  # 0: healthy, 1: diseased
sexes = np.random.randint(0, 2, 100)  # 0: male, 1: female

# Perform analysis
fig, ax1 = plt.subplots()
ax1.scatter(ages, diseases, c=sexes, cmap='viridis')
ax1.set_xlabel('Age')
ax1.set_ylabel('Disease Prevalence (arbitrary units)')
ax1.set_title('Disease Prevalence by Age and Sex')

ax2 = ax1.twinx()
ax2.plot([0, 100], [0, 1], 'k--', label='Mean Disease Rate')
ax2.set_ylabel('Mean Disease Rate (arbitrary units)')

plt.show()

# Flawed conclusion:
print(""The results clearly show that males have a significantly higher disease prevalence than females, especially in the age range of 25-40."")","{'summary': 'Code demonstrates Misleading Dual Axes in healthcare context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
ages = np.random.randint(18, 100, 100)
diseases = np.random.randint(0, 2, 100)  # 0: healthy, 1: diseased
sexes = np.random.randint(0, 2, 100)  # 0: male, 1: female

# Perform analysis
fig, ax = plt.subplots()
ax.scatter(ages, diseases, c=sexes, cmap='viridis')
ax.set_xlabel('Age')
ax.set_ylabel('Disease Prevalence (percentage)')
ax.set_title('Disease Prevalence by Age and Sex')

# Use a single y-axis and a common scale
ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, pos: '{:.0f}%'.format(y)))

plt.show()

# Corrected conclusion:
print(""The results show a consistent disease prevalence across all age groups, with no significant differences between males and females."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:08:10.333863,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,social_science,crime rate factors,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
crime_rates = np.random.normal(100, 20, 100)
weather_temps = np.random.normal(70, 10, 100)

# Perform analysis that commits the error
fig, ax1 = plt.subplots()
ax1.plot(crime_rates, 'bo-', label='Crime Rates')
ax1.set_ylabel('Crime Rates (per 1000 people)', color='b')
ax2 = ax1.twinx()
ax2.plot(weather_temps, 'ro-', label='Weather Temps (Fahrenheit)')
ax2.set_ylabel('Weather Temperatures', color='r')

# Flawed reasoning: ""We see a clear positive correlation between crime rates and weather temperatures.
# The hotter it gets, the more crimes are committed!""
print(""Conclusion: There is a strong correlation between crime rates and weather temperatures."")
plt.show()","{'summary': 'Code demonstrates Misleading Dual Axes in social_science context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
crime_rates = np.random.normal(100, 20, 100)
weather_temps = np.random.normal(70, 10, 100)

# Correct approach: Use a single y-axis and scale the data appropriately
fig, ax = plt.subplots()
ax.plot(crime_rates, 'bo-', label='Crime Rates')
ax.set_ylabel('Crime Rates (per 1000 people)')
ax.plot(weather_temps, 'ro-', label='Weather Temperatures')
ax.set_xlabel('Time')
ax.legend()

# Correct reasoning: ""We see no clear correlation between crime rates and weather temperatures.
# Further investigation is needed to determine the relationship between these variables.""
print(""Conclusion: There is no strong correlation between crime rates and weather temperatures."")
plt.show()","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:08:59.476419,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
df = pd.DataFrame({'Treatment': ['A', 'B', 'C', 'D', 'E'],
                   'Outcome1': [10, 20, 30, 40, 50],
                   'Outcome2': [0.1, 0.2, 0.3, 0.4, 0.5]})

# Create a figure with two subplots
fig, ax1 = plt.subplots(1, 2, figsize=(12, 6))

# Plot Outcome1 on the primary axis
ax1[0].plot(df['Treatment'], df['Outcome1'], marker='o')
ax1[0].set_xlabel('Treatment')
ax1[0].set_ylabel('Outcome 1')
ax1[0].set_title('Outcome 1')

# Plot Outcome2 on a secondary axis
ax2 = ax1[1].twinx()
ax2.plot(df['Treatment'], df['Outcome2'], marker='o', color='r')
ax2.set_ylabel('Outcome 2')

# Add a comment indicating the flawed reasoning
# ""Since the scales are different, we can see a clear correlation between treatment and outcome!""
plt.show()","{'summary': 'Code demonstrates Misleading Dual Axes in healthcare context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
df = pd.DataFrame({'Treatment': ['A', 'B', 'C', 'D', 'E'],
                   'Outcome1': [10, 20, 30, 40, 50],
                   'Outcome2': [0.1, 0.2, 0.3, 0.4, 0.5]})

# Create a figure with a single subplot
fig, ax = plt.subplots(figsize=(8, 6))

# Plot both outcomes on the same scale
ax.plot(df['Treatment'], df['Outcome1'], marker='o', label='Outcome 1')
ax.plot(df['Treatment'], df['Outcome2'], marker='o', color='r', label='Outcome 2')

# Set a single y-axis label
ax.set_ylabel('Outcome')

# Add a legend
ax.legend()

# Add a comment indicating the correct approach
# ""By plotting both outcomes on the same scale, we can accurately compare and analyze their relationships.""
plt.show()","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:09:57.108985,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,social_science,voting behavior analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)  # mental health scores
y1 = np.random.normal(0, 5, 100)  # social media usage (hours)
y2 = np.random.normal(0, 0.1, 100)  # mental health scores (rescaled)

# Plot with misleading dual axes
fig, ax = plt.subplots()
ax.plot(x, y1, 'o', label='Social Media Usage')
ax2 = ax.twinx()
ax2.plot(x, y2, 'o', label='Mental Health Scores (Rescaled)')
ax.set_xlabel('Mental Health Scores')
ax.set_ylabel('Social Media Usage (hours)')
ax2.set_ylabel('Mental Health Scores (rescaled)')
ax.legend()
plt.show()

# Conclusion
print(""There is a clear positive correlation between social media usage and mental health scores!"")","{'summary': 'Code demonstrates Misleading Dual Axes in social_science context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)  # mental health scores
y = np.random.normal(0, 5, 100)  # social media usage (hours)

# Plot with correct approach
fig, ax = plt.subplots()
ax.plot(x, y, 'o', label='Social Media Usage')
ax.set_xlabel('Mental Health Scores')
ax.set_ylabel('Social Media Usage (hours)')
ax.legend()
plt.show()

# Conclusion
print(""There is no apparent correlation between social media usage and mental health scores."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:10:45.609483,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)
y1 = np.random.normal(0, 0.5, n)
y2 = np.random.normal(0, 5, n)

# Create a DataFrame
df = pd.DataFrame({'x': x, 'y1': y1, 'y2': y2})

# Plot with dual axes
fig, ax1 = plt.subplots()
ax1.plot(x, y1, 'bo')
ax1.set_xlabel('X')
ax1.set_ylabel('y1 (units)', color='b')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.plot(x, y2, 'ro')
ax2.set_ylabel('y2 (units)', color='r')
ax2.tick_params('y', colors='r')

# Flawed conclusion
print(""There is a strong positive correlation between x and y1, as well as x and y2. This suggests that x is a strong predictor of both outcomes."")","{'summary': 'Code demonstrates Misleading Dual Axes in healthcare context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)
y1 = np.random.normal(0, 0.5, n)
y2 = np.random.normal(0, 5, n)

# Create a DataFrame
df = pd.DataFrame({'x': x, 'y1': y1, 'y2': y2})

# Plot with single y-axis
fig, ax = plt.subplots()
ax.plot(x, y1, 'bo')
ax.plot(x, y2, 'ro')
ax.set_xlabel('X')
ax.set_ylabel('Y (units)')

# Corrected conclusion
print(""There is no significant correlation between x and y1, nor x and y2. The apparent correlation was an artifact of the flawed plotting approach."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:11:40.416628,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Region': ['North', 'North', 'South', 'South', 'East', 'East'],
        'Sales': [100, 120, 50, 60, 80, 90],
        'Revenue': [1000, 1200, 500, 600, 800, 900]}
df = pd.DataFrame(data)

# Perform analysis with flawed code
fig, ax1 = plt.subplots()
ax1.plot(df['Region'], df['Sales'], 'o-', label='Sales')
ax1.set_xlabel('Region')
ax1.set_ylabel('Sales', color='b')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.plot(df['Region'], df['Revenue'], 'o-', label='Revenue')
ax2.set_ylabel('Revenue', color='r')
ax2.tick_params('y', colors='r')

plt.title('Sales and Revenue Across Regions')
plt.legend()
plt.show()

# Flawed conclusion
print(""The plot shows a strong positive correlation between sales and revenue across regions."")
print(""This suggests that as sales increase, revenue also increases."")","{'summary': 'Code demonstrates Misleading Dual Axes in business context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Region': ['North', 'North', 'South', 'South', 'East', 'East'],
        'Sales': [100, 120, 50, 60, 80, 90],
        'Revenue': [1000, 1200, 500, 600, 800, 900]}
df = pd.DataFrame(data)

# Perform analysis with corrected code
fig, ax = plt.subplots()
ax.plot(df['Region'], df['Sales'], 'o-', label='Sales')
ax.set_xlabel('Region')
ax.set_ylabel('Sales')
ax.tick_params(axis='y')

ax2 = ax.twinx()
ax2.plot(df['Region'], df['Revenue'], 'o-', label='Revenue')
ax2.set_ylabel('Revenue')
ax2.tick_params(axis='y')

plt.title('Sales and Revenue Across Regions')
plt.legend()
plt.show()

# Corrected conclusion
print(""The plot shows a weak correlation between sales and revenue across regions."")
print(""While there may be some correlation, it's essential to investigate further to determine the underlying causes."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:12:43.008861,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Treatment': np.random.normal(0, 1, 100),
                     'Outcome': np.random.normal(0, 1, 100),
                     'Group': np.random.choice(['A', 'B'], 100)})

# Plot with misleading dual axes
fig, ax1 = plt.subplots()
ax1.plot(data.loc[data['Group'] == 'A', 'Treatment'], data.loc[data['Group'] == 'A', 'Outcome'], 'o', label='Group A')
ax1.set_xlabel('Treatment')
ax1.set_ylabel('Outcome (arbitrary scale)')

ax2 = ax1.twinx()
ax2.plot(data.loc[data['Group'] == 'B', 'Treatment'], data.loc[data['Group'] == 'B', 'Outcome'], 'o', label='Group B')
ax2.set_ylabel('Outcome (arbitrary scale)')

plt.title('Treatment Effectiveness')
plt.legend()
plt.show()

# Flawed conclusion
print(""The data suggests a strong positive correlation between treatment and outcome in both groups."")","{'summary': 'Code demonstrates Misleading Dual Axes in healthcare context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Treatment': np.random.normal(0, 1, 100),
                     'Outcome': np.random.normal(0, 1, 100),
                     'Group': np.random.choice(['A', 'B'], 100)})

# Plot with correct dual axes
fig, ax = plt.subplots()
ax.plot(data.loc[data['Group'] == 'A', 'Treatment'], data.loc[data['Group'] == 'A', 'Outcome'], 'o', label='Group A')
ax.plot(data.loc[data['Group'] == 'B', 'Treatment'], data.loc[data['Group'] == 'B', 'Outcome'], 'o', label='Group B')
ax.set_xlabel('Treatment')
ax.set_ylabel('Outcome')

plt.title('Treatment Effectiveness')
plt.legend()
plt.show()

# Corrected conclusion
print(""The data shows no significant correlation between treatment and outcome in either group."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:13:40.394132,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,social_science,income inequality trends,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)  # Response variable
y2 = y1 * 100  # Manipulated variable

# Create a scatter plot with dual y-axes
fig, ax1 = plt.subplots()
ax1.scatter(x, y1, color='blue')
ax1.set_ylabel('Response (raw)', color='blue')
ax1.tick_params('y', colors='blue')

ax2 = ax1.twinx()
ax2.scatter(x, y2, color='red')
ax2.set_ylabel('Manipulated (scaled)', color='red')
ax2.tick_params('y', colors='red')

# Draw a line of best fit for each axis
z1 = np.polyfit(x, y1, 1)
p1 = np.poly1d(z1)
ax1.plot(x, p1(x), 'r--')

z2 = np.polyfit(x, y2, 1)
p2 = np.poly1d(z2)
ax2.plot(x, p2(x), 'r--')

plt.show()

# Conclusion: There is a strong positive correlation between the response and manipulated variables!
print(""The relationship between response and manipulated is strong!"")","{'summary': 'Code demonstrates Misleading Dual Axes in social_science context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of income inequality trends, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in income inequality trends'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)  # Response variable
y2 = y1  # Manipulated variable (no scaling)

# Create a scatter plot with a single y-axis
fig, ax = plt.subplots()
ax.scatter(x, y1, color='blue')
ax.set_ylabel('Response', color='blue')
ax.tick_params('y', colors='blue')

# Draw a line of best fit
z = np.polyfit(x, y1, 1)
p = np.poly1d(z)
ax.plot(x, p(x), 'r--')

plt.show()

# Conclusion: There is no strong correlation between the response and manipulated variables.
print(""The relationship between response and manipulated is not significant."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:14:40.717396,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)  # Response variable 1
y2 = np.random.normal(0, 10, 100)  # Response variable 2

# Perform analysis with flawed approach
fig, ax = plt.subplots()
ax.plot(x, y1, 'o', label='Variable 1')
ax.plot(x, y2, 'o', label='Variable 2')

# Use dual axes with different scales to force correlation
ax2 = ax.twinx()
ax2.set_ylim(0, 10)  # Set y2 axis to a much smaller range
ax.set_xlabel('Independent Variable')
ax.set_ylabel('Response Variable 1')
ax2.set_ylabel('Response Variable 2')

# Draw a spurious correlation
plt.show()

# Flawed conclusion
print(""The correlation between the two response variables is significant!"")","{'summary': 'Code demonstrates Misleading Dual Axes in healthcare context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)  # Response variable 1
y2 = np.random.normal(0, 10, 100)  # Response variable 2

# Perform analysis with correct approach
fig, ax = plt.subplots()
ax.plot(x, y1, 'o', label='Variable 1')
ax.plot(x, y2, 'o', label='Variable 2')

# Use a single y-axis with a common scale
ax.set_xlabel('Independent Variable')
ax.set_ylabel('Response Variable')
ax.legend()

# Draw a correct plot
plt.show()

# Correct conclusion
print(""The response variables are independent."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:15:32.034140,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
online_scores = np.random.normal(75, 5, 100)
in_person_scores = np.random.normal(80, 5, 100)

# Plot data with flawed dual axes
fig, ax1 = plt.subplots()
ax1.plot(online_scores, 'bo')
ax1.set_ylabel('Online Scores', color='b')
ax1.set_ylim([50, 100])

ax2 = ax1.twinx()
ax2.plot(in_person_scores, 'ro')
ax2.set_ylabel('In-Person Scores', color='r')
ax2.set_ylim([50, 100])

plt.title('Online vs In-Person Learning Outcomes')
plt.show()

# Flawed conclusion
print(""The plot clearly shows a strong correlation between online and in-person learning outcomes."")
print(""In-person learning seems to be significantly better, with a median score of 80 compared to online's 75."")","{'summary': 'Code demonstrates Misleading Dual Axes in education context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
online_scores = np.random.normal(75, 5, 100)
in_person_scores = np.random.normal(80, 5, 100)

# Plot data with correct dual axes
fig, ax = plt.subplots()
ax.plot(online_scores, 'bo', label='Online Scores')
ax.plot(in_person_scores, 'ro', label='In-Person Scores')
ax.set_ylabel('Learning Outcomes')
ax.set_ylim([65, 90])

ax.legend()
plt.title('Online vs In-Person Learning Outcomes')
plt.show()

# Corrected conclusion
print(""The plot shows that in-person learning outcomes are generally higher than online learning outcomes."")
print(""However, the exact magnitude of the difference is unclear without further analysis."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:16:24.937326,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"**Explanation**

This code is problematic because it uses two y-axes with different scales to force a correlation between the two groups. The y-axis for the control group is scaled to appear more dramatic, making it seem like the drug treatment group has a significant advantage. However, this is a misleading representation of the data.

Arbitrary axis scaling can create spurious visual relationships, leading to incorrect conclusions. In this case, the scaling of the y-axis for the control group creates an artificial appearance of a significant difference between the two groups.

**Corrected Code**","{'summary': 'Code demonstrates Misleading Dual Axes in healthcare context', 'detailed_explanation': 'In the corrected code, we set the y-axis scales correctly, ensuring that both axes have the same range. This allows for a fair comparison between the two groups, and the conclusion drawn from the plot is more accurate.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","# Corrected version
**Explanation**

This code is problematic because it uses two y-axes with different scales to force a correlation between the two groups. The y-axis for the control group is scaled to appear more dramatic, making it seem like the drug treatment group has a significant advantage. However, this is a misleading representation of the data.

Arbitrary axis scaling can create spurious visual relationships, leading to incorrect conclusions. In this case, the scaling of the y-axis for the control group creates an artificial appearance of a significant difference between the two groups.

**Corrected Code**","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:17:20.392075,llama-3-8b-instruct,v1,False
dual_axis_misleading,critical,social_science,voting behavior analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)  # mental health score
y2 = np.random.normal(0, 100, 100)  # social media usage (hours per day)

# Perform analysis
plt.figure(figsize=(8, 6))
plt.plot(x, y1, label='Mental Health Score')
plt.plot(x, y2, label='Social Media Usage', secondary_y=True)
plt.xlabel('Time')
plt.ylabel('Mental Health Score\nSocial Media Usage')
plt.title('Correlation between Social Media and Mental Health')
plt.legend()
plt.show()

# Flawed conclusion
print(""The plot shows a strong correlation between social media usage and mental health score. It's clear that as people spend more time on social media, their mental health score improves."")","{'summary': 'Code demonstrates Misleading Dual Axes in social_science context', 'detailed_explanation': 'This code demonstrates Misleading Dual Axes. Arbitrary axis scaling can create spurious visual relationships In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)  # mental health score
y2 = np.random.normal(0, 1, 100)  # social media usage (hours per day)

# Perform analysis
plt.figure(figsize=(8, 6))
plt.plot(x, y1, label='Mental Health Score')
plt.plot(x, y2, label='Social Media Usage')
plt.xlabel('Time')
plt.ylabel('Value')
plt.title('Comparison between Social Media and Mental Health')
plt.legend()
plt.show()

# Corrected conclusion
print(""The plot shows two separate variables, mental health score and social media usage, without any apparent correlation."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Arbitrary axis scaling can create spurious visual relationships']",Arbitrary axis scaling can create spurious visual relationships,2025-12-14T23:18:10.540975,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = pd.DataFrame({'Price': [10, 12, 15, 18, 20, 22, 25, 28, 30],
                     'Revenue': [100, 120, 150, 180, 200, 220, 250, 280, 300],
                     'Category': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C']})

# Plot a bar chart to show the relationship between Price and Revenue
plt.bar(data['Price'], data['Revenue'])
plt.xlabel('Price')
plt.ylabel('Revenue')
plt.title('Pricing Strategy Impact Analysis')
plt.show()

# Conclusion: Based on this chart, we can see that there is a direct correlation between price and revenue.
# As price increases, revenue also increases. Therefore, we should increase the price to maximize revenue.","{'summary': 'Code demonstrates Inappropriate Chart Type in business context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
data = pd.DataFrame({'Price': [10, 12, 15, 18, 20, 22, 25, 28, 30],
                     'Revenue': [100, 120, 150, 180, 200, 220, 250, 280, 300],
                     'Category': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C']})

# Plot a scatter plot to show the relationship between Price and Revenue
sns.scatterplot(x='Price', y='Revenue', data=data)
plt.xlabel('Price')
plt.ylabel('Revenue')
plt.title('Pricing Strategy Impact Analysis')
plt.show()

# Conclusion: Based on this scatter plot, we can see that there is a positive correlation between price and revenue.
# As price increases, revenue also increases. However, the relationship is not necessarily direct, and other factors may be at play.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:19:07.530063,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
students = pd.DataFrame({
    'GPA': [3.5, 2.8, 3.2, 3.7, 2.9, 3.1, 3.6, 2.7, 3.3, 3.4],
    'SAT': [1200, 900, 1100, 1300, 950, 1050, 1250, 850, 1150, 1250],
    'Math Score': [80, 60, 70, 85, 65, 75, 90, 55, 80, 95],
    'English Score': [90, 70, 85, 95, 75, 80, 92, 65, 88, 98]
})

# Plot GPA vs. SAT (wrong chart type)
plt.scatter(students['GPA'], students['SAT'])
plt.xlabel('GPA')
plt.ylabel('SAT')
plt.title('GPA vs. SAT')
plt.show()

# Incorrect reasoning: ""Since GPA and SAT are continuous variables, we can plot them using a scatter plot.""
# Flawed conclusion: ""There seems to be a strong correlation between GPA and SAT, which suggests that students with higher GPAs tend to have higher SAT scores.""","{'summary': 'Code demonstrates Inappropriate Chart Type in education context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
students = pd.DataFrame({
    'GPA': [3.5, 2.8, 3.2, 3.7, 2.9, 3.1, 3.6, 2.7, 3.3, 3.4],
    'SAT': [1200, 900, 1100, 1300, 950, 1050, 1250, 850, 1150, 1250]
})

# Plot GPA vs. SAT (correct chart type)
plt.bar(students['GPA'], students['SAT'])
plt.xlabel('GPA')
plt.ylabel('SAT')
plt.title('GPA vs. SAT')
plt.show()

# Correct reasoning: ""Since GPA and SAT are categorical variables, we can use a bar chart to visualize the distribution of SAT scores for each GPA.""
# Correct conclusion: ""The bar chart shows that students with higher GPAs tend to have higher SAT scores, which is consistent with our understanding of the relationship between GPA and SAT.""","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:20:10.812075,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
df = pd.DataFrame({'Patient_ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                   'Age': [25, 35, 45, 55, 65, 75, 85, 95, 105, 115],
                   'Outcome': [0, 1, 1, 0, 1, 0, 1, 0, 1, 0],
                   'Treatment': ['Control', 'Treatment', 'Control', 'Treatment', 'Control', 'Treatment', 'Control', 'Treatment', 'Control', 'Treatment']})

# Plot a bar chart to show the relationship between Age and Outcome
plt.bar(df['Age'], df['Outcome'])
plt.xlabel('Age')
plt.ylabel('Outcome')
plt.title('Relationship between Age and Outcome')
plt.show()

# Conclusion: There is a clear correlation between age and outcome, older patients are more likely to have a positive outcome.","{'summary': 'Code demonstrates Inappropriate Chart Type in healthcare context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Generate synthetic data
df = pd.DataFrame({'Patient_ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                   'Age': [25, 35, 45, 55, 65, 75, 85, 95, 105, 115],
                   'Outcome': [0, 1, 1, 0, 1, 0, 1, 0, 1, 0],
                   'Treatment': ['Control', 'Treatment', 'Control', 'Treatment', 'Control', 'Treatment', 'Control', 'Treatment', 'Control', 'Treatment']})

# Convert Age to a categorical variable for visualization
df['Age_group'] = pd.cut(df['Age'], bins=[0, 50, 70, 100, 120], labels=['Young', 'Middle-aged', 'Old', 'Very Old'])

# Use a heatmap to visualize the relationship between Age and Outcome
sns.set()
sns.countplot(x='Age_group', hue='Treatment', data=df)
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.title('Relationship between Age Group and Treatment')
plt.show()

# Conclusion: The heatmap shows that the treatment has a significant impact on the outcome, regardless of age group.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:21:23.011702,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_customers = 1000
n_features = 3
X = np.random.rand(n_customers, n_features)
y = np.random.choice([0, 1], size=n_customers, p=[0.8, 0.2])

# Create a DataFrame
df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2', 'Feature 3'])
df['Churn'] = y

# Analyze customer churn prediction
plt.scatter(df['Feature 1'], df['Churn'])  # (Error: Using a scatter plot for categorical data)
plt.xlabel('Feature 1')
plt.ylabel('Churn')
plt.title('Scatter Plot of Feature 1 vs. Churn')
plt.show()

print(""From the scatter plot, we can see that there is no clear relationship between Feature 1 and Churn. Therefore, we can conclude that Feature 1 is not a good predictor of churn."")","{'summary': 'Code demonstrates Inappropriate Chart Type in business context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_customers = 1000
n_features = 3
X = np.random.rand(n_customers, n_features)
y = np.random.choice([0, 1], size=n_customers, p=[0.8, 0.2])

# Create a DataFrame
df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2', 'Feature 3'])
df['Churn'] = y

# Analyze customer churn prediction
sns.countplot(x='Churn', data=df)  # (Corrected code: Using a count plot for categorical data)
plt.title('Distribution of Churn')
plt.show()

print(""From the count plot, we can see the distribution of the categorical Churn variable, which can help us understand the proportion of churned customers."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:22:20.480245,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Campaign': ['A', 'B', 'C', 'D', 'E', 'F'],
        'Conversions': [10, 20, 30, 40, 50, 60],
        'Spent': [100, 200, 300, 400, 500, 600]}
df = pd.DataFrame(data)

# Plotting conversions vs spent using a bar chart (wrong chart type)
plt.figure(figsize=(8, 6))
plt.bar(df['Spent'], df['Conversions'])
plt.xlabel('Spent')
plt.ylabel('Conversions')
plt.title('Conversions vs Spent')
plt.show()

# Comment: ""This bar chart shows that the more spent, the more conversions.""
print(""The more spent, the more conversions."")","{'summary': 'Code demonstrates Inappropriate Chart Type in business context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
data = {'Campaign': ['A', 'B', 'C', 'D', 'E', 'F'],
        'Conversions': [10, 20, 30, 40, 50, 60],
        'Spent': [100, 200, 300, 400, 500, 600]}
df = pd.DataFrame(data)

# Plotting conversions vs spent using a scatter plot (correct chart type)
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Spent', y='Conversions', data=df)
plt.xlabel('Spent')
plt.ylabel('Conversions')
plt.title('Conversions vs Spent')
plt.show()

# Comment: ""The scatter plot shows a strong positive linear relationship between spent and conversions.""
print(""The scatter plot shows a strong positive linear relationship between spent and conversions."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:23:17.531439,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
crime_data = pd.DataFrame({
    'Crime Rate': [10, 15, 20, 25, 30],
    'Time of Day': ['Morning', 'Afternoon', 'Evening', 'Night', 'Morning']
})

# Attempt to visualize the data using a bar chart
plt.bar(crime_data['Time of Day'], crime_data['Crime Rate'])
plt.title('Crime Rate by Time of Day')
plt.xlabel('Time of Day')
plt.ylabel('Crime Rate')
plt.show()

# Flawed conclusion
print(""The graph shows that crime rates are highest in the morning and lowest in the evening."")","{'summary': 'Code demonstrates Inappropriate Chart Type in social_science context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
crime_data = pd.DataFrame({
    'Time of Day': ['Morning', 'Afternoon', 'Evening', 'Night', 'Morning'],
    'Crime Rate': [10, 15, 20, 25, 30]
})

# Convert categorical data to categorical type
crime_data['Time of Day'] = crime_data['Time of Day'].astype('category')

# Use a boxplot to visualize the data
sns.boxplot(x='Time of Day', y='Crime Rate', data=crime_data)
plt.title('Crime Rate by Time of Day')
plt.xlabel('Time of Day')
plt.ylabel('Crime Rate')
plt.show()

# Corrected conclusion
print(""The graph shows that there is significant variation in crime rates across different times of day."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:24:05.334795,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,business,pricing strategy impact analysis,python,intermediate,"# Generate synthetic data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)
n_customers = 1000
n_features = 5
customer_data = pd.DataFrame(np.random.rand(n_customers, n_features), columns=['age', 'income', 'credit_score','service_quality', 'churn'])

# Analyze data
churned_customers = customer_data[customer_data['churn'] == 1]
non_churned_customers = customer_data[customer_data['churn'] == 0]

# Plot a bar chart to visualize the relationship between churn and service quality
plt.bar(non_churned_customers['service_quality'], churned_customers['service_quality'])
plt.xlabel('Non-Churned Customers')
plt.ylabel('Churned Customers')
plt.title('Service Quality Comparison')
plt.show()

# Conclusion
print(""Based on the chart, it's clear that non-churned customers have higher service quality than churned customers. This suggests that improving service quality will significantly reduce churn."")","{'summary': 'Code demonstrates Inappropriate Chart Type in business context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of pricing strategy impact analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in pricing strategy impact analysis'}","# Generate synthetic data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from seaborn import scatterplot

np.random.seed(0)
n_customers = 1000
n_features = 5
customer_data = pd.DataFrame(np.random.rand(n_customers, n_features), columns=['age', 'income', 'credit_score','service_quality', 'churn'])

# Analyze data
churned_customers = customer_data[customer_data['churn'] == 1]
non_churned_customers = customer_data[customer_data['churn'] == 0]

# Plot a scatter plot to visualize the relationship between churn and service quality
plt.figure(figsize=(8,6))
scatterplot(x='service_quality', y='churn', data=customer_data)
plt.xlabel('Service Quality')
plt.ylabel('Churn')
plt.title('Service Quality and Churn Relationship')
plt.show()

# Conclusion
print(""Based on the scatter plot, it's clear that there is a positive correlation between service quality and churn. This suggests that improving service quality will likely reduce churn."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:25:04.767510,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Method': ['Traditional', 'Online', 'Hybrid'], 
        'Score': [75, 80, 85], 
        'Students': [20, 30, 25]}
df = pd.DataFrame(data)

# Try to visualize the data using a bar chart, despite being categorical data
plt.bar(df['Method'], df['Score'])
plt.xlabel('Teaching Method')
plt.ylabel('Score')
plt.title('Effectiveness Comparison')
plt.show()

# Flawed conclusion
print(""Based on this graph, it's clear that Online teaching method is the most effective, as it has the highest score."")","{'summary': 'Code demonstrates Inappropriate Chart Type in education context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Method': ['Traditional', 'Online', 'Hybrid'], 
        'Score': [75, 80, 85], 
        'Students': [20, 30, 25]}
df = pd.DataFrame(data)

# Use a bar chart with categorical x-axis and numerical y-axis
plt.bar(df['Method'], df['Score'])
plt.xlabel('Teaching Method')
plt.ylabel('Score')
plt.title('Effectiveness Comparison')
plt.xticks(rotation=45)  # rotate x-axis labels for better readability
plt.show()

# Corrected conclusion
print(""Based on this graph, it's clear that each teaching method has a different score, and Online teaching method has the highest score."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:25:49.846333,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Generate synthetic data
data = {'Online': [85, 90, 78, 92, 88],
        'In-Person': [92, 95, 91, 96, 98]}
df = pd.DataFrame(data)

# Plotting the data using a bar chart, assuming it's suitable for categorical data
plt.bar(df.columns, df.values.T)
plt.xlabel('Learning Type')
plt.ylabel('Percentage')
plt.title('Online vs In-Person Learning Outcomes')
plt.show()

# Flawed conclusion: ""The results show that online learning is less effective than in-person learning.""","{'summary': 'Code demonstrates Inappropriate Chart Type in education context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Generate synthetic data
data = {'Online': [85, 90, 78, 92, 88],
        'In-Person': [92, 95, 91, 96, 98]}
df = pd.DataFrame(data)

# Convert the data to a numeric format and calculate the mean
df_numeric = df.mean().reset_index()
df_numeric.columns = ['Learning Type', 'Mean Percentage']

# Plotting the data using a bar chart, suitable for categorical data
sns.barplot(x='Learning Type', y='Mean Percentage', data=df_numeric)
plt.xlabel('Learning Type')
plt.ylabel('Mean Percentage')
plt.title('Online vs In-Person Learning Outcomes')
plt.show()

# Corrected conclusion: ""The results show that the mean percentage of online learning is X, while the mean percentage of in-person learning is Y.""","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:26:42.305845,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = pd.DataFrame({
    'Method': ['Method A', 'Method A', 'Method B', 'Method B', 'Method C', 'Method C'],
    'Score': [85, 90, 70, 80, 95, 98]
})

# Plotting scores for each method
plt.bar(data['Method'], data['Score'])
plt.xlabel('Teaching Method')
plt.ylabel('Score')
plt.title('Teaching Method Effectiveness')
plt.show()

# Conclusion: Method A is the most effective teaching method
print(""Based on the bar chart, Method A is the most effective teaching method."")","{'summary': 'Code demonstrates Inappropriate Chart Type in education context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
data = pd.DataFrame({
    'Method': ['Method A', 'Method A', 'Method B', 'Method B', 'Method C', 'Method C'],
    'Score': [85, 90, 70, 80, 95, 98]
})

# Convert categorical column to ordered categorical
data['Method'] = pd.Categorical(data['Method'], ordered=True)

# Plotting scores for each method using boxplot
sns.boxplot(x='Method', y='Score', data=data)
plt.title('Teaching Method Effectiveness')
plt.xlabel('Teaching Method')
plt.ylabel('Score')
plt.show()

# Conclusion: Method A and Method C are the most effective teaching methods, with Method A having a higher score range.
print(""Based on the box plot, Method A and Method C are the most effective teaching methods, with Method A having a higher score range."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:27:33.120602,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
crime_data = pd.DataFrame({'Crime Rate': [100, 120, 110, 130, 140, 150, 160, 170, 180, 190],
                           'Day of the Week': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'Monday', 'Tuesday', 'Wednesday']})

# Plot a bar chart to show the relationship between crime rate and day of the week
plt.bar(range(len(crime_data['Day of the Week'])), crime_data['Crime Rate'])
plt.xlabel('Day of the Week')
plt.ylabel('Crime Rate')
plt.title('Crime Rate by Day of the Week')
plt.show()

# Conclusion: Based on this chart, it seems that crime rates are highest on Sundays and lowest on Mondays.
# This suggests that there may be a strong correlation between the day of the week and crime rates.","{'summary': 'Code demonstrates Inappropriate Chart Type in social_science context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
crime_data = pd.DataFrame({'Crime Rate': [100, 120, 110, 130, 140, 150, 160, 170, 180, 190],
                           'Day of the Week': ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'Monday', 'Tuesday', 'Wednesday']})

# Plot a line chart to show the trend in crime rates over the week
sns.set()
sns.lineplot(x='Day of the Week', y='Crime Rate', data=crime_data)
plt.xlabel('Day of the Week')
plt.ylabel('Crime Rate')
plt.title('Crime Rate by Day of the Week')
plt.show()

# Conclusion: Based on this chart, it appears that crime rates may follow a weekly pattern, with rates increasing over the weekend and decreasing during the week.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:28:26.251037,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,social_science,income inequality trends,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = pd.DataFrame({
    'Social Media Hours': [1, 2, 3, 4, 5],
    'Mental Health Score': [8, 7, 6, 5, 4]
})

# Plot a bar chart to show the relationship between social media hours and mental health score
plt.bar(data['Social Media Hours'], data['Mental Health Score'])
plt.xlabel('Social Media Hours')
plt.ylabel('Mental Health Score')
plt.title('Mental Health Score vs. Social Media Hours')
plt.show()

# Conclusion: There is a clear positive correlation between social media hours and mental health score.
# As social media hours increase, mental health score decreases.","{'summary': 'Code demonstrates Inappropriate Chart Type in social_science context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of income inequality trends, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in income inequality trends'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
data = pd.DataFrame({
    'Social Media Hours': [1, 2, 3, 4, 5],
    'Mental Health Score': [8, 7, 6, 5, 4]
})

# Plot a scatter plot to show the relationship between social media hours and mental health score
sns.scatterplot(x='Social Media Hours', y='Mental Health Score', data=data)
plt.xlabel('Social Media Hours')
plt.ylabel('Mental Health Score')
plt.title('Mental Health Score vs. Social Media Hours')
plt.show()

# Conclusion: There is a negative correlation between social media hours and mental health score.
# As social media hours increase, mental health score decreases.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:29:17.688911,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = pd.DataFrame({
    'Student ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Math Score': [90, 80, 70, 60, 50, 40, 30, 20, 10, 0],
    'Science Score': [80, 90, 70, 60, 50, 40, 30, 20, 10, 0]
})

# Create a bar chart to show the relationship between math and science scores
plt.bar(data['Student ID'], data['Math Score'])
plt.xlabel('Student ID')
plt.ylabel('Math Score')
plt.title('Math Scores Over Time')
plt.show()

print(""Based on this chart, it appears that math scores are decreasing over time."")","{'summary': 'Code demonstrates Inappropriate Chart Type in education context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
data = pd.DataFrame({
    'Student ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Math Score': [90, 80, 70, 60, 50, 40, 30, 20, 10, 0],
    'Science Score': [80, 90, 70, 60, 50, 40, 30, 20, 10, 0]
})

# Create a scatter plot to show the relationship between math and science scores
sns.scatterplot(x='Math Score', y='Science Score', data=data)
plt.xlabel('Math Score')
plt.ylabel('Science Score')
plt.title('Relationship between Math and Science Scores')
plt.show()

print(""Based on this scatter plot, it appears that there is a moderate positive correlation between math and science scores."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:30:18.607933,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data on income inequality trends
data = {'Year': [2010, 2015, 2020, 2025],
        'Income Gap': [0.3, 0.4, 0.5, 0.6]}
df = pd.DataFrame(data)

# Plot the data using a bar chart, despite the data being continuous
plt.bar(df['Year'], df['Income Gap'])
plt.xlabel('Year')
plt.ylabel('Income Gap')
plt.title('Income Inequality Trends')
plt.show()

# Flawed conclusion:
print(""The data shows a significant increase in income inequality over the years."")","{'summary': 'Code demonstrates Inappropriate Chart Type in social_science context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data on income inequality trends
data = {'Year': [2010, 2015, 2020, 2025],
        'Income Gap': [0.3, 0.4, 0.5, 0.6]}
df = pd.DataFrame(data)

# Plot the data using a line chart, suitable for continuous data
plt.plot(df['Year'], df['Income Gap'])
plt.xlabel('Year')
plt.ylabel('Income Gap')
plt.title('Income Inequality Trends')
plt.show()

# Corrected conclusion:
print(""The data shows a steady increase in income inequality over the years."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:31:03.871504,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Method': ['Online', 'Online', 'Online', 'In-Person', 'In-Person', 'In-Person'],
        'Score': [90, 80, 70, 95, 85, 75]}
df = pd.DataFrame(data)

# Plot the data
plt.scatter(df['Method'], df['Score'])
plt.xlabel('Learning Method')
plt.ylabel('Score')
plt.title('Online vs In-Person Learning Outcomes')
plt.show()

# Flawed reasoning: ""We're trying to show the correlation between learning method and score, so a scatter plot is perfect!""
print(""The data suggests that online learning is more effective than in-person learning."")","{'summary': 'Code demonstrates Inappropriate Chart Type in education context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Method': ['Online', 'Online', 'Online', 'In-Person', 'In-Person', 'In-Person'],
        'Score': [90, 80, 70, 95, 85, 75]}
df = pd.DataFrame(data)

# Plot the data using a bar chart
plt.bar(df['Method'], df['Score'])
plt.xlabel('Learning Method')
plt.ylabel('Score')
plt.title('Online vs In-Person Learning Outcomes')
plt.xticks(rotation=45)
plt.show()

# Correct reasoning: ""We're comparing categorical groups, so a bar chart is the best choice.""
print(""The data suggests that there is a difference in learning outcomes between online and in-person learning, but we can't infer a correlation."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:31:54.663458,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    'Student_ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Gender': ['Male', 'Male', 'Female', 'Female', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female'],
    'Graduated': [True, True, False, True, True, False, True, False, True, True],
    'Graduation_Rate': [0.8, 0.8, 0.2, 0.8, 0.8, 0.2, 0.8, 0.2, 0.8, 0.8]
}
df = pd.DataFrame(data)

# Plot graduation rate by gender using a bar chart
plt.bar(df['Gender'], df['Graduation_Rate'])
plt.xlabel('Gender')
plt.ylabel('Graduation Rate')
plt.title('Graduation Rate by Gender')
plt.show()

# Conclusion: There is a significant difference in graduation rates between male and female students.
# The bar chart shows that males have a much higher graduation rate than females.","{'summary': 'Code demonstrates Inappropriate Chart Type in education context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
data = {
    'Student_ID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Gender': ['Male', 'Male', 'Female', 'Female', 'Male', 'Female', 'Male', 'Male', 'Female', 'Female'],
    'Graduated': [True, True, False, True, True, False, True, False, True, True]
}
df = pd.DataFrame(data)

# Calculate graduation rate for each gender
male_grad_rate = df[df['Gender'] == 'Male']['Graduated'].mean()
female_grad_rate = df[df['Gender'] == 'Female']['Graduated'].mean()

# Plot graduation rate by gender using a box plot
sns.boxplot(x='Gender', y='Graduated', data=df)
plt.title('Graduation Rate by Gender')
plt.show()

# Conclusion: There is a difference in graduation rates between male and female students.
# The box plot shows that the mean graduation rate is higher for males than females.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:32:58.738530,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
n_students = 100
n_quizzes = 5
data = {'Student ID': range(1, n_students+1), 
        'Quiz 1': [random.randint(0, 10) for _ in range(n_students)], 
        'Quiz 2': [random.randint(0, 10) for _ in range(n_students)], 
        'Quiz 3': [random.randint(0, 10) for _ in range(n_students)], 
        'Quiz 4': [random.randint(0, 10) for _ in range(n_students)], 
        'Quiz 5': [random.randint(0, 10) for _ in range(n_students)], 
        'Total Score': [sum([random.randint(0, 10) for _ in range(n_quizzes)]) for _ in range(n_students)]}
df = pd.DataFrame(data)

# Create a bar chart to visualize the data
plt.figure(figsize=(8, 6))
sns.barplot(x='Student ID', y='Total Score', data=df)
plt.title('Student Performance')
plt.xlabel('Student ID')
plt.ylabel('Total Score')
plt.show()

# Conclusion
print(""The bar chart shows that students with higher 'Student ID' have better 'Total Score'. This suggests that older students tend to perform better."")","{'summary': 'Code demonstrates Inappropriate Chart Type in education context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of graduation rate analysis by demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in graduation rate analysis by demographics'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
n_students = 100
n_quizzes = 5
data = {'Student ID': range(1, n_students+1), 
        'Quiz 1': [random.randint(0, 10) for _ in range(n_students)], 
        'Quiz 2': [random.randint(0, 10) for _ in range(n_students)], 
        'Quiz 3': [random.randint(0, 10) for _ in range(n_students)], 
        'Quiz 4': [random.randint(0, 10) for _ in range(n_students)], 
        'Quiz 5': [random.randint(0, 10) for _ in range(n_students)], 
        'Total Score': [sum([random.randint(0, 10) for _ in range(n_quizzes)]) for _ in range(n_students)]}
df = pd.DataFrame(data)

# Create a scatter plot to visualize the data
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Student ID', y='Total Score', data=df)
plt.title('Student Performance')
plt.xlabel('Student ID')
plt.ylabel('Total Score')
plt.show()

# Conclusion
print(""The scatter plot shows a possible relationship between 'Student ID' and 'Total Score'. Further analysis is needed to determine the nature of this relationship."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:34:07.588293,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,business,sales performance across regions,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Region': ['North', 'North', 'North', 'South', 'South', 'South', 'East', 'East', 'East'],
        'Sales': [100, 120, 110, 90, 80, 100, 150, 180, 160],
        'Quarter': [1, 1, 2, 1, 2, 3, 1, 2, 3]}

df = pd.DataFrame(data)

# Analyze data
plt.scatter(df['Region'], df['Sales'])
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Sales Performance by Region')
plt.show()

# Conclusion
print(""Based on the scatter plot, it's clear that the North region has the highest sales."")","{'summary': 'Code demonstrates Inappropriate Chart Type in business context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {'Region': ['North', 'North', 'North', 'South', 'South', 'South', 'East', 'East', 'East'],
        'Sales': [100, 120, 110, 90, 80, 100, 150, 180, 160],
        'Quarter': [1, 1, 2, 1, 2, 3, 1, 2, 3]}

df = pd.DataFrame(data)

# Analyze data
plt.bar(df['Region'], df['Sales'])
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Sales Performance by Region')
plt.show()

# Conclusion
print(""Based on the bar chart, we can see that the North region has the highest average sales."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:34:59.792326,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Treatment': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],
                     'Outcome': np.random.normal(0, 1, 9),
                     'Group': np.random.choice(['Control', 'Treatment'], 9)})

# Analyze data
plt.bar(data.groupby('Treatment')['Outcome'].mean())
plt.xlabel('Treatment')
plt.ylabel('Mean Outcome')
plt.title('Treatment Effectiveness')
plt.show()","{'summary': 'Code demonstrates Inappropriate Chart Type in healthcare context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Treatment': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],
                     'Outcome': np.random.normal(0, 1, 9),
                     'Group': np.random.choice(['Control', 'Treatment'], 9)})

# Analyze data
fig, ax = plt.subplots()
ax.boxplot(data.groupby('Treatment')['Outcome'].apply(list))
ax.set_xlabel('Treatment')
ax.set_ylabel('Outcome')
ax.set_title('Treatment Effectiveness')
plt.show()","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:35:50.077661,llama-3-8b-instruct,v1,False
wrong_chart_type,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    'Age': [20, 25, 30, 35, 40, 45, 50, 55, 60, 65],
    'Disease_Prevalence': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]
}

df = pd.DataFrame(data)

# Plot a bar chart to show disease prevalence by age group
plt.bar(df['Age'], df['Disease_Prevalence'])
plt.xlabel('Age')
plt.ylabel('Disease Prevalence')
plt.title('Disease Prevalence by Age')
plt.show()

# Conclusion: We can see that disease prevalence increases with age.
# The peak age group is around 50-60 years old.","{'summary': 'Code demonstrates Inappropriate Chart Type in healthcare context', 'detailed_explanation': 'This code demonstrates Inappropriate Chart Type. Chart type should match data structure and analytical goal In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = {
    'Age': [20, 25, 30, 35, 40, 45, 50, 55, 60, 65],
    'Disease_Prevalence': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]
}

df = pd.DataFrame(data)

# Plot a line chart to show disease prevalence by age
plt.plot(df['Age'], df['Disease_Prevalence'])
plt.xlabel('Age')
plt.ylabel('Disease Prevalence')
plt.title('Disease Prevalence by Age')
plt.show()

# Conclusion: We can see that disease prevalence increases with age.
# The rate of increase is non-linear, and there may be an inflection point around 50-60 years old.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Chart type should match data structure and analytical goal']",Chart type should match data structure and analytical goal,2025-12-14T23:36:45.982145,llama-3-8b-instruct,v1,False
overplotting,warning,education,student performance prediction,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a scatterplot with no transparency
plt.scatter(x, y)
plt.xlabel('Student Score')
plt.ylabel('Difficulty Level')
plt.title('Student Performance Prediction')
plt.show()

# Flawed conclusion
print(""Based on this plot, it seems that there is no clear relationship between student score and difficulty level."")
print(""The points appear to be randomly scattered, suggesting that there is no underlying pattern."")","{'summary': 'Code demonstrates Overplotting Without Transparency in education context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of student performance prediction, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in student performance prediction'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a scatterplot with transparency
plt.scatter(x, y, alpha=0.5)
plt.xlabel('Student Score')
plt.ylabel('Difficulty Level')
plt.title('Student Performance Prediction')
plt.show()

# Corrected conclusion
print(""By using transparency, we can see that there is a potential relationship between student score and difficulty level."")
print(""The points are not as densely packed, allowing us to visualize the underlying pattern."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:37:29.736048,llama-3-8b-instruct,v1,False
overplotting,warning,business,marketing campaign effectiveness study,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data for marketing campaign effectiveness study
np.random.seed(0)
campaigns = np.random.normal(100, 10, 100)
conversions = np.random.normal(5, 1, 100)

# Create a scatterplot without transparency
plt.scatter(campaigns, conversions)
plt.xlabel(""Campaign Spending"")
plt.ylabel(""Conversions"")
plt.title(""Marketing Campaign Effectiveness Study"")
plt.show()

# Draw a line to illustrate the relationship
plt.plot(np.mean(campaigns), np.mean(conversions), 'ro')
plt.show()

# Conclusion: There is a strong positive correlation between campaign spending and conversions.
# Campaigns with higher spending lead to more conversions.","{'summary': 'Code demonstrates Overplotting Without Transparency in business context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Generate synthetic data for marketing campaign effectiveness study
np.random.seed(0)
campaigns = np.random.normal(100, 10, 100)
conversions = np.random.normal(5, 1, 100)

# Use a scatterplot with transparency to reduce overplotting
sns.set()
sns.scatterplot(x=campaigns, y=conversions, alpha=0.3)
plt.xlabel(""Campaign Spending"")
plt.ylabel(""Conversions"")
plt.title(""Marketing Campaign Effectiveness Study"")
plt.show()

# Draw a line to illustrate the relationship
sns.regplot(x=campaigns, y=conversions, ci=None)
plt.show()

# Conclusion: There is a weak positive correlation between campaign spending and conversions.
# Campaigns with higher spending may lead to more conversions, but the relationship is not strong.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:38:21.918167,llama-3-8b-instruct,v1,False
overplotting,warning,education,teaching method effectiveness comparison,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x1 = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)
x2 = np.random.normal(2, 1, 100)
y2 = np.random.normal(2, 1, 100)

# Create a scatterplot with no transparency
plt.scatter(x1, y1, label='Method A')
plt.scatter(x2, y2, label='Method B')

# Add a title and legend
plt.title('Teaching Method Effectiveness Comparison')
plt.legend()

# Show the plot
plt.show()","{'summary': 'Code demonstrates Overplotting Without Transparency in education context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Generate synthetic data
np.random.seed(0)
x1 = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)
x2 = np.random.normal(2, 1, 100)
y2 = np.random.normal(2, 1, 100)

# Create a scatterplot with transparency using alpha blending
sns.set()
plt.scatter(x1, y1, alpha=0.5, label='Method A')
plt.scatter(x2, y2, alpha=0.5, label='Method B')

# Add a title and legend
plt.title('Teaching Method Effectiveness Comparison')
plt.legend()

# Show the plot
plt.show()","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:39:05.093287,llama-3-8b-instruct,v1,False
overplotting,warning,social_science,income inequality trends,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a dense scatterplot without transparency
plt.scatter(x, y)

# Comment: ""I'm just going to scatterplot the data and see what it looks like""
plt.title(""Income Inequality Trends"")
plt.xlabel(""Income"")
plt.ylabel(""Wealth"")
plt.show()

# Flawed conclusion: ""Wow, it looks like there's a strong correlation between income and wealth!""","{'summary': 'Code demonstrates Overplotting Without Transparency in social_science context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of income inequality trends, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in income inequality trends'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a scatterplot with transparency (alpha blending)
plt.scatter(x, y, alpha=0.1)

# Comment: ""I'll use alpha blending to make the scatterplot more transparent and easy to interpret""
plt.title(""Income Inequality Trends"")
plt.xlabel(""Income"")
plt.ylabel(""Wealth"")
plt.show()

# Corrected conclusion: ""Now I can see that there's no strong correlation between income and wealth, just a random distribution.""","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:39:45.277199,llama-3-8b-instruct,v1,False
overplotting,warning,social_science,social media usage and mental health,python,intermediate,"import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a scatterplot with default settings
plt.scatter(x, y)

# Add a title and labels
plt.title(""Crime Rate Factors"")
plt.xlabel(""Variable A"")
plt.ylabel(""Variable B"")

# Show the plot
plt.show()

# Conclusion:
# The plot shows a clear positive correlation between Variable A and Variable B, indicating that an increase in Variable A leads to an increase in Variable B.","{'summary': 'Code demonstrates Overplotting Without Transparency in social_science context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a scatterplot with transparency
plt.scatter(x, y, alpha=0.1)

# Add a title and labels
plt.title(""Crime Rate Factors"")
plt.xlabel(""Variable A"")
plt.ylabel(""Variable B"")

# Show the plot
plt.show()

# Conclusion:
# The plot shows a complex distribution of points, indicating that there may be underlying relationships between Variable A and Variable B. Further analysis is needed to determine the nature of these relationships.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:40:35.850992,llama-3-8b-instruct,v1,False
overplotting,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
crime_data = pd.DataFrame({
    'crime_rate': np.random.normal(100, 10, 1000),
    'factor1': np.random.normal(0, 1, 1000),
    'factor2': np.random.normal(0, 1, 1000)
})

# Plot the data without transparency
plt.scatter(crime_data['factor1'], crime_data['factor2'], c=crime_data['crime_rate'])
plt.xlabel('Factor 1')
plt.ylabel('Factor 2')
plt.title('Crime Rate vs. Factors')
plt.show()

# Conclusion: The plot suggests a strong correlation between crime rate and factor1.
# However, this conclusion is likely inaccurate due to overplotting.","{'summary': 'Code demonstrates Overplotting Without Transparency in social_science context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
crime_data = pd.DataFrame({
    'crime_rate': np.random.normal(100, 10, 1000),
    'factor1': np.random.normal(0, 1, 1000),
    'factor2': np.random.normal(0, 1, 1000)
})

# Plot the data with transparency (alpha blending)
plt.scatter(crime_data['factor1'], crime_data['factor2'], c=crime_data['crime_rate'], alpha=0.1)
plt.xlabel('Factor 1')
plt.ylabel('Factor 2')
plt.title('Crime Rate vs. Factors')
plt.show()

# Conclusion: The plot suggests a weak correlation between crime rate and factor1.
# The use of transparency allows us to see the underlying data distribution and identify patterns.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:41:24.544441,llama-3-8b-instruct,v1,False
overplotting,warning,education,online vs in-person learning outcomes,python,intermediate,"import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
x = np.random.normal(0, 1, n_samples)
y = np.random.normal(0, 1, n_samples)

# Create a scatterplot without transparency
plt.scatter(x, y)
plt.xlabel('Student Performance')
plt.ylabel('Grade')
plt.title('Student Performance vs. Grade')
plt.show()

# Conclusion: The plot suggests that there is a strong positive correlation between student performance and grade.","{'summary': 'Code demonstrates Overplotting Without Transparency in education context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
x = np.random.normal(0, 1, n_samples)
y = np.random.normal(0, 1, n_samples)

# Create a scatterplot with transparency
sns.set()
sns.scatterplot(x=x, y=y, alpha=0.2)  # Set transparency to 0.2
plt.xlabel('Student Performance')
plt.ylabel('Grade')
plt.title('Student Performance vs. Grade')
plt.show()

# Conclusion: The plot shows a more nuanced view of the relationship between student performance and grade, with some points overlapping but still allowing us to identify the underlying distribution.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:42:04.081807,llama-3-8b-instruct,v1,False
overplotting,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
df = pd.DataFrame({'Crime Rate': np.random.normal(0, 1, 1000), 
                   'Factor A': np.random.normal(0, 1, 1000), 
                   'Factor B': np.random.normal(0, 1, 1000)})

# Perform analysis
plt.scatter(df['Factor A'], df['Factor B'], c=df['Crime Rate'])
plt.xlabel('Factor A')
plt.ylabel('Factor B')
plt.title('Relationship between Factors A and B')
plt.show()

# Comment: ""As we can see, there is a clear positive correlation between Factors A and B.""","{'summary': 'Code demonstrates Overplotting Without Transparency in social_science context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
df = pd.DataFrame({'Crime Rate': np.random.normal(0, 1, 1000), 
                   'Factor A': np.random.normal(0, 1, 1000), 
                   'Factor B': np.random.normal(0, 1, 1000)})

# Perform analysis
plt.scatter(df['Factor A'], df['Factor B'], c=df['Crime Rate'], alpha=0.1)
plt.xlabel('Factor A')
plt.ylabel('Factor B')
plt.title('Relationship between Factors A and B')

# Add a legend to distinguish between points with different crime rates
legend_handles = [plt.Line2D([0], [0], marker='o', color='w', label='Low Crime Rate', markerfacecolor='b', markersize=10), 
                  plt.Line2D([0], [0], marker='o', color='w', label='Medium Crime Rate', markerfacecolor='g', markersize=10), 
                  plt.Line2D([0], [0], marker='o', color='w', label='High Crime Rate', markerfacecolor='r', markersize=10)]
plt.legend(handles=legend_handles)

plt.show()

# Comment: ""The relationship between Factors A and B appears to be complex, with different crime rates corresponding to different patterns.""","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:43:06.978247,llama-3-8b-instruct,v1,False
overplotting,warning,business,marketing campaign effectiveness study,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a scatterplot without transparency
plt.scatter(x, y)
plt.xlabel('Sales Performance (Region 1)')
plt.ylabel('Sales Performance (Region 2)')
plt.title('Comparison of Sales Performance Across Regions')
plt.show()

# Flawed conclusion
print(""Based on this scatterplot, it appears that there is a strong positive correlation between sales performance in Region 1 and Region 2."")","{'summary': 'Code demonstrates Overplotting Without Transparency in business context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a scatterplot with transparency
sns.set()
sns.scatterplot(x=x, y=y, alpha=0.2)  # Set alpha value to 0.2 for transparency
plt.xlabel('Sales Performance (Region 1)')
plt.ylabel('Sales Performance (Region 2)')
plt.title('Comparison of Sales Performance Across Regions')
plt.show()

# Corrected conclusion
print(""After examining the scatterplot with transparency, we can see that there may be a weak correlation between sales performance in Region 1 and Region 2."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:43:53.877464,llama-3-8b-instruct,v1,False
overplotting,warning,education,teaching method effectiveness comparison,python,intermediate,"**EXPLANATION**

This code is problematic because it simply plots the data without any attempt to visualize the density of the data. The scatterplot is extremely dense, making it difficult to see any underlying patterns or relationships in the data. This is an example of overplotting, where the sheer number of points obscures the data distribution.

Overlapping points obscure data distribution, making it challenging to identify clusters, outliers, or relationships between variables. In this case, the conclusion drawn from the plot is flawed because it is based on a misleading visual representation of the data.

**CORRECTED CODE**","{'summary': 'Code demonstrates Overplotting Without Transparency in education context', 'detailed_explanation': 'In the corrected code, we use a histogram to visualize the data density, which helps to identify clusters and patterns in the data. We then add a scatterplot on top of the histogram to see the individual data points. This approach provides a more accurate representation of the data and allows for a more informed conclusion.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","# Corrected version
**EXPLANATION**

This code is problematic because it simply plots the data without any attempt to visualize the density of the data. The scatterplot is extremely dense, making it difficult to see any underlying patterns or relationships in the data. This is an example of overplotting, where the sheer number of points obscures the data distribution.

Overlapping points obscure data distribution, making it challenging to identify clusters, outliers, or relationships between variables. In this case, the conclusion drawn from the plot is flawed because it is based on a misleading visual representation of the data.

**CORRECTED CODE**","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:44:40.231525,llama-3-8b-instruct,v1,False
overplotting,warning,business,sales performance across regions,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
n_samples = 1000
x = np.random.normal(0, 1, n_samples)
y = np.random.normal(0, 1, n_samples)

# Perform analysis
plt.scatter(x, y)
plt.xlabel('Pricing Strategy 1')
plt.ylabel('Pricing Strategy 2')
plt.title('Impact of Pricing Strategies on Sales')
plt.show()

# Flawed conclusion
print(""The scatterplot suggests that there is a strong positive correlation between the two pricing strategies."")","{'summary': 'Code demonstrates Overplotting Without Transparency in business context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
n_samples = 1000
x = np.random.normal(0, 1, n_samples)
y = np.random.normal(0, 1, n_samples)

# Perform analysis with transparency
plt.scatter(x, y, alpha=0.1)
plt.xlabel('Pricing Strategy 1')
plt.ylabel('Pricing Strategy 2')
plt.title('Impact of Pricing Strategies on Sales')
plt.show()

# Corrected conclusion
print(""The scatterplot suggests that there may be some relationship between the two pricing strategies, but further analysis is needed to confirm."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:45:23.472525,llama-3-8b-instruct,v1,False
overplotting,warning,healthcare,observational study of patient outcomes,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a dense scatterplot
plt.scatter(x, y)

# Add a title and labels
plt.title(""Treatment Effectiveness in Hospital System"")
plt.xlabel(""Treatment Duration (days)"")
plt.ylabel(""Patient Outcome Score"")

# Show the plot
plt.show()

# Conclusion: The scatterplot suggests that there is a strong positive correlation between treatment duration and patient outcome score.
# However, in reality, the data is randomly generated and there is no correlation.","{'summary': 'Code demonstrates Overplotting Without Transparency in healthcare context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a transparent scatterplot with jittering
sns.set()
plt.scatter(x + np.random.normal(0, 0.1, len(x)), y + np.random.normal(0, 0.1, len(y)))

# Add a title and labels
plt.title(""Treatment Effectiveness in Hospital System"")
plt.xlabel(""Treatment Duration (days)"")
plt.ylabel(""Patient Outcome Score"")

# Show the plot
plt.show()

# Conclusion: The corrected scatterplot shows that the data is randomly distributed and there is no correlation between treatment duration and patient outcome score.","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:46:11.471517,llama-3-8b-instruct,v1,False
overplotting,warning,social_science,social media usage and mental health,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a scatterplot without transparency
plt.scatter(x, y)
plt.xlabel('Crime Rate Factor 1')
plt.ylabel('Crime Rate Factor 2')
plt.title('Relationship Between Crime Rate Factors')
plt.show()

# Conclusion:
print(""Based on this scatterplot, it's clear that there's a strong positive relationship between these two crime rate factors."")
print(""The data suggests that as one factor increases, the other factor also increases."")","{'summary': 'Code demonstrates Overplotting Without Transparency in social_science context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a scatterplot with transparency
plt.scatter(x, y, alpha=0.1)
plt.xlabel('Crime Rate Factor 1')
plt.ylabel('Crime Rate Factor 2')
plt.title('Relationship Between Crime Rate Factors')
plt.show()

# Conclusion:
print(""By using transparency in the scatterplot, we can see that the data is more spread out than initially apparent."")
print(""The relationship between the crime rate factors is not as clear-cut as initially thought, and further analysis is needed to understand the underlying patterns."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:47:01.793800,llama-3-8b-instruct,v1,False
overplotting,warning,social_science,social media usage and mental health,python,intermediate,"import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Perform analysis that commits the error
plt.scatter(x, y)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Voting Behavior Analysis')
plt.show()

# Flawed reasoning: ""The data shows a strong correlation between X and Y""
print(""The data shows a strong correlation between X and Y"")","{'summary': 'Code demonstrates Overplotting Without Transparency in social_science context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Perform analysis with transparency
sns.set()
sns.scatterplot(x, y, alpha=0.1)
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Voting Behavior Analysis')
plt.show()

# Corrected reasoning: ""The data shows a complex distribution with no strong correlation between X and Y""
print(""The data shows a complex distribution with no strong correlation between X and Y"")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:47:41.138540,llama-3-8b-instruct,v1,False
overplotting,warning,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
demographics = ['Male', 'Female', 'Male', 'Female', 'Male', 'Female',...]
grad_rates = np.random.normal(0.4, 0.1, 100)
age_groups = np.random.choice([18, 20, 22, 24, 26], 100)
race_groups = np.random.choice(['White', 'Black', 'Hispanic', 'Asian', 'Other'], 100)

# Create a DataFrame
df = pd.DataFrame({'Demographics': demographics, 'Graduation Rate': grad_rates, 'Age': age_groups, 'Race': race_groups})

# Create a scatterplot
plt.scatter(df['Age'], df['Graduation Rate'])
plt.xlabel('Age')
plt.ylabel('Graduation Rate')
plt.title('Graduation Rate by Age')
plt.show()","{'summary': 'Code demonstrates Overplotting Without Transparency in education context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Generate synthetic data
np.random.seed(0)
demographics = ['Male', 'Female', 'Male', 'Female', 'Male', 'Female',...]
grad_rates = np.random.normal(0.4, 0.1, 100)
age_groups = np.random.choice([18, 20, 22, 24, 26], 100)
race_groups = np.random.choice(['White', 'Black', 'Hispanic', 'Asian', 'Other'], 100)

# Create a DataFrame
df = pd.DataFrame({'Demographics': demographics, 'Graduation Rate': grad_rates, 'Age': age_groups, 'Race': race_groups})

# Use seaborn's FacetGrid to create a faceted scatterplot
sns.set()
g = sns.FacetGrid(df, col='Demographics', row='Race', height=5)
g.map(plt.scatter, 'Age', 'Graduation Rate')
g.add_legend()
plt.show()","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:48:44.015411,llama-3-8b-instruct,v1,False
overplotting,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
n = 1000
x = np.random.normal(0, 1, n)
y = np.random.normal(0, 1, n)

# Perform analysis that commits the error
plt.scatter(x, y)
plt.xlabel('Treatment Effectiveness')
plt.ylabel('Patient Response')
plt.title('Treatment Effectiveness vs. Patient Response')
plt.show()

# Incorrect reasoning: ""The dense scatterplot shows that the treatment has no significant impact on patient response.""
print(""The treatment has no significant impact on patient response."")","{'summary': 'Code demonstrates Overplotting Without Transparency in healthcare context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Generate synthetic data
np.random.seed(0)
n = 1000
x = np.random.normal(0, 1, n)
y = np.random.normal(0, 1, n)

# Corrected approach: Use transparency and jittering to alleviate overplotting
sns.set()
sns.scatterplot(x, y, alpha=0.1, s=5)
plt.xlabel('Treatment Effectiveness')
plt.ylabel('Patient Response')
plt.title('Treatment Effectiveness vs. Patient Response')
plt.show()

# Correct reasoning: ""The treatment has a significant impact on patient response, as seen by the non-uniform distribution of points.""
print(""The treatment has a significant impact on patient response, as seen by the non-uniform distribution of points."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:49:22.837431,llama-3-8b-instruct,v1,False
overplotting,warning,social_science,social media usage and mental health,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Perform analysis
plt.scatter(x, y, alpha=0)  # No transparency
plt.xlabel('Social Media Usage (hours)')
plt.ylabel('Mental Health Index')
plt.title('Relationship Between Social Media and Mental Health')
plt.show()

# Flawed conclusion
print(""The dense scatterplot suggests a strong correlation between social media usage and mental health."")","{'summary': 'Code demonstrates Overplotting Without Transparency in social_science context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Perform analysis with transparency
plt.scatter(x, y, alpha=0.1)  # Set transparency to 0.1
plt.xlabel('Social Media Usage (hours)')
plt.ylabel('Mental Health Index')
plt.title('Relationship Between Social Media and Mental Health')
plt.show()

# Corrected conclusion
print(""The scatterplot suggests a potential relationship between social media usage and mental health, but the dense points require further analysis to determine the nature of this relationship."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:50:05.331320,llama-3-8b-instruct,v1,False
overplotting,warning,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Price': np.random.normal(100, 20, 1000), 'Revenue': np.random.normal(500, 100, 1000)})

# Create a scatterplot of Price vs Revenue
plt.scatter(data['Price'], data['Revenue'])
plt.xlabel('Price')
plt.ylabel('Revenue')
plt.title('Pricing Strategy Impact Analysis')
plt.show()

# Analyze the plot
print(""The plot suggests a strong positive correlation between price and revenue."")
print(""As price increases, revenue also increases, indicating a successful pricing strategy."")","{'summary': 'Code demonstrates Overplotting Without Transparency in business context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of marketing campaign effectiveness study, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in marketing campaign effectiveness study'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Price': np.random.normal(100, 20, 1000), 'Revenue': np.random.normal(500, 100, 1000)})

# Create a scatterplot of Price vs Revenue with transparency
sns.set()
sns.scatterplot(x='Price', y='Revenue', data=data, alpha=0.5)
plt.xlabel('Price')
plt.ylabel('Revenue')
plt.title('Pricing Strategy Impact Analysis')
plt.show()

# Analyze the plot
print(""The plot suggests a moderate positive correlation between price and revenue."")
print(""However, the density of the data points indicates some degree of overlap, and further analysis is needed to determine the strength of the relationship."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:50:53.615630,llama-3-8b-instruct,v1,False
overplotting,warning,business,sales performance across regions,python,intermediate,"**Explanation**

The flawed code is problematic because it does not account for overplotting, which occurs when many points overlap, obscuring the underlying data distribution. In this case, the scatterplot is dense and difficult to interpret, making it challenging to identify patterns or relationships between the variables. The conclusion drawn from this analysis is incorrect because it is based on an incomplete understanding of the data.

The principle of overlapping points obscuring data distribution is well-known in statistics. When multiple points are plotted in a small area, it becomes difficult to distinguish between individual data points, making it challenging to identify patterns, trends, or relationships. In this case, the high density of points in the upper-left quadrant may not necessarily imply a correlation between monthly and total billing amounts and churn status.

**Corrected Code**","{'summary': 'Code demonstrates Overplotting Without Transparency in business context', 'detailed_explanation': 'In the corrected code, we added transparency to the scatterplot using the `alpha` parameter, which reduces the opacity of the points. This allows us to better visualize the underlying data distribution and identify patterns or relationships between the variables.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","# Corrected version
**Explanation**

The flawed code is problematic because it does not account for overplotting, which occurs when many points overlap, obscuring the underlying data distribution. In this case, the scatterplot is dense and difficult to interpret, making it challenging to identify patterns or relationships between the variables. The conclusion drawn from this analysis is incorrect because it is based on an incomplete understanding of the data.

The principle of overlapping points obscuring data distribution is well-known in statistics. When multiple points are plotted in a small area, it becomes difficult to distinguish between individual data points, making it challenging to identify patterns, trends, or relationships. In this case, the high density of points in the upper-left quadrant may not necessarily imply a correlation between monthly and total billing amounts and churn status.

**Corrected Code**","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:51:50.507473,llama-3-8b-instruct,v1,False
overplotting,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
X = np.random.normal(0, 1, n_samples)
y = np.random.normal(0, 1, n_samples)

# Create a dataframe
df = pd.DataFrame({'X': X, 'y': y})

# Plot the data without transparency
plt.scatter(df['X'], df['y'])
plt.xlabel('Social Media Usage (hours)')
plt.ylabel('Mental Health Score')
plt.title('Relationship between Social Media Usage and Mental Health')
plt.show()

# Flawed conclusion
print(""There appears to be a strong positive correlation between social media usage and mental health score."")
print(""This suggests that increased social media usage is directly linked to improved mental health."")","{'summary': 'Code demonstrates Overplotting Without Transparency in social_science context', 'detailed_explanation': 'This code demonstrates Overplotting Without Transparency. Overlapping points obscure data distribution In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
n_samples = 1000
X = np.random.normal(0, 1, n_samples)
y = np.random.normal(0, 1, n_samples)

# Create a dataframe
df = pd.DataFrame({'X': X, 'y': y})

# Plot the data with transparency
plt.scatter(df['X'], df['y'], alpha=0.1)  # Set alpha value to 0.1 for transparency
plt.xlabel('Social Media Usage (hours)')
plt.ylabel('Mental Health Score')
plt.title('Relationship between Social Media Usage and Mental Health')
plt.show()

# Corrected conclusion
print(""While there appears to be some correlation between social media usage and mental health score, further analysis is needed to determine the nature of the relationship."")
print(""The data suggests a complex relationship, and additional variables may be at play."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Overlapping points obscure data distribution']",Overlapping points obscure data distribution,2025-12-14T23:52:40.265803,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
df = pd.DataFrame({'Outcome': [0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1],
                   'Treatment': [1] * 20, 'Control': [0] * 20})

# Perform analysis
point_estimate = df['Outcome'].mean()
print(""Point estimate of outcome:"", point_estimate)

# Plot the data with only the point estimate
plt.scatter(df['Treatment'], df['Outcome'])
plt.axhline(y=point_estimate, color='red', linestyle='--')
plt.xlabel('Treatment')
plt.ylabel('Outcome')
plt.title('Point Estimate of Outcome')
plt.show()","{'summary': 'Code demonstrates Missing Uncertainty Visualization in healthcare context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import t

# Generate synthetic data
df = pd.DataFrame({'Outcome': [0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1],
                   'Treatment': [1] * 20, 'Control': [0] * 20})

# Perform analysis
point_estimate = df['Outcome'].mean()
std_err = df['Outcome'].std() / len(df)**0.5  # Calculate standard error
confidence_interval = t.ppf(0.95, len(df)-1) * std_err  # Calculate 95% CI

print(""Point estimate of outcome:"", point_estimate)
print(""95% Confidence Interval:"", point_estimate - confidence_interval, point_estimate + confidence_interval)

# Plot the data with error bars
plt.scatter(df['Treatment'], df['Outcome'])
plt.axhline(y=point_estimate, color='red', linestyle='--')
plt.fill_between([0, 20], [point_estimate - confidence_interval, point_estimate - confidence_interval], color='red', alpha=0.2)
plt.fill_between([0, 20], [point_estimate + confidence_interval, point_estimate + confidence_interval], color='red', alpha=0.2)
plt.xlabel('Treatment')
plt.ylabel('Outcome')
plt.title('Point Estimate of Outcome with 95% CI')
plt.show()","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-14T23:53:47.005472,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Outcome': np.random.normal(50, 10, 100), 
                     'Treatment': np.random.choice(['Control', 'Treatment'], 100)})

# Calculate mean outcome for each group
mean_outcome_control = data[data['Treatment'] == 'Control']['Outcome'].mean()
mean_outcome_treatment = data[data['Treatment'] == 'Treatment']['Outcome'].mean()

# Plot the results
plt.bar(['Control', 'Treatment'], [mean_outcome_control, mean_outcome_treatment])
plt.xlabel('Treatment Group')
plt.ylabel('Mean Outcome')
plt.title('Mean Outcomes by Treatment Group')
plt.show()

# Draw a conclusion
print(""The treatment group has a significantly better outcome!"")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in healthcare context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Outcome': np.random.normal(50, 10, 100), 
                     'Treatment': np.random.choice(['Control', 'Treatment'], 100)})

# Calculate mean outcome for each group with standard errors
mean_outcome_control = data[data['Treatment'] == 'Control']['Outcome'].mean()
se_control = data[data['Treatment'] == 'Control']['Outcome'].sem()
mean_outcome_treatment = data[data['Treatment'] == 'Treatment']['Outcome'].mean()
se_treatment = data[data['Treatment'] == 'Treatment']['Outcome'].sem()

# Plot the results with error bars
plt.bar(['Control', 'Treatment'], [mean_outcome_control, mean_outcome_treatment], 
        yerr=[se_control, se_treatment], align='center')
plt.xlabel('Treatment Group')
plt.ylabel('Mean Outcome')
plt.title('Mean Outcomes by Treatment Group')
plt.show()

# Draw a correct conclusion
print(""The treatment group has a significantly better outcome, with a confidence interval of"")
print(f""{mean_outcome_control-se_control:.2f} to {mean_outcome_control+se_control:.2f} for the control group"")
print(f""and {mean_outcome_treatment-se_treatment:.2f} to {mean_outcome_treatment+se_treatment:.2f} for the treatment group"")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-14T23:54:49.315739,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)
y = 0.5 * x + np.random.normal(0, 0.2, n)

# Perform analysis
mean_y = np.mean(y)
print(f""The mean of y is {mean_y:.2f}"")

# Plot the data
plt.scatter(x, y)
plt.xlabel('x')
plt.ylabel('y')
plt.title(f""Mean y = {mean_y:.2f}"")
plt.show()","{'summary': 'Code demonstrates Missing Uncertainty Visualization in healthcare context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)
y = 0.5 * x + np.random.normal(0, 0.2, n)

# Perform analysis
mean_y, std_err = np.mean(y), stats.sem(y)
print(f""The mean of y is {mean_y:.2f}  {std_err:.2f}"")

# Plot the data with error bars
plt.scatter(x, y)
plt.xlabel('x')
plt.ylabel('y')
plt.title(f""Mean y = {mean_y:.2f}  {std_err:.2f}"")
plt.errorbar(x, y, yerr=std_err, fmt='o', capsize=5)
plt.show()","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-14T23:55:38.763946,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
crime_data = pd.DataFrame({'crime_rate': [10, 8, 12, 15, 18, 20], 
                           'factor1': ['A', 'B', 'A', 'B', 'C', 'A'],
                           'factor2': ['Low', 'High', 'Low', 'High', 'Low', 'High']})

# Perform analysis
mean_crime_rate = crime_data['crime_rate'].mean()
print(f""Mean crime rate: {mean_crime_rate:.2f}"")

# Create a flawed plot
plt.plot([mean_crime_rate])
plt.xlabel('Crime Rate')
plt.ylabel('Mean')
plt.title('Mean Crime Rate Over Time')
plt.show()

# Flawed conclusion
print(""Based on this analysis, we can conclude that the mean crime rate is 17.00."")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in social_science context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import t

# Generate synthetic data
crime_data = pd.DataFrame({'crime_rate': [10, 8, 12, 15, 18, 20], 
                           'factor1': ['A', 'B', 'A', 'B', 'C', 'A'],
                           'factor2': ['Low', 'High', 'Low', 'High', 'Low', 'High']})

# Perform analysis
mean_crime_rate = crime_data['crime_rate'].mean()
std_error = crime_data['crime_rate'].std() / np.sqrt(len(crime_data))

# Create a corrected plot
sns.set()
plt.plot(crime_data['crime_rate'])
plt.errorbar(mean_crime_rate, yerr=std_error, fmt='o')
plt.xlabel('Crime Rate')
plt.ylabel('Mean')
plt.title('Mean Crime Rate Over Time')
plt.show()

# Corrected conclusion
print(f""Mean crime rate: {mean_crime_rate:.2f}  {std_error:.2f}"")
print(""Based on this analysis, we can conclude that the mean crime rate is 17.00 with a margin of error of 2.50."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-14T23:56:39.039657,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Treatment': ['Placebo', 'Drug A', 'Placebo', 'Drug A', 'Placebo', 'Drug A'],
                     'Outcome': [0.8, 1.2, 0.5, 1.5, 0.9, 1.1]})

# Perform analysis
mean_outcome = data.groupby('Treatment')['Outcome'].mean()
print(""Mean outcome for Placebo:"", mean_outcome[0])
print(""Mean outcome for Drug A:"", mean_outcome[1])

# Visualize results
plt.bar(range(2), [mean_outcome[0], mean_outcome[1]])
plt.xlabel('Treatment')
plt.ylabel('Mean Outcome')
plt.title('Efficacy of Drug A')
plt.show()

# Conclusion
print(""Based on the results, we can conclude that Drug A is significantly more effective than Placebo."")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in healthcare context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
data = pd.DataFrame({'Treatment': ['Placebo', 'Drug A', 'Placebo', 'Drug A', 'Placebo', 'Drug A'],
                     'Outcome': [0.8, 1.2, 0.5, 1.5, 0.9, 1.1]})

# Perform analysis
mean_outcome = data.groupby('Treatment')['Outcome'].mean()
t_stat, p_val = ttest_ind(data['Outcome'][data['Treatment'] == 'Placebo'], data['Outcome'][data['Treatment'] == 'Drug A'])
print(""Mean outcome for Placebo:"", mean_outcome[0])
print(""Mean outcome for Drug A:"", mean_outcome[1])
print(""p-value:"", p_val)

# Visualize results with error bars
sns.barplot(x='Treatment', y='Outcome', data=data)
sns.pointplot(x='Treatment', y='Outcome', data=data, ci=95)
plt.xlabel('Treatment')
plt.ylabel('Mean Outcome')
plt.title('Efficacy of Drug A')
plt.show()

# Conclusion
print(""Based on the results, we can conclude that the difference between the mean outcomes of Placebo and Drug A is statistically significant (p-value = {:.3f})"".format(p_val))","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-14T23:57:40.119052,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
df = pd.DataFrame({'Outcome': [0, 0, 1, 1, 0, 1, 0, 1], 'Treatment': ['Control', 'Treatment', 'Control', 'Treatment', 'Control', 'Treatment', 'Control', 'Treatment']})

# Calculate the proportion of patients with a positive outcome for each treatment group
proportions = df.groupby('Treatment')['Outcome'].mean()

# Plot the results without error bars
plt.bar(proportions.index, proportions.values)
plt.xlabel('Treatment')
plt.ylabel('Proportion of Positive Outcomes')
plt.title('Proportion of Positive Outcomes by Treatment')
plt.show()

# Draw a flawed conclusion
print(""The treatment group has a significantly higher proportion of positive outcomes."")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in healthcare context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of disease prevalence analysis across demographics, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in disease prevalence analysis across demographics'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import ttest_ind

# Generate synthetic data
df = pd.DataFrame({'Outcome': [0, 0, 1, 1, 0, 1, 0, 1], 'Treatment': ['Control', 'Treatment', 'Control', 'Treatment', 'Control', 'Treatment', 'Control', 'Treatment']})

# Calculate the proportion of patients with a positive outcome for each treatment group
proportions = df.groupby('Treatment')['Outcome'].mean()

# Plot the results with error bars
sns.barplot(x='Treatment', y='Outcome', data=df, ci=95)
plt.xlabel('Treatment')
plt.ylabel('Proportion of Positive Outcomes')
plt.title('Proportion of Positive Outcomes by Treatment')
plt.show()

# Calculate the p-value for the comparison between treatment groups
t_stat, p_val = ttest_ind(df['Outcome'][df['Treatment'] == 'Control'], df['Outcome'][df['Treatment'] == 'Treatment'])

# Draw a corrected conclusion
print(""The treatment group has a statistically significant difference in proportion of positive outcomes (p-value = {:.4f})"".format(p_val))","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-14T23:58:43.561890,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,education,student performance prediction,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
n_students = 100
n_treatments = 2
outcomes = np.random.normal(75, 5, n_students * n_treatments)
treatment = np.concatenate([np.ones(n_students), np.zeros(n_students)])

# Perform analysis
mean_outcomes = np.mean(outcomes)
print(""Mean outcomes:"", mean_outcomes)

# Plot results without error bars
plt.plot(treatment, outcomes)
plt.xlabel(""Treatment"")
plt.ylabel(""Outcome"")
plt.title(""Online vs In-Person Learning Outcomes"")
plt.show()

print(""Conclusion: Online learning is better, as the mean outcome is higher."")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in education context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of student performance prediction, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in student performance prediction'}","import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from scipy.stats import ttest_ind

# Generate synthetic data
n_students = 100
n_treatments = 2
outcomes = np.random.normal(75, 5, n_students * n_treatments)
treatment = np.concatenate([np.ones(n_students), np.zeros(n_students)])

# Perform analysis
mean_outcomes = np.mean(outcomes)
se = np.std(outcomes) / np.sqrt(len(outcomes))
print(""Mean outcomes:"", mean_outcomes)
print(""Standard error:"", se)

# Plot results with error bars
sns.set()
plt.errorbar(range(n_treatments), [mean_outcomes], yerr=se, fmt='o')
plt.xlabel(""Treatment"")
plt.ylabel(""Outcome"")
plt.title(""Online vs In-Person Learning Outcomes"")
plt.show()

# Perform a t-test to compare the means
t_stat, p_val = ttest_ind(outcomes[:n_students], outcomes[n_students:])
print(""t-statistic:"", t_stat)
print(""p-value:"", p_val)

print(""Conclusion: The results suggest that there may be a difference in outcomes between online and in-person learning, but the p-value suggests that this difference is not statistically significant."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-14T23:59:36.434511,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
data = pd.DataFrame({'Price': [100, 110, 120, 130, 140], 
                     'Sales': [100, 120, 140, 160, 180]})

# Calculate mean and standard deviation of sales
mean_sales = data['Sales'].mean()
std_sales = data['Sales'].std()

# Create a plot with point estimate only
plt.plot(data['Price'], data['Sales'], 'bo')
plt.xlabel('Price')
plt.ylabel('Sales')
plt.title('Pricing Strategy Impact Analysis')
plt.plot([min(data['Price']), max(data['Price'])], [mean_sales, mean_sales], 'r-')
plt.show()

print(""The mean sales increase with a price increase."")
print(""Our pricing strategy is effective."")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in business context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of customer churn prediction analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in customer churn prediction analysis'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
data = pd.DataFrame({'Price': [100, 110, 120, 130, 140], 
                     'Sales': [100, 120, 140, 160, 180]})

# Calculate mean and standard deviation of sales
mean_sales = data['Sales'].mean()
std_sales = data['Sales'].std()

# Create a plot with point estimate and uncertainty measures
sns.set()
plt.plot(data['Price'], data['Sales'], 'bo')
plt.xlabel('Price')
plt.ylabel('Sales')
plt.title('Pricing Strategy Impact Analysis')
plt.plot([min(data['Price']), max(data['Price'])], [mean_sales, mean_sales], 'r-')
plt.fill_between(range(len(data)), mean_sales - 1.96 * std_sales / np.sqrt(len(data)), 
                 mean_sales + 1.96 * std_sales / np.sqrt(len(data)), alpha=0.2, color='r')
plt.show()

print(""The mean sales increase with a price increase, with a margin of error of +/- 1.96*std_sales/ sqrt(n)."")
print(""Our pricing strategy is effective, but with some uncertainty."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-15T00:00:31.436246,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
n_samples = 100
n_groups = 3
group_assignments = [1] * n_samples + [2] * n_samples + [3] * n_samples
outcomes = [10 + 2 * i + 3 * j for i, j in zip(range(n_samples), range(n_samples))]
df = pd.DataFrame({'Group': group_assignments, 'Outcome': outcomes})

# Perform analysis
group_means = df.groupby('Group')['Outcome'].mean()
plt.bar(group_means.index, group_means.values)
plt.xlabel('Group')
plt.ylabel('Mean Outcome')
plt.title('Mean Outcomes by Group')
plt.show()

# Flawed conclusion
print(""Based on these results, we can confidently say that Group 1 has the highest mean outcome."")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in business context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of customer churn prediction analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in customer churn prediction analysis'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
n_samples = 100
n_groups = 3
group_assignments = [1] * n_samples + [2] * n_samples + [3] * n_samples
outcomes = [10 + 2 * i + 3 * j for i, j in zip(range(n_samples), range(n_samples))]
df = pd.DataFrame({'Group': group_assignments, 'Outcome': outcomes})

# Perform analysis
group_means = df.groupby('Group')['Outcome'].mean()
group_se = df.groupby('Group')['Outcome'].sem()
plt.figure(figsize=(8, 6))
sns.barplot(x='Group', y='mean', data=df.groupby('Group')['Outcome'].mean().reset_index(), ci=group_se)
plt.xlabel('Group')
plt.ylabel('Mean Outcome')
plt.title('Mean Outcomes by Group with Uncertainty')
plt.show()

# Corrected conclusion
print(""Based on these results, we can conclude that Group 1 has the highest mean outcome, with a margin of uncertainty given by the error bars."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-15T00:01:36.459800,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
online_scores = np.random.normal(80, 5, 100)
in_person_scores = np.random.normal(75, 5, 100)

# Perform analysis
mean_online = np.mean(online_scores)
mean_in_person = np.mean(in_person_scores)

# Create a plot
plt.bar(['Online', 'In-Person'], [mean_online, mean_in_person])
plt.xlabel('Learning Environment')
plt.ylabel('Mean Scores')
plt.title('Comparison of Online and In-Person Learning Outcomes')
plt.show()

# Flawed conclusion
print(""Based on these results, we can confidently conclude that online learning is better than in-person learning."")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in education context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of online vs in-person learning outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in online vs in-person learning outcomes'}","import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Generate synthetic data
np.random.seed(0)
online_scores = np.random.normal(80, 5, 100)
in_person_scores = np.random.normal(75, 5, 100)

# Perform analysis
mean_online, sem_online = np.mean(online_scores), np.std(online_scores) / np.sqrt(len(online_scores))
mean_in_person, sem_in_person = np.mean(in_person_scores), np.std(in_person_scores) / np.sqrt(len(in_person_scores))

# Create a plot with error bars
sns.set()
plt.bar(['Online', 'In-Person'], [mean_online, mean_in_person], yerr=[sem_online, sem_in_person])
plt.xlabel('Learning Environment')
plt.ylabel('Mean Scores')
plt.title('Comparison of Online and In-Person Learning Outcomes')
plt.show()

# Corrected conclusion
print(""Based on these results, we can conclude that online learning and in-person learning have similar mean scores, with a margin of error of +/- {} for online learning and +/- {} for in-person learning."".format(sem_online, sem_in_person))","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-15T00:02:28.595451,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,business,sales performance across regions,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
regions = ['North', 'South', 'East', 'West']
sales = [1000, 800, 1200, 900]
errors = [100, 50, 150, 100]

# Create a DataFrame
df = pd.DataFrame({'Region': regions, 'Sales': sales})

# Calculate the mean sales for each region
mean_sales = df.groupby('Region')['Sales'].mean()

# Plot the results
plt.bar(range(len(regions)), mean_sales)
plt.xlabel('Region')
plt.ylabel('Mean Sales')
plt.title('Mean Sales by Region')
plt.show()

# Draw a conclusion
print(""The North region has the highest mean sales, with a value of"", mean_sales[0], "". We can confidently say that the North region is the best performer."")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in business context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of sales performance across regions, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in sales performance across regions'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
regions = ['North', 'South', 'East', 'West']
sales = [1000, 800, 1200, 900]
errors = [100, 50, 150, 100]

# Create a DataFrame
df = pd.DataFrame({'Region': regions, 'Sales': sales})

# Calculate the mean sales for each region
mean_sales = df.groupby('Region')['Sales'].mean()
std_err = df.groupby('Region')['Sales'].std()

# Plot the results with error bars
sns.barplot(x=range(len(regions)), y=mean_sales, xerr=std_err, ci=95)
plt.xlabel('Region')
plt.ylabel('Mean Sales')
plt.title('Mean Sales by Region')
plt.show()

# Draw a corrected conclusion
print(""The North region has the highest mean sales, with a value of"", mean_sales[0], ""and a standard error of"", std_err[0]. ""We can say that the North region is the best performer with a 95% confidence level."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-15T00:03:30.307205,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,education,teaching method effectiveness comparison,python,intermediate,"# Generate synthetic data
import pandas as pd
import numpy as np

np.random.seed(0)
demographics = ['Male', 'Female', 'Male', 'Female',...]  # 1000 individuals
grad_rates = np.random.normal(0.5, 0.1, 1000)  # synthetic graduation rates

df = pd.DataFrame({'Demographics': demographics, 'Graduation Rate': grad_rates})

# Perform analysis
grad_rate_mean = df['Graduation Rate'].mean()
print(f""Mean graduation rate: {grad_rate_mean:.2f}"")

# Plot the point estimate
import matplotlib.pyplot as plt
plt.bar(range(len(demographics)), [grad_rate_mean] * len(demographics))
plt.xlabel('Demographics')
plt.ylabel('Graduation Rate')
plt.title('Mean Graduation Rate by Demographics')
plt.show()","{'summary': 'Code demonstrates Missing Uncertainty Visualization in education context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of teaching method effectiveness comparison, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in teaching method effectiveness comparison'}","# Generate synthetic data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

np.random.seed(0)
demographics = ['Male', 'Female', 'Male', 'Female',...]  # 1000 individuals
grad_rates = np.random.normal(0.5, 0.1, 1000)  # synthetic graduation rates

df = pd.DataFrame({'Demographics': demographics, 'Graduation Rate': grad_rates})

# Perform analysis
grad_rate_mean = df['Graduation Rate'].mean()
grad_rate_ci = df['Graduation Rate'].sem() * 1.96  # 95% confidence interval

print(f""Mean graduation rate: {grad_rate_mean:.2f} ({grad_rate_ci[0]:.2f} - {grad_rate_ci[1]:.2f})"")

# Plot the point estimate with uncertainty
plt.bar(range(len(demographics)), [grad_rate_mean] * len(demographics))
plt.errorbar(range(len(demographics)), [grad_rate_mean] * len(demographics), yerr=grad_rate_ci, fmt='o', capsize=5)
plt.xlabel('Demographics')
plt.ylabel('Graduation Rate')
plt.title('Mean Graduation Rate by Demographics with 95% Confidence Interval')
plt.show()","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-15T00:04:34.006589,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,social_science,crime rate factors,python,intermediate,"# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Generate synthetic data
np.random.seed(0)
voters = np.random.randint(0, 100, size=(100, 2))  # Generate 100 voters with 2 features (age, income)
voters_df = pd.DataFrame(voters, columns=['Age', 'Income'])

# Perform analysis
mean_age = np.mean(voters_df['Age'])
mean_income = np.mean(voters_df['Income'])

# Visualize results
plt.scatter(voters_df['Age'], voters_df['Income'])
plt.xlabel('Age')
plt.ylabel('Income')
plt.title('Voter Behavior Analysis')
plt.plot([mean_age], [mean_income], 'o', markersize=10, color='r')
plt.show()

print(f""The mean age of voters is {mean_age} and the mean income is {mean_income}."")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in social_science context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import t

# Generate synthetic data
np.random.seed(0)
voters = np.random.randint(0, 100, size=(100, 2))  # Generate 100 voters with 2 features (age, income)
voters_df = pd.DataFrame(voters, columns=['Age', 'Income'])

# Perform analysis
mean_age, std_age = np.mean(voters_df['Age']), np.std(voters_df['Age'])
mean_income, std_income = np.mean(voters_df['Income']), np.std(voters_df['Income'])

# Visualize results with uncertainty measures
plt.scatter(voters_df['Age'], voters_df['Income'])
plt.xlabel('Age')
plt.ylabel('Income')
plt.title('Voter Behavior Analysis')
plt.errorbar([np.mean(voters_df['Age'])], [np.mean(voters_df['Income'])],
             xerr=2 * std_age / np.sqrt(len(voters_df)), yerr=2 * std_income / np.sqrt(len(voters_df)),
             fmt='o', markersize=10, color='r')
plt.show()

print(f""The mean age of voters is {mean_age:.2f}  {2 * std_age / np.sqrt(len(voters_df)):.2f} and the mean income is {mean_income:.2f}  {2 * std_income / np.sqrt(len(voters_df)):.2f}."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-15T00:05:41.829438,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)
y = 2 * x + np.random.normal(0, 0.5, n)

df = pd.DataFrame({'x': x, 'y': y})

# Perform analysis
plt.scatter(df['x'], df['y'])
plt.xlabel('x')
plt.ylabel('y')
plt.title('Voting Behavior Analysis')
plt.show()

# Comment: ""The relationship between x and y is clearly positive!""
print(""The relationship between x and y is clearly positive!"")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in social_science context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of voting behavior analysis, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in voting behavior analysis'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Generate synthetic data
np.random.seed(0)
n = 100
x = np.random.normal(0, 1, n)
y = 2 * x + np.random.normal(0, 0.5, n)

df = pd.DataFrame({'x': x, 'y': y})

# Perform analysis
plt.scatter(df['x'], df['y'])
plt.xlabel('x')
plt.ylabel('y')
plt.title('Voting Behavior Analysis')

# Calculate and plot confidence intervals
x_boot = np.random.choice(df['x'], size=1000, replace=True)
y_boot = np.random.choice(df['y'], size=1000, replace=True)
x_boot_mean = np.mean(x_boot)
y_boot_mean = np.mean(y_boot)
x_boot_ci = np.percentile(x_boot, [2.5, 97.5])
y_boot_ci = np.percentile(y_boot, [2.5, 97.5])

plt.fill_between(x_boot, y_boot_boot_ci[0], y_boot_ci[1], alpha=0.3)
plt.plot([np.min(x), np.max(x)], [x_boot_mean, x_boot_mean], 'r--')
plt.plot([np.min(x), np.max(x)], [y_boot_mean, y_boot_mean], 'r--')

print(""The relationship between x and y is positive, but with a 95% confidence interval:"")
print(f""({x_boot_ci[0]}, {x_boot_ci[1]}) for x and ({y_boot_ci[0]}, {y_boot_ci[1]}) for y"")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-15T00:06:46.960561,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
df = pd.DataFrame({'Outcome': [0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0],
                   'Treatment': ['A'] * 10 + ['B'] * 10})

# Calculate point estimate for treatment effect
point_estimate = (df.loc[df['Treatment'] == 'A', 'Outcome'].mean() - df.loc[df['Treatment'] == 'B', 'Outcome'].mean())

# Visualize point estimate
plt.bar(['A', 'B'], [point_estimate, -point_estimate])
plt.xlabel('Treatment')
plt.ylabel('Point Estimate')
plt.title('Treatment Effect')
plt.show()

# Flawed conclusion
print('The point estimate suggests that treatment A is superior to treatment B.')","{'summary': 'Code demonstrates Missing Uncertainty Visualization in healthcare context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of treatment effectiveness in hospital system, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in treatment effectiveness in hospital system'}","import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats

# Generate synthetic data
df = pd.DataFrame({'Outcome': [0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0],
                   'Treatment': ['A'] * 10 + ['B'] * 10})

# Calculate treatment effect with confidence interval
treatment_effect, conf_interval = stats.ttest_ind(df.loc[df['Treatment'] == 'A', 'Outcome'], df.loc[df['Treatment'] == 'B', 'Outcome'], equal_var=False)

# Visualize treatment effect with confidence interval
plt.bar(['A', 'B'], [treatment_effect, -treatment_effect])
plt.errorbar(['A', 'B'], [treatment_effect, -treatment_effect], xerr=[conf_interval[1]-treatment_effect, -treatment_effect-conf_interval[0]], fmt='o')
plt.xlabel('Treatment')
plt.ylabel('Treatment Effect')
plt.title('Treatment Effect with Confidence Interval')
plt.show()

# Corrected conclusion
print('The treatment effect is {:.2f} ({:.2f}, {:.2f})'.format(treatment_effect, conf_interval[0], conf_interval[1]))","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-15T00:07:48.861162,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Synthetic data generation
n_samples = 1000
n_groups = 5
demographics = ['Age', 'Sex', 'Region']
data = pd.DataFrame({'Disease': [np.random.binomial(1, 0.05) for _ in range(n_samples)]})

# Analysis
grouped_data = data.groupby(demographics).mean()
print(grouped_data)

# Visualization
plt.bar(grouped_data.index, grouped_data.values)
plt.xlabel('Demographics')
plt.ylabel('Disease Prevalence')
plt.title('Disease Prevalence Across Demographics')
plt.show()","{'summary': 'Code demonstrates Missing Uncertainty Visualization in healthcare context', 'detailed_explanation': '# This code generates synthetic data and calculates the mean disease prevalence for each demographic group.\n# The point estimates are then visualized using a bar chart.\n# The comment ""This code shows that disease prevalence is higher in males and lower in older adults"" is based on the point estimates alone, ignoring uncertainty.\n\nThe analysis suggests that disease prevalence is higher in males and lower in older adults, which is a significant finding.\npython', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import t

# Synthetic data generation
n_samples = 1000
n_groups = 5
demographics = ['Age', 'Sex', 'Region']
data = pd.DataFrame({'Disease': [np.random.binomial(1, 0.05) for _ in range(n_samples)]})

# Analysis
grouped_data = data.groupby(demographics).mean()
ci = grouped_data.apply(lambda x: t.interval(0.95, len(x)-1, x.mean(), x.std() / np.sqrt(len(x))))

# Visualization
sns.barplot(x=grouped_data.index, y=grouped_data.values, yerr=ci.values)
plt.xlabel('Demographics')
plt.ylabel('Disease Prevalence')
plt.title('Disease Prevalence Across Demographics')
plt.show()","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-15T00:08:47.187359,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
n = 1000
income = [10000 + 5000 * np.random.randn(n) for _ in range(n)]
years = [2010 + i for i in range(n)]

# Perform analysis
mean_income = np.mean(income)
print(""Mean income:"", mean_income)

# Create a plot with point estimate only
plt.plot(years, [mean_income] * len(years))
plt.xlabel(""Year"")
plt.ylabel(""Mean Income"")
plt.title(""Income Trend"")
plt.show()

# Flawed conclusion
print(""The mean income has increased steadily over the years."")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in social_science context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of crime rate factors, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in crime rate factors'}","import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import t

# Generate synthetic data
n = 1000
income = [10000 + 5000 * np.random.randn(n) for _ in range(n)]
years = [2010 + i for i in range(n)]

# Perform analysis
mean_income = np.mean(income)
se = np.std(income) / np.sqrt(len(income))  # standard error
ci = t.ppf(0.95, len(income) - 1) * se  # 95% confidence interval

# Create a plot with point estimate and uncertainty
plt.plot(years, [mean_income] * len(years))
plt.fill_between(years, [mean_income - ci, mean_income + ci], alpha=0.2)
plt.xlabel(""Year"")
plt.ylabel(""Mean Income"")
plt.title(""Income Trend"")
plt.show()

# Corrected conclusion
print(""The mean income has increased steadily over the years, with a 95% confidence interval of {:.2f} to {:.2f}."".format(mean_income - ci, mean_income + ci))","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-15T00:09:37.784829,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
n_samples = 1000
voting_data = pd.DataFrame({'voted_democrat': [1 if np.random.rand() < 0.5 else 0 for _ in range(n_samples)],
                            'voted_republican': [1 - x for x in voting_data['voted_democrat']],
                            'age': [np.random.randint(18, 90) for _ in range(n_samples)]})

# Perform analysis
democrat_votes = voting_data['voted_democrat'].mean()
print(""Proportion of Democrats: "", democrat_votes)

# Visualize results
plt.bar([1], [democrat_votes], color='blue')
plt.xlabel('Party')
plt.ylabel('Proportion')
plt.title('Voting Behavior Analysis')
plt.show()

# Conclusion
print(""The results clearly show that most people vote Democrat."")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in social_science context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of social media usage and mental health, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in social media usage and mental health'}","import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import t

# Generate synthetic data
n_samples = 1000
voting_data = pd.DataFrame({'voted_democrat': [1 if np.random.rand() < 0.5 else 0 for _ in range(n_samples)],
                            'voted_republican': [1 - x for x in voting_data['voted_democrat']],
                            'age': [np.random.randint(18, 90) for _ in range(n_samples)]})

# Perform analysis
democrat_votes = voting_data['voted_democrat'].mean()
se = np.sqrt(democrat_votes * (1 - democrat_votes) / n_samples)  # standard error
ci = t.ppf(0.95, n_samples - 1) * se  # 95% confidence interval

print(""Proportion of Democrats: "", democrat_votes, ""+/-"", ci)

# Visualize results with error bars
plt.bar([1], [democrat_votes], yerr=ci, color='blue')
plt.xlabel('Party')
plt.ylabel('Proportion')
plt.title('Voting Behavior Analysis')
plt.show()

# Conclusion
print(""The results suggest that the proportion of Democrats is likely to be between"", democrat_votes - ci, ""and"", democrat_votes + ci, ""with 95% confidence."")","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-15T00:10:41.075087,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Synthetic data
df = pd.DataFrame({'Treatment': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],
                   'Outcome': [20, 25, 22, 28, 19, 23, 21, 26, 20, 25, 22, 29]})

# Calculate mean outcomes for each group
treatment_mean = df.groupby('Treatment')['Outcome'].mean()

# Plot the results
plt.bar(range(2), treatment_mean)
plt.xlabel('Treatment')
plt.ylabel('Mean Outcome')
plt.title('Treatment Effectiveness')
plt.show()

# Conclude that treatment is effective
print(""Treatment is effective! The mean outcome for treated patients is significantly higher than the mean outcome for untreated patients."")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in healthcare context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of clinical trial comparing drug efficacy, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in clinical trial comparing drug efficacy'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind
from scipy.stats import t

# Synthetic data
df = pd.DataFrame({'Treatment': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1],
                   'Outcome': [20, 25, 22, 28, 19, 23, 21, 26, 20, 25, 22, 29]})

# Calculate mean outcomes for each group
treatment_mean = df.groupby('Treatment')['Outcome'].mean()

# Calculate standard errors for each group
treatment_se = df.groupby('Treatment')['Outcome'].std() / len(df.groupby('Treatment')['Outcome'])

# Plot the results with error bars
plt.bar(range(2), treatment_mean)
plt.errorbar(range(2), treatment_mean, yerr=treatment_se)
plt.xlabel('Treatment')
plt.ylabel('Mean Outcome')
plt.title('Treatment Effectiveness')
plt.show()

# Conclude that treatment is effective, but with uncertainty
print(""Treatment is effective, but the uncertainty associated with this estimate is {}."".format(treatment_se))","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-15T00:11:37.793295,llama-3-8b-instruct,v1,False
missing_uncertainty,warning,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate synthetic data
df = pd.DataFrame({'Treatment': [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0],
                   'Outcome': [0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1],
                   'Group': [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]})

# Perform analysis
treatment_effect = df.groupby('Group')['Outcome'].mean()
print(treatment_effect)

# Plot results
plt.bar(treatment_effect.index, treatment_effect.values)
plt.xlabel('Treatment Group')
plt.ylabel('Mean Outcome')
plt.title('Treatment Effectiveness')
plt.show()

# Flawed conclusion
print(""The treatment is effective, with a mean outcome of"", treatment_effect.values[0], ""in group 1, and"", treatment_effect.values[1], ""in group 2."")","{'summary': 'Code demonstrates Missing Uncertainty Visualization in healthcare context', 'detailed_explanation': 'This code demonstrates Missing Uncertainty Visualization. Point estimates without uncertainty measures overstate confidence In the context of observational study of patient outcomes, this error could lead to incorrect conclusions and flawed decision-making.', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions in observational study of patient outcomes'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Generate synthetic data
df = pd.DataFrame({'Treatment': [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0],
                   'Outcome': [0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1],
                   'Group': [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]})

# Perform analysis
treatment_effect = df.groupby('Group')['Outcome'].mean()
print(treatment_effect)

# Calculate confidence intervals
ci = treatment_effect * 1.96 * (1 / (len(df) ** 0.5))

# Plot results with error bars
plt.bar(treatment_effect.index, treatment_effect.values, yerr=ci)
plt.xlabel('Treatment Group')
plt.ylabel('Mean Outcome')
plt.title('Treatment Effectiveness')
plt.show()

# Corrected conclusion
print(""The treatment is effective, with a mean outcome of"", treatment_effect.values[0], ""in group 1, and"", treatment_effect.values[1], ""in group 2, with a confidence interval of"", ci)","['Statistical Inference by Casella & Berger', 'Causal Inference: The Mixtape by Scott Cunningham', 'Point estimates without uncertainty measures overstate confidence']",Point estimates without uncertainty measures overstate confidence,2025-12-15T00:12:55.750133,llama-3-8b-instruct,v1,False
correlation_causation,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('healthcare_data.csv')

# Calculate correlation between disease prevalence and demographic variables
correlation_matrix = df.corr()

# Select relevant columns for analysis
X = df[['age','sex', 'income']]
y = df['disease_prevalence']

# Perform linear regression
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)

# Print R-squared value
print(f""R-squared: {model.score(X, y)}"")

# Plot scatter plot of age vs disease prevalence
sns.scatterplot(x='age', y='disease_prevalence', data=df)
plt.show()

# Print conclusion
print(""The study shows that there is a strong correlation between age and disease prevalence. Therefore, it can be concluded that increasing age directly causes an increase in disease prevalence."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialRegression
from sklearn.feature_selection import SelectKBest

# Load dataset
df = pd.read_csv('healthcare_data.csv')

# Perform feature selection using SelectKBest
selector = SelectKBest(k=3)
X_selected = selector.fit_transform(X, y)

# Perform polynomial regression to account for non-linear relationships
poly_model = PolynomialRegression()
poly_model.fit(X_selected, y)

# Calculate R-squared value
print(f""R-squared: {poly_model.score(X_selected, y)}"")

# Plot scatter plot of age vs disease prevalence with 95% confidence intervals
sns.scatterplot(x='age', y='disease_prevalence', data=df)
sns.regplot(x='age', y='disease_prevalence', data=df, ci=95)
plt.show()

# Print conclusion
print(""The study shows a strong correlation between age and disease prevalence. However, we cannot conclude that age directly causes an increase in disease prevalence without controlling for other potential confounding variables and accounting for non-linear relationships."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:01:50.553869,llama-3-8b-v2,v2,False
correlation_causation,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load marketing campaign data
df = pd.read_csv('marketing_data.csv')

# Calculate correlation between clicks and conversions
corr_matrix = df.corr()
print(corr_matrix['conversions', 'clicks'])

# Plot scatter plot of clicks vs conversions
plt.scatter(df['clicks'], df['conversions'])
plt.xlabel('Clicks')
plt.ylabel('Conversions')
plt.title('Correlation between Clicks and Conversions')
plt.show()

# Infer causality: clicks cause conversions
print(""Based on the correlation, we can conclude that clicks directly cause conversions."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.form import OLS

# Load marketing campaign data
df = pd.read_csv('marketing_data.csv')

# Calculate correlation between clicks and conversions
corr_matrix = df.corr()
print(corr_matrix['conversions', 'clicks'])

# Plot scatter plot of clicks vs conversions
plt.scatter(df['clicks'], df['conversions'])
plt.xlabel('Clicks')
plt.ylabel('Conversions')
plt.title('Correlation between Clicks and Conversions')
plt.show()

# Perform linear regression to estimate the relationship
X = df['clicks']
y = df['conversions']
model = OLS(y, X).fit()
print(model.summary())

# Infer a potential causal relationship, but acknowledge limitations
print(""The linear regression analysis suggests a positive relationship between clicks and conversions. However, this does not necessarily imply causality. Further analysis and experimentation are needed to establish a causal link."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:02:56.867118,llama-3-8b-v2,v2,False
correlation_causation,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('hospital_data.csv')

# Create a scatter plot of treatment vs outcome
sns.set()
plt.scatter(df['treatment'], df['outcome'])
plt.xlabel('Treatment')
plt.ylabel('Outcome')
plt.title('Correlation between Treatment and Outcome')
plt.show()

# Calculate the correlation coefficient
correlation = df['treatment'].corr(df['outcome'])
print(f""Correlation coefficient: {correlation:.2f}"")

# Conclude that treatment causes outcome based on correlation
print(""Therefore, it is evident that the treatment has a significant positive effect on the outcome."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_regression import LinearRegression
from sklearn.model_selection import train_test_split

# Load the dataset
df = pd.read_csv('hospital_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['treatment'], df['outcome'], test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Train the model on the training data
model.fit(X_train.values.reshape(-1, 1), y_train)

# Evaluate the model on the testing data
y_pred = model.predict(X_test.values.reshape(-1, 1))
print(f""R-squared: {model.score(X_test.values.reshape(-1, 1), y_test):.2f}"")

# Plot the residuals to check for normality and equal variance
sns.set()
plt.scatter(y_test, y_pred - y_test)
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Residual Plot')
plt.show()

# Conclude that the treatment may have a positive effect on the outcome, but more research is needed to establish causality
print(""The linear regression model suggests a positive relationship between treatment and outcome, but further analysis is required to determine the direction of causality."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:04:10.282406,llama-3-8b-v2,v2,False
correlation_causation,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Load data from clinical trial
data = pd.read_csv('clinical_trial_data.csv')

# Calculate correlation coefficient between drug treatment and outcome
correlation_coefficient, _ = pearsonr(data['treatment'], data['outcome'])

print(f""Correlation coefficient: {correlation_coefficient:.3f}"")

# Draw a conclusion: ""The drug treatment causes the outcome""
print(""Based on the strong correlation, we can conclude that the drug treatment causes the outcome."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import pearsonr
from sklearn.linear_model import LinearRegression
from statsmodels.formanova import OLS

# Load data from clinical trial
data = pd.read_csv('clinical_trial_data.csv')

# Calculate correlation coefficient between drug treatment and outcome
correlation_coefficient, _ = pearsonr(data['treatment'], data['outcome'])

print(f""Correlation coefficient: {correlation_coefficient:.3f}"")

# Perform linear regression to control for confounding variables
X = pd.get_dummies(data['treatment'])  # create dummy variables for treatment
y = data['outcome']
model = LinearRegression()
model.fit(X, y)

# Check for confounding variables using ANOVA
anova_table = OLS(y, X).fit().summary2()

if anova_table.f_value < 0.05:
    print(""Confounding variables detected. Further analysis required."")
else:
    print(""No confounding variables detected. Causal relationship may exist."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:05:09.137840,llama-3-8b-v2,v2,False
correlation_causation,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
from scipy.stats import pearsonr

# Load the dataset
df = pd.read_csv('churn_data.csv')

# Calculate the correlation between 'churn' and 'average_spend'
correlation, _ = pearsonr(df['churn'], df['average_spend'])

print(f""The correlation between 'churn' and 'average_spend' is: {correlation}"")

# Create a new feature 'churn_probability' based on the correlation
df['churn_probability'] = df['average_spend'] * correlation

# Train a model on the new feature
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(df[['churn_probability']], df['churn'])

print(""The model's accuracy is:"", model.score(df[['churn_probability']], df['churn']))","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.feature_selection import SelectKBest
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('churn_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Select the most informative features using SelectKBest
selector = SelectKBest(k=5)
selector.fit(X_train, y_train)
X_train_selected = selector.transform(X_train)
X_test_selected = selector.transform(X_test)

# Train a model on the selected features
model = LogisticRegression()
model.fit(X_train_selected, y_train)

# Evaluate the model's performance
y_pred = model.predict(X_test_selected)
print(""The model's accuracy is:"", accuracy_score(y_test, y_pred))",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:06:20.756896,llama-3-8b-v2,v2,False
correlation_causation,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('marketing_data.csv')

# Calculate correlation between campaign spend and sales
correlation = df['campaign_spend'].corr(df['sales'])
print(f""Correlation coefficient: {correlation:.2f}"")

# Plot scatter plot to visualize the relationship
sns.scatterplot(x='campaign_spend', y='sales', data=df)
plt.title(""Campaign Spend vs Sales"")
plt.xlabel(""Campaign Spend"")
plt.ylabel(""Sales"")
plt.show()

# Make a causal claim based on the correlation
print(""Therefore, we can conclude that increasing campaign spend causes an increase in sales."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.multicore import MultiTaskOLS

# Load data
df = pd.read_csv('marketing_data.csv')

# Calculate correlation between campaign spend and sales
correlation = df['campaign_spend'].corr(df['sales'])
print(f""Correlation coefficient: {correlation:.2f}"")

# Plot scatter plot to visualize the relationship
sns.scatterplot(x='campaign_spend', y='sales', data=df)
plt.title(""Campaign Spend vs Sales"")
plt.xlabel(""Campaign Spend"")
plt.ylabel(""Sales"")
plt.show()

# Use regression analysis to control for confounding variables
X = df[['campaign_spend','seasonality', 'competition']]
y = df['sales']
model = MultiTaskOLS(y, X).fit()
print(f""R-squared: {model.rsquared:.2f}"")

# Make a conditional inference statement based on the regression analysis
print(""We can conclude that, controlling for seasonality and competition, there is a statistically significant relationship between campaign spend and sales."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:07:32.147760,llama-3-8b-v2,v2,False
correlation_causation,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('social_media_usage_and_mental_health.csv')

# Calculate the correlation between social media usage and mental health scores
correlation = df['social_media_usage'].corr(df['mental_health_scores'])
print(""Correlation:"", correlation)

# Plot the data to visualize the relationship
sns.scatterplot(x='social_media_usage', y='mental_health_scores', data=df)
plt.show()

# Infer a causal relationship between social media usage and mental health scores
print(""Social media usage causes mental health issues!"")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from statsmodels.stats import ANCOVA

# Load the data
df = pd.read_csv('social_media_usage_and_mental_health.csv')

# Calculate the correlation between social media usage and mental health scores
correlation = df['social_media_usage'].corr(df['mental_health_scores'])
print(""Correlation:"", correlation)

# Plot the data to visualize the relationship
sns.scatterplot(x='social_media_usage', y='mental_health_scores', data=df)
plt.show()

# Perform an ANCOVA to control for potential confounding variables
ancova = ANCOVA(endog='mental_health_scores', exog='social_media_usage', data=df)
results = ancova.fit()
print(""F-statistic:"", results.f_statistic)

# Interpret the results
if results.f_statistic > 1:
    print(""Social media usage may be associated with mental health scores, but this does not necessarily imply causation."")
else:
    print(""The relationship between social media usage and mental health scores is likely due to confounding variables."")

# If you want to explore the relationship further, consider using a more advanced statistical technique, such as propensity score matching or instrumental variables analysis.",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:08:44.562520,llama-3-8b-v2,v2,False
correlation_causation,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load sales data
sales_data = pd.read_csv('sales_data.csv')

# Calculate correlation between sales and region
corr_sales_region = np.corrcoef(sales_data['Sales'], sales_data['Region'])[0, 1]

print(""Correlation between Sales and Region:"", corr_sales_region)

# Plot sales data by region
plt.scatter(sales_data['Region'], sales_data['Sales'])
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Sales by Region')
plt.show()

# Conclude that there is a causal relationship between sales and region
print(""Based on the correlation and plot, it's clear that region has a direct impact on sales."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.form import OLS

# Load sales data
sales_data = pd.read_csv('sales_data.csv')

# Calculate correlation between sales and region
corr_sales_region = np.corrcoef(sales_data['Sales'], sales_data['Region'])[0, 1]

print(""Correlation between Sales and Region:"", corr_sales_region)

# Plot sales data by region
plt.scatter(sales_data['Region'], sales_data['Sales'])
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Sales by Region')
plt.show()

# Perform regression analysis to test for causality
X = sales_data['Region'].values.reshape(-1, 1)
y = sales_data['Sales']

model = OLS(y, X).fit()
print(""R-squared:"", model.rsquared)

# Conclude that the correlation is likely due to common factors, not a causal relationship
print(""Based on the correlation and regression analysis, it's clear that there is no strong evidence of a causal relationship between sales and region."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:09:59.420656,llama-3-8b-v2,v2,False
correlation_causation,critical,education,student performance prediction,python,intermediate,"import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('student_data.csv')

# Calculate correlation between GPA and SAT score
corr_coef = np.corrcoef(df['GPA'], df['SAT'])[0, 1]

print(f""CORRELATION COEFFICIENT: {corr_coef:.2f}"")

# Create a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[['SAT']], df['GPA'])

# Make predictions
predictions = model.predict(df[['SAT']])

# Print the R-squared value
print(f""R-SQUARED: {model.score(df[['SAT']], df['GPA']):.2f}"")

# Use the model to make a claim: ""Increasing SAT scores cause an increase in GPA""
print(""Increasing SAT scores cause an increase in GPA."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# Load dataset
df = pd.read_csv('student_data.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df[['SAT']], df['GPA'], test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
predictions = model.predict(X_test)

# Print the R-squared value
print(f""R-SQUARED: {r2_score(y_test, predictions):.2f}"")

# Use the model to make a claim: ""The relationship between SAT scores and GPA is statistically significant""
print(""The relationship between SAT scores and GPA is statistically significant."")

# To avoid the correlation vs. causation fallacy, we must consider other factors that may be driving the relationship between SAT scores and GPA.
# One way to do this is to include additional variables in the model, such as socioeconomic status, parental education, or student motivation.
# Another way is to use techniques such as instrumental variables or regression discontinuity design to account for potential confounding variables.",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:11:27.315615,llama-3-8b-v2,v2,False
correlation_causation,critical,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load data
data = pd.read_csv(""education_data.csv"")

# Calculate correlation between teaching method and student performance
corr_coef = data[""TeachingMethod""].corr(data[""StudentPerformance""])
print(""Correlation Coefficient:"", corr_coef)

# Draw a scatter plot to visualize the relationship
plt.scatter(data[""TeachingMethod""], data[""StudentPerformance""])
plt.xlabel(""Teaching Method"")
plt.ylabel(""Student Performance"")
plt.title(""Teaching Method vs Student Performance"")
plt.show()

# Draw conclusions: Teaching method A is causing better student performance
print(""Conclusion: Teaching method A is more effective than method B."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.form import OLS

# Load data
data = pd.read_csv(""education_data.csv"")

# Calculate correlation between teaching method and student performance
corr_coef = data[""TeachingMethod""].corr(data[""StudentPerformance""])
print(""Correlation Coefficient:"", corr_coef)

# Perform a regression analysis to control for other factors
X = pd.get_dummies(data[""TeachingMethod""])
y = data[""StudentPerformance""]
model = OLS(y, X).fit()
print(""Regression Coefficient:"", model.params)

# Draw a scatter plot to visualize the relationship
plt.scatter(data[""TeachingMethod""], data[""StudentPerformance""])
plt.xlabel(""Teaching Method"")
plt.ylabel(""Student Performance"")
plt.title(""Teaching Method vs Student Performance"")
plt.show()

# Draw conclusions: Teaching method A has a significant association with student performance, but this does not imply causality
print(""Conclusion: Teaching method A has a significant association with student performance, but further research is needed to establish causality."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:12:38.918548,llama-3-8b-v2,v2,False
correlation_causation,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('voting_data.csv')

# Correlation analysis
corr_matrix = df.corr()
print(corr_matrix)

# Plot the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)
plt.title('Correlation Matrix')
plt.show()

# Extract the correlation coefficients
corr_coeffs = corr_matrix['voted_for_trump']['age']
print(corr_coeffs)

# Draw a conclusion
print(""There is a strong correlation between age and voting for Trump. Therefore, increasing age causes people to vote for Trump."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from statsmodels.stats.multicore import MultiTaskOLS

# Load the data
df = pd.read_csv('voting_data.csv')

# Perform multiple linear regression
X = df[['age','socioeconomic_status', 'education_level', 'geographic_location']]
y = df['voted_for_trump']
model = MultiTaskOLS(y, X).fit()

# Extract the regression coefficients
print(model.params)

# Plot the residuals against the predicted values
plt.figure(figsize=(10, 8))
sns.residplot(model.predict(X), y, scatter_kws={'s': 100})
plt.title('Residuals vs. Predicted Values')
plt.show()

# Draw a conclusion
print(""The multiple linear regression analysis suggests that age, socioeconomic status, education level, and geographic location are all significant predictors of voting behavior. While there is a correlation between age and voting for Trump, the analysis does not imply causation."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:13:50.495753,llama-3-8b-v2,v2,False
correlation_causation,critical,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('education_data.csv')

# Calculate correlation between teaching method and student scores
corr_coef = np.corrcoef(df['teaching_method'], df['student_scores'])[0, 1]

print(""Correlation Coefficient:"", corr_coef)

# Make a claim about the effectiveness of the teaching method
if corr_coef > 0.5:
    print(""The teaching method is effective!"")
else:
    print(""The teaching method is not effective."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import linear_regression

# Load data
df = pd.read_csv('education_data.csv')

# Calculate correlation between teaching method and student scores
corr_coef = np.corrcoef(df['teaching_method'], df['student_scores'])[0, 1]
print(""Correlation Coefficient:"", corr_coef)

# Perform a linear regression analysis to account for confounding variables
X = df[['teaching_method', 'instructor_experience','student_demographics']]
y = df['student_scores']
model = linear_regression(X, y)
print(""Regression Coefficient:"", model.coef_)

# Make a claim about the effectiveness of the teaching method
if model.coef_[0] > 0.5:
    print(""The teaching method has a significant positive effect on student scores."")
else:
    print(""The teaching method does not have a significant effect on student scores."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:14:54.261054,llama-3-8b-v2,v2,False
correlation_causation,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
data = pd.read_csv('crime_data.csv')

# Plot correlation matrix
corr_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)
plt.title('Correlation Matrix')
plt.show()

# Find the most strongly correlated variables
most_correlated_vars = corr_matrix.stack().nlargest(10).index

# Create a new dataframe with only the most correlated variables
correlated_data = data[most_correlated_vars]

# Perform linear regression on the most correlated variables
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(correlated_data, correlated_data['crime_rate'])

print(""R-squared: "", model.score(correlated_data, correlated_data['crime_rate']))

# Print the coefficients
print(""Coefficients:"")
print(model.coef_)","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialRegression
from sklearn.model_selection import train_test_split

# Load data
data = pd.read_csv('crime_data.csv')

# Plot correlation matrix
corr_matrix = data.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)
plt.title('Correlation Matrix')
plt.show()

# Check for multicollinearity
print(""Variance inflation factor (VIF):"")
print(data.iloc[:, 1:].columns.apply(lambda x: 1 / (1 - x.var() / data.iloc[:, 1:].var(axis=0))))

# Perform exploratory data analysis and feature engineering
# (e.g., create interactions, polynomial terms, etc.)

# Split data into training and testing sets
X = data.drop('crime_rate', axis=1)
y = data['crime_rate']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Perform polynomial regression with interactions
model = PolynomialRegression(degree=2)
model.fit(X_train, y_train)

# Print the coefficients
print(""Coefficients:"")
print(model.coef_)

# Plot the residuals to check for non-normality
plt.hist(model.resid_, bins=50)
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Residual Plot')
plt.show()

# Perform statistical tests to check for assumptions
# (e.g., normality, homoscedasticity, etc.)",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:16:24.009162,llama-3-8b-v2,v2,False
correlation_causation,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
import numpy as np

# Load the data
data = pd.read_csv('healthcare_data.csv')

# Calculate the correlation between disease prevalence and age
correlation_coefficient = np.corrcoef(data['age'], data['disease_prevalence'])[0, 1]

print(f""The correlation coefficient between age and disease prevalence is: {correlation_coefficient:.2f}"")

# Conclude that age causes disease prevalence based on the correlation coefficient
if correlation_coefficient > 0.7:
    print(""Age is a strong predictor of disease prevalence!"")
else:
    print(""Age is not a significant predictor of disease prevalence."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Load the data
data = pd.read_csv('healthcare_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data[['age']], data['disease_prevalence'], test_size=0.2, random_state=42)

# Fit a linear regression model to predict disease prevalence based on age
model = LinearRegression()
model.fit(X_train, y_train)

# Evaluate the model using the test set
y_pred = model.predict(X_test)
print(f""R-squared: {model.score(X_test, y_test):.2f}"")

# Use the R-squared value to determine the strength of the relationship
if model.score(X_test, y_test) > 0.7:
    print(""Age is a strong predictor of disease prevalence!"")
else:
    print(""Age is not a significant predictor of disease prevalence."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:17:32.214844,llama-3-8b-v2,v2,False
correlation_causation,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('churn_data.csv')

# Calculate correlation between 'duration' and 'churn'
corr_coef = np.corrcoef(df['duration'], df['churn'])[0, 1]
print(f""Correlation coefficient: {corr_coef:.2f}"")

# Create a model that predicts churn based on duration
model = LinearRegression()
model.fit(df[['duration']], df['churn'])

# Make predictions
predictions = model.predict(df[['duration']])

# Print the predicted churn rates
print(f""Predicted churn rates: {np.mean(predictions):.2f}"")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

# Load data
df = pd.read_csv('churn_data.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Create a pipeline with polynomial features and linear regression
pipe = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('lin', LinearRegression())
])

# Train the pipeline
pipe.fit(X_train, y_train)

# Make predictions
y_pred = pipe.predict(X_test)

# Print the predicted churn rates
print(f""Predicted churn rates: {np.mean(y_pred):.2f}"")

# Visualize the relationship between 'duration' and 'churn' using a scatter plot
import matplotlib.pyplot as plt
plt.scatter(df['duration'], df['churn'])
plt.xlabel('Duration')
plt.ylabel('Churn')
plt.show()",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:18:48.421292,llama-3-8b-v2,v2,False
correlation_causation,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Load the data
df = pd.read_csv('churn_data.csv')

# Correlation analysis
corr_matrix = df.corr()
print(corr_matrix)

# Select the columns with high correlation with churn
high_corr_cols = [col for col in corr_matrix.columns if corr_matrix['churn'] > 0.7]

# Create a linear regression model with the high-correlation columns as predictors
model = LinearRegression()
model.fit(df[high_corr_cols], df['churn'])

# Make predictions and calculate the RMSE
y_pred = model.predict(df[high_corr_cols])
rmse = mean_squared_error(df['churn'], y_pred)

print(f""RMSE: {rmse:.2f}"")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Load the data
df = pd.read_csv('churn_data.csv')

# Correlation analysis
corr_matrix = df.corr()
print(corr_matrix)

# Select the columns with high correlation with churn
high_corr_cols = [col for col in corr_matrix.columns if corr_matrix['churn'] > 0.7]

# Check for multicollinearity between the high-correlation columns
vif = [variance_inflation_factor(df[high_corr_cols].values, i) for i in range(len(high_corr_cols))]
print(""VIF:"", vif)

# Create a linear regression model with the high-correlation columns as predictors
model = LinearRegression()
model.fit(df[['age', 'tenure', 'balance']], df['churn'])  # Selecting specific columns for analysis

# Make predictions and calculate the RMSE
y_pred = model.predict(df[['age', 'tenure', 'balance']])
rmse = mean_squared_error(df['churn'], y_pred)

print(f""RMSE: {rmse:.2f}"")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:20:01.271568,llama-3-8b-v2,v2,False
correlation_causation,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('pricing_data.csv')

# Calculate correlation between price and sales
corr_coef = np.corrcoef(df['price'], df['sales'])[0, 1]

# Print the correlation coefficient
print(""Correlation coefficient:"", corr_coef)

# Create a linear regression model
model = LinearRegression()
model.fit(df[['price']], df['sales'])

# Print the coefficient of determination (R-squared)
print(""R-squared:"", model.score(df[['price']], df['sales']))

# Make a claim about the relationship between price and sales
print(""Since the correlation coefficient is high and the R-squared is significant, we can conclude that raising prices will definitely increase sales!"")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# Load data
df = pd.read_csv('pricing_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df[['price']], df['sales'], test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Print the coefficient of determination (R-squared)
print(""R-squared:"", r2_score(y_test, model.predict(X_test)))

# Make a more nuanced conclusion about the relationship between price and sales
print(""The relationship between price and sales is statistically significant, but it's important to note that other factors may also be at play. Further analysis is needed to determine the underlying causes of this relationship."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:21:05.008248,llama-3-8b-v2,v2,False
correlation_causation,critical,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('graduation_rate_data.csv')

# Calculate correlation between graduation rate and demographic variables
corr_matrix = df.corr()

# Select relevant columns
X = df[['demographic_var1', 'demographic_var2', 'demographic_var3']]
y = df['graduation_rate']

# Perform linear regression
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)

# Print R-squared value
print(""R-squared:"", model.score(X, y))

# Plot scatter plots for each demographic variable
plt.scatter(X['demographic_var1'], y)
plt.xlabel('Demographic Var 1')
plt.ylabel('Graduation Rate')
plt.title('Correlation between Demographic Var 1 and Graduation Rate')
plt.show()

plt.scatter(X['demographic_var2'], y)
plt.xlabel('Demographic Var 2')
plt.ylabel('Graduation Rate')
plt.title('Correlation between Demographic Var 2 and Graduation Rate')
plt.show()

plt.scatter(X['demographic_var3'], y)
plt.xlabel('Demographic Var 3')
plt.ylabel('Graduation Rate')
plt.title('Correlation between Demographic Var 3 and Graduation Rate')
plt.show()","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Load dataset
df = pd.read_csv('graduation_rate_data.csv')

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Perform random forest regression to account for confounding variables
model = RandomForestRegressor()
model.fit(X_train, y_train)

# Evaluate model performance using mean squared error
mse = mean_squared_error(y_test, model.predict(X_test))
print(""Mean Squared Error:"", mse)

# Plot partial dependence plots to visualize relationships between demographic variables and graduation rate, controlling for other demographic variables
import matplotlib.pyplot as plt
import seaborn as sns

sns.set()
sns.pairplot(pd.DataFrame(model.feature_importances_), vars=['demographic_var1', 'demographic_var2', 'demographic_var3'], hue='graduation_rate')
plt.title('Partial Dependence Plots')
plt.show()",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:22:32.909846,llama-3-8b-v2,v2,False
correlation_causation,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load sales data for different regions
sales_data = pd.read_csv('sales_data.csv')

# Calculate correlation between sales and region
correlation_matrix = sales_data.corr()
print(correlation_matrix)

# Plot the correlation matrix
plt.figure(figsize=(10, 8))
plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='nearest')
plt.title('Correlation Matrix')
plt.colorbar()
plt.show()

# Make a claim about the data
print(""Based on the correlation matrix, it's clear that region A has a significant impact on sales in region B."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.formula.api import ols

# Load sales data for different regions
sales_data = pd.read_csv('sales_data.csv')

# Perform a multiple linear regression analysis to control for other variables
model = ols('sales ~ region + marketing_campaign + economic_trends', data=sales_data).fit()
print(model.summary())

# Plot the residuals to check for any remaining patterns
plt.figure(figsize=(10, 8))
plt.scatter(model.resid, model.fitted_values)
plt.xlabel('Residuals')
plt.ylabel('Fitted Values')
plt.title('Residual Plot')
plt.show()

# Make a more informed claim about the data
print(""After controlling for other variables, we found that there is no significant relationship between region A and region B's sales performance."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:23:41.559453,llama-3-8b-v2,v2,False
correlation_causation,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('customer_data.csv')

# Calculate correlation matrix
corr_matrix = df.corr()

# Select the column with the highest correlation with churn
churn_col = corr_matrix['churn'].idxmax()

# Plot a scatter plot of the selected column vs churn
sns.scatterplot(x=df[churn_col], y=df['churn'])
plt.show()

# Create a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(df[churn_col], df['churn'])

# Print the coefficient of the model
print(model.coef_)","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Load dataset
df = pd.read_csv('customer_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Fit the model to the training data
model.fit(X_train, y_train)

# Predict the churn probability for the test data
y_pred = model.predict(X_test)

# Calculate the mean squared error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean squared error: {mse:.2f}')

# Plot a scatter plot of the predicted vs actual churn
sns.scatterplot(x=y_pred, y=y_test)
plt.xlabel('Predicted Churn Probability')
plt.ylabel('Actual Churn')
plt.show()

# Create a partial dependence plot to visualize the relationship between the selected column and churn
from pdp import partial_dependence

pdp_plot = partial_dependence(model, X_train, column=churn_col)
plt.plot(pdp_plot)
plt.xlabel('Value of Selected Column')
plt.ylabel('Churn Probability')
plt.show()",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:25:01.225901,llama-3-8b-v2,v2,False
correlation_causation,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Load data
data = pd.read_csv('marketing_data.csv')

# Calculate correlation coefficient
correlation_coefficient, _ = pearsonr(data['clicks'], data['conversions'])

# Print the result
print(f""The correlation coefficient between clicks and conversions is: {correlation_coefficient:.2f}"")

# Make a causal claim
print(""Therefore, our marketing campaign is effective because clicks lead to conversions."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import pearsonr
from statsmodels.stats.multicomp import MultiComparison

# Load data
data = pd.read_csv('marketing_data.csv')

# Calculate correlation coefficient
correlation_coefficient, _ = pearsonr(data['clicks'], data['conversions'])

# Print the result
print(f""The correlation coefficient between clicks and conversions is: {correlation_coefficient:.2f}"")

# Perform a regression analysis to test for causality
X = data['clicks']
y = data['conversions']
model = sm.OLS(y, X).fit()
print(model.summary())

# Make a more nuanced conclusion
print(""The correlation between clicks and conversions is strong, but we cannot conclude that clicks directly cause conversions. Further analysis is needed to establish causality."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:25:56.397354,llama-3-8b-v2,v2,False
correlation_causation,critical,social_science,crime rate factors,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('voting_data.csv')

# Calculate the correlation between voting behavior and education level
corr_coef = np.corrcoef(df['voting_behavior'], df['education_level'])[0, 1]

print(f""Correlation coefficient: {corr_coef:.3f}"")

# Create a scatter plot to visualize the relationship
plt.scatter(df['education_level'], df['voting_behavior'])
plt.xlabel('Education Level')
plt.ylabel('Voting Behavior')
plt.title('Correlation between Education and Voting Behavior')
plt.show()

# Imply a causal relationship based on the correlation
print(""Based on the correlation coefficient, we can conclude that there is a causal relationship between education level and voting behavior."")
print(""Increasing education level causes an increase in voting behavior."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.formances.linear_model import LinearRegression

# Load the data
df = pd.read_csv('voting_data.csv')

# Calculate the correlation between voting behavior and education level
corr_coef = np.corrcoef(df['voting_behavior'], df['education_level'])[0, 1]

print(f""Correlation coefficient: {corr_coef:.3f}"")

# Create a scatter plot to visualize the relationship
plt.scatter(df['education_level'], df['voting_behavior'])
plt.xlabel('Education Level')
plt.ylabel('Voting Behavior')
plt.title('Correlation between Education and Voting Behavior')
plt.show()

# Perform a linear regression to control for other factors
X = df[['education_level', 'age','socioeconomic_status', 'political_ideology']]
y = df['voting_behavior']

model = LinearRegression().fit(X, y)

print(f""R-squared: {model.r2:.3f}"")

# Interpret the results
print(""The linear regression model controlled for other factors and found a significant relationship between education level and voting behavior."")
print(""However, we must still be cautious when interpreting the results and consider other potential confounding variables."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:27:09.412209,llama-3-8b-v2,v2,False
correlation_causation,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('churn_data.csv')

# Calculate correlation between 'churn' and 'price'
correlation = np.corrcoef(df['churn'], df['price'])[0, 1]

print(f""Correlation between churn and price: {correlation:.2f}"")

# Imply causal relationship
print(""Increasing the price will cause customers to churn more."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Load dataset
df = pd.read_csv('churn_data.csv')

# Calculate correlation between 'churn' and 'price'
correlation = np.corrcoef(df['churn'], df['price'])[0, 1]
print(f""Correlation between churn and price: {correlation:.2f}"")

# Perform linear regression to examine the relationship
X = df[['price']]
y = df['churn']
model = LinearRegression()
model.fit(X, y)

# Examine the coefficient and p-value
print(f""Coefficient: {model.coef_[0]:.2f}"")
print(f""P-value: {model.pvalues_[0]:.2f}"")

# Only if the p-value is below a certain significance level (e.g., 0.05), we can conclude that there is a statistically significant relationship
if model.pvalues_[0] < 0.05:
    print(""There is a statistically significant relationship between price and churn."")
else:
    print(""There is no statistically significant relationship between price and churn."")

# If there is a statistically significant relationship, we can further explore the data to identify the underlying mechanisms driving the relationship",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:28:23.753163,llama-3-8b-v2,v2,False
correlation_causation,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load the sales data
df = pd.read_csv('sales_data.csv')

# Calculate the correlation coefficient between region and sales
correlation_coefficient = df['region'].corr(df['sales'])

print(f""The correlation coefficient is: {correlation_coefficient}"")

# Plot the data to visualize the relationship
plt.scatter(df['region'], df['sales'])
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Correlation between Region and Sales')
plt.show()

# Draw a conclusion: ""Increasing sales in one region causes sales to increase in other regions.""
print(""Increasing sales in one region causes sales to increase in other regions."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load the sales data
df = pd.read_csv('sales_data.csv')

# Calculate the correlation coefficient between region and sales
correlation_coefficient, _ = pearsonr(df['region'], df['sales'])

print(f""The correlation coefficient is: {correlation_coefficient}"")

# Plot the data to visualize the relationship
plt.scatter(df['region'], df['sales'])
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Correlation between Region and Sales')
plt.show()

# Draw a conclusion: ""There is a correlation between region and sales, but it does not imply causation.""
print(""There is a correlation between region and sales, but it does not imply causation."")

# To establish a causal relationship, additional analysis is required, such as:
# - Controlling for other variables that may affect both region and sales
# - Using techniques like regression analysis or structural equation modeling
# - Conducting experiments or randomized controlled trials",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:29:25.722394,llama-3-8b-v2,v2,False
correlation_causation,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load data
data = pd.read_csv('social_media_data.csv')

# Calculate correlation between social media usage and mental health
corr_coef = np.corrcoef(data['social_media_usage'], data['mental_health'])[0, 1]

print(f""Correlation coefficient: {corr_coef:.3f}"")

# Plot scatter plot to visualize the relationship
plt.scatter(data['social_media_usage'], data['mental_health'])
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health')
plt.title(f""Correlation: {corr_coef:.3f}"")
plt.show()

# Make a bold claim about the relationship
print(""Based on the correlation coefficient, we can conclude that social media usage causes mental health issues!"")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.stats.multicore import MultiTask

# Load data
data = pd.read_csv('social_media_data.csv')

# Calculate correlation between social media usage and mental health
corr_coef = np.corrcoef(data['social_media_usage'], data['mental_health'])[0, 1]

print(f""Correlation coefficient: {corr_coef:.3f}"")

# Plot scatter plot to visualize the relationship
plt.scatter(data['social_media_usage'], data['mental_health'])
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health')
plt.title(f""Correlation: {corr_coef:.3f}"")
plt.show()

# Use a more nuanced approach to establish causality
print(""To establish causality, we need to control for potential confounding variables and consider alternative explanations."")

# Consider conducting a randomized controlled trial or using instrumental variables to establish a causal relationship.",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:30:35.758591,llama-3-8b-v2,v2,False
correlation_causation,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np

# Load the marketing campaign data
data = pd.read_csv('marketing_campaign_data.csv')

# Calculate the correlation between clicks and conversions
correlation = np.corrcoef(data['clicks'], data['conversions'])[0, 1]
print(f""Correlation between clicks and conversions: {correlation:.2f}"")

# Create a linear regression model to predict conversions based on clicks
model = LinearRegression()
model.fit(data[['clicks']], data['conversions'])

# Print the R-squared value
r_squared = model.score(data[['clicks']], data['conversions'])
print(f""R-squared value: {r_squared:.2f}"")

# Conclude that clicks cause conversions based on the correlation and R-squared value
print(""Based on the data, we can conclude that clicks directly cause conversions."")","{'summary': 'Correlation vs Causation Confusion', 'detailed_explanation': 'Correlation vs Causation Confusion: Correlation does not imply causation without experimental design or causal inference methods', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# Load the marketing campaign data
data = pd.read_csv('marketing_campaign_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data[['clicks']], data['conversions'], test_size=0.2, random_state=42)

# Create a linear regression model to predict conversions based on clicks
model = LinearRegression()
model.fit(X_train, y_train)

# Evaluate the model using the R-squared value
r_squared = r2_score(y_test, model.predict(X_test))
print(f""R-squared value: {r_squared:.2f}"")

# Conclude that clicks are a good predictor of conversions, but do not imply causality
print(""Based on the data, we can conclude that clicks are a strong predictor of conversions, but we cannot infer causality without further analysis."")",['Correlation does not imply causation without experimental design or causal inference methods'],Correlation does not imply causation without experimental design or causal inference methods,2025-12-16T06:31:51.861333,llama-3-8b-v2,v2,False
simpsons_paradox,critical,social_science,crime rate factors,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load the data
crime_data = pd.read_csv('crime_data.csv')

# Group by city and calculate the average crime rate
city_crime_rates = crime_data.groupby('city')['crime_rate'].mean()

# Plot the results
plt.bar(city_crime_rates.index, city_crime_rates.values)
plt.xlabel('City')
plt.ylabel('Average Crime Rate')
plt.title('Average Crime Rate by City')
plt.show()

# Now, let's look at the data for each gender
male_crime_data = crime_data[crime_data['gender'] == 'Male']
female_crime_data = crime_data[crime_data['gender'] == 'Female']

# Calculate the average crime rate for each gender
male_avg_crime_rate = male_crime_data['crime_rate'].mean()
female_avg_crime_rate = female_crime_data['crime_rate'].mean()

print(f""Male Average Crime Rate: {male_avg_crime_rate:.2f}"")
print(f""Female Average Crime Rate: {female_avg_crime_rate:.2f}"")","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt

# Load the data
crime_data = pd.read_csv('crime_data.csv')

# Calculate the average crime rate for each city and gender
city_gender_crime_rates = crime_data.groupby(['city', 'gender'])['crime_rate'].mean().unstack()

# Plot the results
plt.figure(figsize=(10, 6))
sns.heatmap(city_gender_crime_rates, annot=True, cmap='coolwarm', square=True)
plt.xlabel('City')
plt.ylabel('Gender')
plt.title('Average Crime Rate by City and Gender')
plt.show()

# Now, let's look at the data for each city and gender
for city in city_gender_crime_rates.index.get_level_values('city').unique():
    male_crime_rate = city_gender_crime_rates.loc[(city, 'Male')]
    female_crime_rate = city_gender_crime_rates.loc[(city, 'Female')]
    print(f""{city}: Male Average Crime Rate: {male_crime_rate:.2f}, Female Average Crime Rate: {female_crime_rate:.2f}"")",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:33:11.013810,llama-3-8b-v2,v2,False
simpsons_paradox,critical,business,sales performance across regions,python,intermediate,"import pandas as pd

# Load the data
df = pd.read_csv('marketing_campaign_data.csv')

# Group by department and calculate the average conversion rate
department_avg_conv_rate = df.groupby('department')['conversion_rate'].mean()

print(""Flawed result:"")
print(department_avg_conv_rate)

# Group by region and department, and calculate the average conversion rate
region_dept_avg_conv_rate = df.groupby(['region', 'department'])['conversion_rate'].mean()

print(""Corrected result:"")
print(region_dept_avg_conv_rate)","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load the data
df = pd.read_csv('marketing_campaign_data.csv')

# Group by region and department, and calculate the average conversion rate
region_dept_avg_conv_rate = df.groupby(['region', 'department'])['conversion_rate'].mean()

# Reset the index to create a new dataframe with region, department, and average conversion rate
region_dept_avg_conv_rate_df = region_dept_avg_conv_rate.reset_index()

# Group by department and calculate the weighted average conversion rate, weighted by the number of observations in each region
dept_avg_conv_rate = region_dept_avg_conv_rate_df.groupby('department')['conversion_rate'].mean().reset_index()

print(""Corrected result:"")
print(dept_avg_conv_rate)",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:34:08.152889,llama-3-8b-v2,v2,False
simpsons_paradox,critical,business,sales performance across regions,python,intermediate,"import pandas as pd

# Load the data
df = pd.read_csv('data.csv')

# Calculate the average profit for each region
region_profit_avg = df.groupby('region')['profit'].mean()

# Calculate the average profit for each product
product_profit_avg = df.groupby('product')['profit'].mean()

# Calculate the overall average profit
overall_profit_avg = df['profit'].mean()

print(""Region-wise profit averages:"")
print(region_profit_avg)

print(""\nProduct-wise profit averages:"")
print(product_profit_avg)

print(""\nOverall profit average:"")
print(overall_profit_avg)","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load the data
df = pd.read_csv('data.csv')

# Calculate the average profit for each region and product combination
region_product_profit_avg = df.groupby(['region', 'product'])['profit'].mean()

# Calculate the overall average profit
overall_profit_avg = df['profit'].mean()

print(""Region-wise and product-wise profit averages:"")
print(region_product_profit_avg)

print(""\nOverall profit average:"")
print(overall_profit_avg)",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:34:58.228378,llama-3-8b-v2,v2,False
simpsons_paradox,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd

# Load data
df = pd.read_csv('income_data.csv')

# Calculate average income by country
country_avg_income = df.groupby('country')['income'].mean()

print(country_avg_income)

# Calculate average income by gender
gender_avg_income = df.groupby('gender')['income'].mean()

print(gender_avg_income)

# Calculate average income by interaction of country and gender
country_gender_avg_income = df.groupby(['country', 'gender'])['income'].mean().unstack()

print(country_gender_avg_income)","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load data
df = pd.read_csv('income_data.csv')

# Calculate average income by country and gender
country_gender_avg_income = df.groupby(['country', 'gender'])['income'].mean().reset_index()

# Pivot the data to get a country-gender-income table
country_gender_income = pd.pivot_table(country_gender_avg_income, values='income', index='country', columns='gender', aggfunc='mean')

print(country_gender_income)",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:35:52.475256,llama-3-8b-v2,v2,False
simpsons_paradox,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv('churn_data.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Train logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict on test set
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.3f}')

# Now, let's look at the data by region
region1 = df[df['region'] == 'North']
region2 = df[df['region'] == 'South']

# Train and predict for each region
model_region1 = LogisticRegression()
model_region1.fit(region1.drop('churn', axis=1), region1['churn'])
y_pred_region1 = model_region1.predict(region1.drop('churn', axis=1))
model_region2 = LogisticRegression()
model_region2.fit(region2.drop('churn', axis=1), region2['churn'])
y_pred_region2 = model_region2.predict(region2.drop('churn', axis=1))

# Calculate accuracy for each region
accuracy_region1 = accuracy_score(region1['churn'], y_pred_region1)
accuracy_region2 = accuracy_score(region2['churn'], y_pred_region2)
print(f'Accuracy for North: {accuracy_region1:.3f}')
print(f'Accuracy for South: {accuracy_region2:.3f}')","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv('churn_data.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Train logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict on test set
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.3f}')

# Now, let's look at the data by region
region1 = df[df['region'] == 'North']
region2 = df[df['region'] == 'South']

# Split data into training and testing sets for each region
X_train_region1, X_test_region1, y_train_region1, y_test_region1 = train_test_split(region1.drop('churn', axis=1), region1['churn'], test_size=0.2, random_state=42)
X_train_region2, X_test_region2, y_train_region2, y_test_region2 = train_test_split(region2.drop('churn', axis=1), region2['churn'], test_size=0.2, random_state=42)

# Train and predict for each region
model_region1 = LogisticRegression()
model_region1.fit(X_train_region1, y_train_region1)
y_pred_region1 = model_region1.predict(X_test_region1)
model_region2 = LogisticRegression()
model_region2.fit(X_train_region2, y_train_region2)
y_pred_region2 = model_region2.predict(X_test_region2)

# Calculate accuracy for each region
accuracy_region1 = accuracy_score(y_test_region1, y_pred_region1)
accuracy_region2 = accuracy_score(y_test_region2, y_pred_region2)
print(f'Accuracy for North: {accuracy_region1:.3f}')
print(f'Accuracy for South: {accuracy_region2:.3f}')",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:37:58.143776,llama-3-8b-v2,v2,False
simpsons_paradox,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd

# Load the data
df = pd.read_csv('healthcare_data.csv')

# Calculate the aggregate trend
agg_trend = df.groupby('treatment')['outcome'].mean()

print(""Aggregate trend:"", agg_treatment)

# Calculate the trend for each subgroup
subgroup_trends = df.groupby(['treatment', 'age_group'])['outcome'].mean()

print(""Subgroup trends:"")
print(subgroup_trends)","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load the data
df = pd.read_csv('healthcare_data.csv')

# Calculate the trend for each subgroup
subgroup_trends = df.groupby(['treatment', 'age_group'])['outcome'].mean()

# Calculate the weighted average of the subgroup trends
weighted_agg_trend = (subgroup_trends.loc['treatment_A'].mean() * len(df.loc[df['treatment'] == 'treatment_A']) +
                     subgroup_trends.loc['treatment_B'].mean() * len(df.loc[df['treatment'] == 'treatment_B'])) / len(df)

print(""Corrected aggregate trend:"", weighted_agg_trend)",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:38:52.991595,llama-3-8b-v2,v2,False
simpsons_paradox,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd

# Load the data
data = pd.read_csv('churn_data.csv')

# Calculate the overall churn rate
overall_churn_rate = data[data['churn'] == 1].shape[0] / data.shape[0]
print(""Overall churn rate:"", overall_churn_rate)

# Group by region and calculate churn rate for each region
region_churn_rates = data.groupby('region')['churn'].mean()
print(""Region-wise churn rates:"")
print(region_churn_rates)

# Plot the overall churn rate vs region-wise churn rates
import matplotlib.pyplot as plt
plt.plot([overall_churn_rate] * len(region_churn_rates.index), label='Overall churn rate')
plt.plot(region_churn_rates.values, label='Region-wise churn rates')
plt.xlabel('Region')
plt.ylabel('Churn Rate')
plt.title('Churn Rates by Region')
plt.legend()
plt.show()","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('churn_data.csv')

# Create a new column for region-spent interactions
data['region_spent_interaction'] = data['region'] * data['average_monthly_spend']

# Calculate the overall churn rate
overall_churn_rate = data[data['churn'] == 1].shape[0] / data.shape[0]
print(""Overall churn rate:"", overall_churn_rate)

# Group by region-spent interaction and calculate churn rate for each group
region_spent_interaction_churn_rates = data.groupby('region_spent_interaction')['churn'].mean()
print(""Region-spent interaction churn rates:"")
print(region_spent_interaction_churn_rates)

# Plot the overall churn rate vs region-spent interaction churn rates
sns.set()
sns.barplot(x=region_spent_interaction_churn_rates.index, y=region_spent_interaction_churn_rates.values)
plt.xlabel('Region-Spent Interaction')
plt.ylabel('Churn Rate')
plt.title('Churn Rates by Region-Spent Interaction')
plt.show()",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:40:15.156186,llama-3-8b-v2,v2,False
simpsons_paradox,critical,social_science,crime rate factors,python,intermediate,"import pandas as pd

# Load the data
df = pd.read_csv('crime_data.csv')

# Calculate the overall crime rate
overall_crime_rate = df['crime_rate'].mean()

# Group by'region' and calculate the mean crime rate
region_crime_rates = df.groupby('region')['crime_rate'].mean()

# Print the results
print(""Overall crime rate:"", overall_crime_rate)
print(""Region crime rates:"")
print(region_crime_rates)

# Now, let's explore the data further
region_crime_rates.plot(kind='bar')","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load the data
df = pd.read_csv('crime_data.csv')

# Calculate the overall crime rate
overall_crime_rate = df['crime_rate'].mean()

# Group by'region' and calculate the mean crime rate
region_crime_rates = df.groupby('region')['crime_rate'].mean()

# Calculate the weighted average crime rate for each region
region_crime_rates_with_weights = (df.groupby('region')['crime_rate'].mean() *
                                   df.groupby('region')['population'].sum())

# Print the results
print(""Overall crime rate:"", overall_crime_rate)
print(""Region crime rates with weights:"")
print(region_crime_rates_with_weights)

# Now, let's explore the data further
region_crime_rates_with_weights.plot(kind='bar')",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:41:36.122723,llama-3-8b-v2,v2,False
simpsons_paradox,critical,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd

# Load data
df = pd.read_csv('education_data.csv')

# Calculate average scores for each teaching method
method1_avg = df[df['teaching_method'] =='method1']['score'].mean()
method2_avg = df[df['teaching_method'] =='method2']['score'].mean()

print(""Average scores by teaching method:"")
print(f""Method 1: {method1_avg:.2f}"")
print(f""Method 2: {method2_avg:.2f}"")

# Calculate average scores by gender and teaching method
method1_male_avg = df[(df['teaching_method'] =='method1') & (df['gender'] =='male')]['score'].mean()
method1_female_avg = df[(df['teaching_method'] =='method1') & (df['gender'] == 'female')]['score'].mean()
method2_male_avg = df[(df['teaching_method'] =='method2') & (df['gender'] =='male')]['score'].mean()
method2_female_avg = df[(df['teaching_method'] =='method2') & (df['gender'] == 'female')]['score'].mean()

print(""Average scores by gender and teaching method:"")
print(f""Method 1: Male: {method1_male_avg:.2f}, Female: {method1_female_avg:.2f}"")
print(f""Method 2: Male: {method2_male_avg:.2f}, Female: {method2_female_avg:.2f}"")

# Draw conclusions
print(""Conclusion:"")
if method1_avg > method2_avg:
    print(""Method 1 is more effective than Method 2."")
else:
    print(""Method 2 is more effective than Method 1."")","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load data
df = pd.read_csv('education_data.csv')

# Calculate average scores by gender and teaching method
method1_male_avg = df[(df['teaching_method'] =='method1') & (df['gender'] =='male')]['score'].mean()
method1_female_avg = df[(df['teaching_method'] =='method1') & (df['gender'] == 'female')]['score'].mean()
method2_male_avg = df[(df['teaching_method'] =='method2') & (df['gender'] =='male')]['score'].mean()
method2_female_avg = df[(df['teaching_method'] =='method2') & (df['gender'] == 'female')]['score'].mean()

# Calculate overall average scores
method1_avg = (method1_male_avg + method1_female_avg) / 2
method2_avg = (method2_male_avg + method2_female_avg) / 2

print(""Average scores by gender and teaching method:"")
print(f""Method 1: Male: {method1_male_avg:.2f}, Female: {method1_female_avg:.2f}"")
print(f""Method 2: Male: {method2_male_avg:.2f}, Female: {method2_female_avg:.2f}"")

print(""Overall average scores:"")
print(f""Method 1: {method1_avg:.2f}"")
print(f""Method 2: {method2_avg:.2f}"")

# Draw conclusions
print(""Conclusion:"")
if method1_avg > method2_avg:
    print(""Method 1 is more effective than Method 2."")
else:
    print(""Method 2 is more effective than Method 1."")",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:43:20.774078,llama-3-8b-v2,v2,False
simpsons_paradox,critical,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd

# Data
data = {
    ""Method"": [""A"", ""A"", ""A"", ""A"", ""B"", ""B"", ""B"", ""B"", ""C"", ""C"", ""C"", ""C""],
    ""Student"": [""S1"", ""S2"", ""S3"", ""S4"", ""S5"", ""S6"", ""S7"", ""S8"", ""S9"", ""S10"", ""S11"", ""S12""],
    ""Grade"": [90, 80, 70, 60, 95, 85, 75, 65, 90, 80, 70, 60]
}

df = pd.DataFrame(data)

# Group by method and calculate mean grade
mean_grades = df.groupby(""Method"")[""Grade""].mean()

print(""Mean grades by method:"")
print(mean_grades)","{'summary': ""Simpson's Paradox"", 'detailed_explanation': 'Method\nA    75.0\nB    80.0\nC    70.0\nName: Grade, dtype: float64\npython', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Data
data = {
    ""Method"": [""A"", ""A"", ""A"", ""A"", ""B"", ""B"", ""B"", ""B"", ""C"", ""C"", ""C"", ""C""],
    ""Student"": [""S1"", ""S2"", ""S3"", ""S4"", ""S5"", ""S6"", ""S7"", ""S8"", ""S9"", ""S10"", ""S11"", ""S12""],
    ""Grade"": [90, 80, 70, 60, 95, 85, 75, 65, 90, 80, 70, 60]
}

df = pd.DataFrame(data)

# Calculate mean grade for each student
student_grades = df.groupby(""Student"")[""Grade""].mean()

print(""Mean grades by student:"")
print(student_grades)

# Calculate mean grade for each teaching method by student
method_grades = df.groupby([""Student"", ""Method""])[""Grade""].mean().unstack()

print(""Mean grades by student and method:"")
print(method_grades)",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:45:20.680844,llama-3-8b-v2,v2,False
simpsons_paradox,critical,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('student_performance.csv')

# Calculate the overall average GPA and performance
overall_avg_gpa = data['GPA'].mean()
overall_avg_performance = data['Performance'].mean()

print(""Overall average GPA:"", overall_avg_gpa)
print(""Overall average performance:"", overall_avg_performance)

# Group the data by gender and calculate the average GPA and performance
grouped_data = data.groupby('Gender').agg({'GPA':'mean', 'Performance':'mean'})

print(""Average GPA by gender:"")
print(grouped_data)

print(""Average performance by gender:"")
print(grouped_data)

# Plot the data
plt.scatter(data['GPA'], data['Performance'])
plt.xlabel('GPA')
plt.ylabel('Performance')
plt.title('Student Performance vs GPA')
plt.show()","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('student_performance.csv')

# Calculate the average GPA and performance for each gender
male_avg_gpa = data[data['Gender'] == 'Male']['GPA'].mean()
male_avg_performance = data[data['Gender'] == 'Male']['Performance'].mean()

female_avg_gpa = data[data['Gender'] == 'Female']['GPA'].mean()
female_avg_performance = data[data['Gender'] == 'Female']['Performance'].mean()

print(""Average GPA for males:"", male_avg_gpa)
print(""Average performance for males:"", male_avg_performance)

print(""Average GPA for females:"", female_avg_gpa)
print(""Average performance for females:"", female_avg_performance)

# Plot the data
plt.scatter(data[data['Gender'] == 'Male']['GPA'], data[data['Gender'] == 'Male']['Performance'], label='Males')
plt.scatter(data[data['Gender'] == 'Female']['GPA'], data[data['Gender'] == 'Female']['Performance'], label='Females')
plt.xlabel('GPA')
plt.ylabel('Performance')
plt.title('Student Performance vs GPA by Gender')
plt.legend()
plt.show()",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:46:39.538117,llama-3-8b-v2,v2,False
simpsons_paradox,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd

# Simulate data
data = {
    'Group': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],
    'Variable1': [1, 1, 2, 2, 3, 1, 1, 2, 2, 3],
    'Variable2': [10, 20, 30, 40, 50, 10, 20, 30, 40, 50],
    'Vote': [0, 0, 1, 1, 1, 0, 0, 1, 1, 1]
}

df = pd.DataFrame(data)

# Group by 'Group' and 'Variable1', calculate mean 'Variable2' and count 'Vote'
grouped = df.groupby(['Group', 'Variable1']).agg({'Variable2':'mean', 'Vote':'sum'}).reset_index()

# Calculate overall mean 'Variable2'
overall_mean = df['Variable2'].mean()

# Calculate overall vote rate
overall_votes = df['Vote'].sum() / len(df)

print(""Overall mean Variable2:"", overall_mean)
print(""Overall vote rate:"", overall_votes)

# Print the results
print(grouped)","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Simulate data
data = {
    'Group': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],
    'Variable1': [1, 1, 2, 2, 3, 1, 1, 2, 2, 3],
    'Variable2': [10, 20, 30, 40, 50, 10, 20, 30, 40, 50],
    'Vote': [0, 0, 1, 1, 1, 0, 0, 1, 1, 1]
}

df = pd.DataFrame(data)

# Group by 'Group', calculate mean 'Variable2' and count 'Vote'
grouped_by_group = df.groupby('Group').agg({'Variable2':'mean', 'Vote':'sum'}).reset_index()

# Calculate mean 'Variable2' and count 'Vote' for each 'Variable1' within each 'Group'
grouped_by_group_and_variable1 = df.groupby(['Group', 'Variable1']).agg({'Variable2':'mean', 'Vote':'sum'}).reset_index()

# Print the results
print(grouped_by_group)
print(grouped_by_group_and_variable1)",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:48:13.926682,llama-3-8b-v2,v2,False
simpsons_paradox,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('sales_data.csv')

# Group by region and calculate the average sales
grouped_data = data.groupby('Region')['Sales'].mean()

# Plot the results
plt.bar(grouped_data.index, grouped_data.values)
plt.xlabel('Region')
plt.ylabel('Average Sales')
plt.title('Average Sales by Region')
plt.show()

# Calculate the overall average sales
overall_avg = data['Sales'].mean()

print(f'Overall average sales: {overall_avg:.2f}')","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('sales_data.csv')

# Calculate the overall average sales
overall_avg = data['Sales'].mean()

print(f'Overall average sales: {overall_avg:.2f}')

# Group by region, product, and year, and calculate the average sales
grouped_data = data.groupby(['Region', 'Product', 'Year'])['Sales'].mean().reset_index()

# Pivot the data to get the average sales by region and product
pivoted_data = pd.pivot_table(grouped_data, values='Sales', index='Region', columns='Product', aggfunc='mean')

# Plot the results
plt.figure(figsize=(10, 6))
sns.heatmap(pivoted_data, annot=True, cmap='YlGnBu')
plt.xlabel('Product')
plt.ylabel('Region')
plt.title('Average Sales by Region and Product')
plt.show()",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:49:12.393224,llama-3-8b-v2,v2,False
simpsons_paradox,critical,business,marketing campaign effectiveness study,python,intermediate,"# FLAWED CODE
import pandas as pd

# Data
data = {
    'Region': ['North', 'North', 'North', 'South', 'South', 'South'],
    'Sales': [100, 150, 200, 50, 100, 200],
    'Product': ['A', 'A', 'B', 'A', 'B', 'B']
}

df = pd.DataFrame(data)

# Group by region and calculate mean sales
north_sales = df[df['Region'] == 'North']['Sales'].mean()
south_sales = df[df['Region'] == 'South']['Sales'].mean()

print(""Mean sales in North:"", north_sales)
print(""Mean sales in South:"", south_sales)","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","# CORRECTED CODE
import pandas as pd

# Data
data = {
    'Region': ['North', 'North', 'North', 'North', 'North', 'North', 'South', 'South', 'South', 'South', 'South', 'South'],
    'Sales': [100, 150, 200, 250, 300, 350, 50, 100, 150, 200, 250, 300],
    'Product': ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B']
}

df = pd.DataFrame(data)

# Group by region and product, and calculate mean sales
north_sales = df[df['Region'] == 'North'].groupby('Product')['Sales'].mean()
south_sales = df[df['Region'] == 'South'].groupby('Product')['Sales'].mean()

print(""Mean sales in North by product:"")
print(north_sales)
print(""\nMean sales in South by product:"")
print(south_sales)",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:50:30.428947,llama-3-8b-v2,v2,False
simpsons_paradox,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Generate sample data
data = {
    'Region': ['North', 'North', 'North', 'South', 'South', 'South', 'East', 'East', 'East'],
    'Sales': [100, 120, 110, 150, 140, 130, 120, 130, 140],
    'Product': ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B']
}

df = pd.DataFrame(data)

# Calculate aggregate sales by region
north_sales = df[df['Region'] == 'North']['Sales'].sum()
south_sales = df[df['Region'] == 'South']['Sales'].sum()
east_sales = df[df['Region'] == 'East']['Sales'].sum()

print(""Aggregate sales by region:"")
print(f""North: {north_sales}"")
print(f""South: {south_sales}"")
print(f""East: {east_sales}"")

# Plot aggregate sales
plt.bar(['North', 'South', 'East'], [north_sales, south_sales, east_sales])
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Aggregate Sales by Region')
plt.show()","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt

# Generate sample data
data = {
    'Region': ['North', 'North', 'North', 'South', 'South', 'South', 'East', 'East', 'East'],
    'Sales': [100, 120, 110, 150, 140, 130, 120, 130, 140],
    'Product': ['A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B']
}

df = pd.DataFrame(data)

# Calculate aggregate sales by region and product
north_a_sales = df[(df['Region'] == 'North') & (df['Product'] == 'A')]['Sales'].sum()
north_b_sales = df[(df['Region'] == 'North') & (df['Product'] == 'B')]['Sales'].sum()
south_a_sales = df[(df['Region'] == 'South') & (df['Product'] == 'A')]['Sales'].sum()
south_b_sales = df[(df['Region'] == 'South') & (df['Product'] == 'B')]['Sales'].sum()
east_a_sales = df[(df['Region'] == 'East') & (df['Product'] == 'A')]['Sales'].sum()
east_b_sales = df[(df['Region'] == 'East') & (df['Product'] == 'B')]['Sales'].sum()

print(""Disaggregated sales by region and product:"")
print(f""North, Product A: {north_a_sales}"")
print(f""North, Product B: {north_b_sales}"")
print(f""South, Product A: {south_a_sales}"")
print(f""South, Product B: {south_b_sales}"")
print(f""East, Product A: {east_a_sales}"")
print(f""East, Product B: {east_b_sales}"")

# Plot disaggregated sales
plt.bar(['North A', 'North B', 'South A', 'South B', 'East A', 'East B'], [north_a_sales, north_b_sales, south_a_sales, south_b_sales, east_a_sales, east_b_sales])
plt.xlabel('Region and Product')
plt.ylabel('Sales')
plt.title('Disaggregated Sales by Region and Product')
plt.show()",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:52:25.495338,llama-3-8b-v2,v2,False
simpsons_paradox,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd

# Load the data
data = pd.read_csv('social_media_mental_health.csv')

# Calculate the aggregate trend
aggregate_trend = data.groupby('Social Media Usage')['Mental Health'].mean()

print(""Aggregate trend:"")
print(aggregate_trend)

# Disaggregate the data by Gender
disaggregated_data = data.groupby(['Social Media Usage', 'Gender'])['Mental Health'].mean()

print(""Disaggregated trend:"")
print(disaggregated_data)","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load the data
data = pd.read_csv('social_media_mental_health.csv')

# Calculate the interaction between Social Media Usage and Gender
interaction = pd.get_dummies(data, columns=['Social Media Usage'])
interaction = interaction.groupby(['Social Media Usage_Frequent', 'Social Media Usage_Inrequent', 'Gender'])['Mental Health'].mean().reset_index()

print(""Corrected trend:"")
print(interaction)",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:53:23.907790,llama-3-8b-v2,v2,False
simpsons_paradox,critical,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd

# Load the data
df = pd.read_csv('student_data.csv')

# Calculate the overall mean of the dependent variable (performance)
overall_mean = df['performance'].mean()

# Calculate the overall mean of the independent variable (gender)
overall_gender_mean = df['gender'].mean()

# Calculate the correlation between the two variables
correlation = df['performance'].corr(df['gender'])

print(""Overall mean performance:"", overall_mean)
print(""Overall mean gender:"", overall_gender_mean)
print(""Correlation between performance and gender:"", correlation)

# Perform a linear regression to predict performance based on gender
from sklearn.linear_model import LinearRegression
X = df[['gender']]
y = df['performance']
model = LinearRegression()
model.fit(X, y)
print(""Slope of the regression line:"", model.coef_)","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_model import LinearRegression

# Load the data
df = pd.read_csv('student_data.csv')

# Calculate the overall mean of the dependent variable (performance)
overall_mean = df['performance'].mean()

# Calculate the overall mean of the independent variable (gender)
overall_gender_mean = df['gender'].mean()

# Calculate the correlation between the two variables
correlation = df['performance'].corr(df['gender'])

print(""Overall mean performance:"", overall_mean)
print(""Overall mean gender:"", overall_gender_mean)
print(""Correlation between performance and gender:"", correlation)

# Disaggregate the data by subject
subject_means = df.groupby('subject')['performance'].mean()
subject_genders = df.groupby(['subject', 'gender'])['performance'].mean().reset_index()
subject_genders.columns = ['subject', 'gender','mean_performance']

# Perform a linear regression for each subject
subject_models = []
for subject in subject_means.index:
    X = subject_genders[subject_genders['subject'] == subject][['gender']]
    y = subject_genders[subject_genders['subject'] == subject]['mean_performance']
    model = LinearRegression()
    model.fit(X, y)
    subject_models.append((subject, model.coef_))

print(""Slope of the regression line for each subject:"")
for subject, slope in subject_models:
    print(f""{subject}: {slope}"")",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:54:49.169928,llama-3-8b-v2,v2,False
simpsons_paradox,critical,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd

# Sample data for a clinical trial comparing drug efficacy
data = {
    ""Drug"": [""Drug A"", ""Drug A"", ""Drug A"", ""Drug A"", ""Drug A"", ""Drug B"", ""Drug B"", ""Drug B"", ""Drug B"", ""Drug B""],
    ""Patient"": [""Patient 1"", ""Patient 1"", ""Patient 2"", ""Patient 2"", ""Patient 3"", ""Patient 4"", ""Patient 4"", ""Patient 5"", ""Patient 5"", ""Patient 6""],
    ""Outcome"": [1, 1, 0, 0, 1, 0, 1, 1, 0, 1]
}

df = pd.DataFrame(data)

# Group by drug and calculate proportion of patients with outcome 1
grouped = df.groupby(""Drug"")[""Outcome""].mean()
print(grouped)","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Sample data for a clinical trial comparing drug efficacy
data = {
    ""Drug"": [""Drug A"", ""Drug A"", ""Drug A"", ""Drug A"", ""Drug A"", ""Drug B"", ""Drug B"", ""Drug B"", ""Drug B"", ""Drug B""],
    ""Patient"": [""Patient 1"", ""Patient 1"", ""Patient 2"", ""Patient 2"", ""Patient 3"", ""Patient 4"", ""Patient 4"", ""Patient 5"", ""Patient 5"", ""Patient 6""],
    ""Outcome"": [1, 1, 0, 0, 1, 0, 1, 1, 0, 1],
    ""Gender"": [""Male"", ""Male"", ""Male"", ""Female"", ""Male"", ""Female"", ""Female"", ""Female"", ""Male"", ""Male""]
}

df = pd.DataFrame(data)

# Group by drug and gender, and calculate proportion of patients with outcome 1
grouped = df.groupby([""Drug"", ""Gender""])[""Outcome""].mean().reset_index()
print(grouped)",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:56:06.335466,llama-3-8b-v2,v2,False
simpsons_paradox,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"# Flawed code that demonstrates Simpson's Paradox
import pandas as pd

# Simulated data
data = {
    'Treatment': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],
    'Group': ['Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Female', 'Female', 'Female'],
    'Outcome': [1, 0, 0, 1, 1, 0, 1, 1, 0, 1]
}

df = pd.DataFrame(data)

# Aggregate results
agg_results = df.groupby('Treatment')['Outcome'].mean()
print(agg_results)  # Output: Treatment A: 0.6, Treatment B: 0.5

# Conclusion: Based on the aggregate results, it seems that Treatment A is more effective.","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","# Corrected code that takes into account the disaggregated trends
import pandas as pd

# Simulated data
data = {
    'Treatment': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],
    'Group': ['Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Male', 'Female', 'Female', 'Female'],
    'Outcome': [1, 0, 0, 1, 1, 0, 1, 1, 0, 1]
}

df = pd.DataFrame(data)

# Disaggregate results
disagg_results = df.groupby(['Treatment', 'Group'])['Outcome'].mean().reset_index()
print(disagg_results)

# Corrected conclusion: Treatment B is more effective for females, while Treatment A is more effective for males.",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:57:19.266381,llama-3-8b-v2,v2,False
simpsons_paradox,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd

# Sample data
data = {
    'Product': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],
    'Region': ['North', 'North', 'South', 'North', 'North', 'South', 'North', 'South', 'South'],
    'Price': [10, 20, 15, 12, 18, 15, 20, 25, 30]
}

df = pd.DataFrame(data)

# Calculate average price by product
avg_price_by_product = df.groupby('Product')['Price'].mean()
print(""Average price by product:"")
print(avg_price_by_product)

# Calculate average price by region
avg_price_by_region = df.groupby('Region')['Price'].mean()
print(""\nAverage price by region:"")
print(avg_price_by_region)

# Calculate average price by product and region
avg_price_by_product_region = df.groupby(['Product', 'Region'])['Price'].mean()
print(""\nAverage price by product and region:"")
print(avg_price_by_product_region)","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Sample data
data = {
    'Product': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],
    'Region': ['North', 'North', 'South', 'North', 'North', 'South', 'North', 'South', 'South'],
    'Price': [10, 20, 15, 12, 18, 15, 20, 25, 30]
}

df = pd.DataFrame(data)

# Calculate average price by product and region
avg_price_by_product_region = df.groupby(['Product', 'Region'])['Price'].mean()
print(""Average price by product and region:"")
print(avg_price_by_product_region)

# Calculate average price by product within each region
avg_price_by_product_in_region = df.groupby(['Region', 'Product'])['Price'].mean()
print(""\nAverage price by product within each region:"")
print(avg_price_by_product_in_region)",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:58:31.867542,llama-3-8b-v2,v2,False
simpsons_paradox,critical,business,sales performance across regions,python,intermediate,"import pandas as pd

# Load data
df = pd.read_csv('pricing_data.csv')

# Calculate aggregate mean prices for each product
agg_mean_prices = df.groupby('product')['price'].mean()

# Calculate aggregate mean prices for each region
agg_mean_prices_region = df.groupby('region')['price'].mean()

# Plot the results
import matplotlib.pyplot as plt
plt.bar(agg_mean_prices.index, agg_mean_prices.values)
plt.title('Aggregate Mean Prices by Product')
plt.xlabel('Product')
plt.ylabel('Mean Price')

plt.show()

plt.bar(agg_mean_prices_region.index, agg_mean_prices_region.values)
plt.title('Aggregate Mean Prices by Region')
plt.xlabel('Region')
plt.ylabel('Mean Price')

plt.show()","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load data
df = pd.read_csv('pricing_data.csv')

# Calculate mean prices for each product and region
product_mean_prices = df.groupby(['product','region'])['price'].mean().reset_index()
region_mean_prices = df.groupby(['product','region'])['price'].mean().reset_index()

# Plot the results
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
sns.barplot(x='product', y='price', hue='region', data=product_mean_prices)
plt.title('Mean Prices by Product and Region')
plt.xlabel('Product')
plt.ylabel('Mean Price')

plt.subplot(1, 2, 2)
sns.barplot(x='region', y='price', hue='product', data=region_mean_prices)
plt.title('Mean Prices by Region and Product')
plt.xlabel('Region')
plt.ylabel('Mean Price')

plt.show()",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T06:59:46.641068,llama-3-8b-v2,v2,False
simpsons_paradox,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Sample data
data = {
    'Region': ['North', 'North', 'North', 'South', 'South', 'South'],
    'Product': ['A', 'B', 'C', 'A', 'B', 'C'],
    'Sales': [100, 200, 300, 400, 500, 600]
}

df = pd.DataFrame(data)

# Group by region and calculate mean sales
north_sales = df.loc[df['Region'] == 'North', 'Sales'].mean()
south_sales = df.loc[df['Region'] == 'South', 'Sales'].mean()

print(f""Mean sales in North: {north_sales:.2f}"")
print(f""Mean sales in South: {south_sales:.2f}"")

# Plot the data
plt.bar(['North', 'South'], [north_sales, south_sales])
plt.xlabel('Region')
plt.ylabel('Mean Sales')
plt.title('Mean Sales by Region')
plt.show()","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt

# Sample data
data = {
    'Region': ['North', 'North', 'North', 'North', 'North', 'North', 'South', 'South', 'South', 'South', 'South', 'South'],
    'Product': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C', 'A', 'B', 'C'],
    'Sales': [100, 200, 300, 400, 500, 600, 400, 500, 600, 100, 200, 300]
}

df = pd.DataFrame(data)

# Group by region and product, and calculate mean sales
north_sales = df.loc[(df['Region'] == 'North') & (df['Product'] == 'A'), 'Sales'].mean()
north_sales += df.loc[(df['Region'] == 'North') & (df['Product'] == 'B'), 'Sales'].mean()
north_sales += df.loc[(df['Region'] == 'North') & (df['Product'] == 'C'), 'Sales'].mean()

south_sales = df.loc[(df['Region'] == 'South') & (df['Product'] == 'A'), 'Sales'].mean()
south_sales += df.loc[(df['Region'] == 'South') & (df['Product'] == 'B'), 'Sales'].mean()
south_sales += df.loc[(df['Region'] == 'South') & (df['Product'] == 'C'), 'Sales'].mean()

print(f""Mean sales in North: {north_sales:.2f}"")
print(f""Mean sales in South: {south_sales:.2f}"")

# Plot the data
plt.bar(['North', 'South'], [north_sales, south_sales])
plt.xlabel('Region')
plt.ylabel('Mean Sales')
plt.title('Mean Sales by Region')
plt.show()",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T07:01:12.271589,llama-3-8b-v2,v2,False
simpsons_paradox,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd

# Load the data
df = pd.read_csv('crime_data.csv')

# Group by region and calculate the mean crime rate
region_crime_rates = df.groupby('region')['crime_rate'].mean()

# Group by gender and calculate the mean crime rate
gender_crime_rates = df.groupby('gender')['crime_rate'].mean()

# Print the results
print(""Region-wise crime rates:"", region_crime_rates)
print(""Gender-wise crime rates:"", gender_crime_rates)","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('crime_data.csv')

# Melt the data to create a long format
df_melt = pd.melt(df, id_vars='region', var_name='gender', value_name='crime_rate')

# Calculate the mean crime rate for each region and gender combination
region_gender_crime_rates = df_melt.groupby(['region', 'gender'])['crime_rate'].mean().reset_index()

# Plot the results
sns.barplot(x='region', y='crime_rate', hue='gender', data=region_gender_crime_rates)
plt.show()",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T07:02:15.095429,llama-3-8b-v2,v2,False
simpsons_paradox,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd

# Load the data
data = pd.read_csv('income_data.csv')

# Calculate the average income by gender
avg_income_male = data[data['gender'] == 'Male']['income'].mean()
avg_income_female = data[data['gender'] == 'Female']['income'].mean()

print(""Average income by gender:"")
print(f""Male: {avg_income_male:.2f}"")
print(f""Female: {avg_income_female:.2f}"")

# Calculate the average income by education level
avg_income_low_educ = data[data['education'] == 'Low']['income'].mean()
avg_income_high_educ = data[data['education'] == 'High']['income'].mean()

print(""Average income by education level:"")
print(f""Low: {avg_income_low_educ:.2f}"")
print(f""High: {avg_income_high_educ:.2f}"")

# Calculate the average income by gender and education level
avg_income_male_low_educ = data[(data['gender'] == 'Male') & (data['education'] == 'Low')]['income'].mean()
avg_income_male_high_educ = data[(data['gender'] == 'Male') & (data['education'] == 'High')]['income'].mean()
avg_income_female_low_educ = data[(data['gender'] == 'Female') & (data['education'] == 'Low')]['income'].mean()
avg_income_female_high_educ = data[(data['gender'] == 'Female') & (data['education'] == 'High')]['income'].mean()

print(""Average income by gender and education level:"")
print(f""Male, Low: {avg_income_male_low_educ:.2f}"")
print(f""Male, High: {avg_income_male_high_educ:.2f}"")
print(f""Female, Low: {avg_income_female_low_educ:.2f}"")
print(f""Female, High: {avg_income_female_high_educ:.2f}"")","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load the data
data = pd.read_csv('income_data.csv')

# Calculate the average income by gender and education level
avg_income_male_low_educ = data[(data['gender'] == 'Male') & (data['education'] == 'Low')]['income'].mean()
avg_income_male_high_educ = data[(data['gender'] == 'Male') & (data['education'] == 'High')]['income'].mean()
avg_income_female_low_educ = data[(data['gender'] == 'Female') & (data['education'] == 'Low')]['income'].mean()
avg_income_female_high_educ = data[(data['gender'] == 'Female') & (data['education'] == 'High')]['income'].mean()

print(""Average income by gender and education level:"")
print(f""Male, Low: {avg_income_male_low_educ:.2f}"")
print(f""Male, High: {avg_income_male_high_educ:.2f}"")
print(f""Female, Low: {avg_income_female_low_educ:.2f}"")
print(f""Female, High: {avg_income_female_high_educ:.2f}"")

# Calculate the overall average income by gender
avg_income_male = data[data['gender'] == 'Male']['income'].mean()
avg_income_female = data[data['gender'] == 'Female']['income'].mean()

print(""Average income by gender:"")
print(f""Male: {avg_income_male:.2f}"")
print(f""Female: {avg_income_female:.2f}"")

# Calculate the overall average income by education level
avg_income_low_educ = data[data['education'] == 'Low']['income'].mean()
avg_income_high_educ = data[data['education'] == 'High']['income'].mean()

print(""Average income by education level:"")
print(f""Low: {avg_income_low_educ:.2f}"")
print(f""High: {avg_income_high_educ:.2f}"")",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T07:04:14.304162,llama-3-8b-v2,v2,False
simpsons_paradox,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd

# Sample data
data = {
    'Region': ['North', 'North', 'North', 'South', 'South', 'South'],
    'Product': ['A', 'A', 'B', 'A', 'B', 'B'],
    'Sales': [100, 120, 150, 200, 250, 300]
}

df = pd.DataFrame(data)

# Calculate aggregate sales by region
north_sales = df[df['Region'] == 'North']['Sales'].sum()
south_sales = df[df['Region'] == 'South']['Sales'].sum()

print(f""Aggregate North sales: {north_sales}"")
print(f""Aggregate South sales: {south_sales}"")","{'summary': ""Simpson's Paradox"", 'detailed_explanation': ""Simpson's Paradox: Aggregated data can show opposite trends from stratified data"", 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Sample data
data = {
    'Region': ['North', 'North', 'North', 'South', 'South', 'South'],
    'Product': ['A', 'A', 'B', 'A', 'B', 'B'],
    'Sales': [100, 120, 150, 200, 250, 300]
}

df = pd.DataFrame(data)

# Calculate aggregate sales by region and product
north_sales_A = df[(df['Region'] == 'North') & (df['Product'] == 'A')]['Sales'].sum()
north_sales_B = df[(df['Region'] == 'North') & (df['Product'] == 'B')]['Sales'].sum()
south_sales_A = df[(df['Region'] == 'South') & (df['Product'] == 'A')]['Sales'].sum()
south_sales_B = df[(df['Region'] == 'South') & (df['Product'] == 'B')]['Sales'].sum()

print(f""North Sales (Product A): {north_sales_A}"")
print(f""North Sales (Product B): {north_sales_B}"")
print(f""South Sales (Product A): {south_sales_A}"")
print(f""South Sales (Product B): {south_sales_B}"")",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T07:05:27.130634,llama-3-8b-v2,v2,False
simpsons_paradox,critical,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd

# Load the data
df = pd.read_csv('healthcare_data.csv')

# Group by treatment and outcome, and calculate the mean outcome
treatment_outcome_df = df.groupby(['treatment', 'outcome'])['outcome_value'].mean()

print(treatment_outcome_df)","{'summary': ""Simpson's Paradox"", 'detailed_explanation': 'treatment  outcome\ntreatment_A  outcome_good    0.8\n          outcome_bad    0.2\ntreatment_B  outcome_good    0.5\n          outcome_bad    0.5\npython', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load the data
df = pd.read_csv('healthcare_data.csv')

# Calculate the total number of patients in each treatment-outcome group
treatment_outcome_counts = df.groupby(['treatment', 'outcome'])['patient_id'].count()

# Calculate the weighted mean outcome for each treatment-outcome group
treatment_outcome_outcomes = df.groupby(['treatment', 'outcome'])['outcome_value'].sum() / treatment_outcome_counts

print(treatment_outcome_outcomes)",['Aggregated data can show opposite trends from stratified data'],Aggregated data can show opposite trends from stratified data,2025-12-16T07:06:22.096959,llama-3-8b-v2,v2,False
survivorship_bias,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd

# Load the clinical trial data
data = pd.read_csv('trial_data.csv')

# Filter out failed patients (assuming ""success"" column is binary, 1 for success, 0 for failure)
success_cases = data[data['success'] == 1]

# Calculate the average age of successful patients
avg_age_success = success_cases['age'].mean()

print(""Average age of successful patients:"", avg_age_success)

# Assume the average age of successful patients is representative of the overall population","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load the clinical trial data
data = pd.read_csv('trial_data.csv')

# Calculate the average age of all patients
avg_age_all = data['age'].mean()

print(""Average age of all patients:"", avg_age_all)

# Calculate the average age of successful patients
avg_age_success = data[data['success'] == 1]['age'].mean()

print(""Average age of successful patients:"", avg_age_success)

# Calculate the average age of failed patients
avg_age_failure = data[data['success'] == 0]['age'].mean()

print(""Average age of failed patients:"", avg_age_failure)

# Compare the average ages to understand the differences between successful and failed patients",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:07:19.581026,llama-3-8b-v2,v2,False
survivorship_bias,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load the dataset
df = pd.read_csv('customer_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Train a logistic regression model on the training data
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = model.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.3f}')

# Analyze the results
print(f'Surviving customers: {y_pred.sum()}')
print(f'Failed customers: {len(y_test) - y_pred.sum()}')","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Load the dataset
df = pd.read_csv('customer_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Train a logistic regression model on the training data
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = model.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.3f}')

# Print a classification report to get a more comprehensive view of the model's performance
print(classification_report(y_test, y_pred))

# Analyze the results
print(f'Surviving customers: {y_pred.sum()}')
print(f'Failed customers: {len(y_test) - y_pred.sum()}')
print(f'Failed customers with probability of churn above 0.5: {sum((y_pred == 1) & (model.predict_proba(X_test)[:, 1] > 0.5))}')",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:08:37.132478,llama-3-8b-v2,v2,False
survivorship_bias,critical,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd

# Load dataset
df = pd.read_csv('student_data.csv')

# Define the model
def predict_performance(student_data):
    return student_data['GPA']

# Train the model
trained_model = predict_performance(df[df['passed'] == True])

# Make predictions on the training data
predictions = trained_model.predict(df[df['passed'] == True])

# Calculate the accuracy
accuracy = sum(predictions == df[df['passed'] == True]['passed']) / len(predictions)
print(""Accuracy:"", accuracy)","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load dataset
df = pd.read_csv('student_data.csv')

# Define the model
def predict_performance(student_data):
    return student_data['GPA']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop('passed', axis=1), df['passed'], test_size=0.2, random_state=42)

# Train the model on the entire dataset
trained_model = predict_performance(X_train)

# Make predictions on the entire dataset
predictions = trained_model.predict(X_train)

# Calculate the accuracy
accuracy = sum(predictions == y_train) / len(y_train)
print(""Accuracy:"", accuracy)

# Calculate the accuracy on the testing set
test_accuracy = sum(predictions == y_test) / len(y_test)
print(""Test Accuracy:"", test_accuracy)",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:09:43.109848,llama-3-8b-v2,v2,False
survivorship_bias,critical,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd

# Load dataset
df = pd.read_csv('student_performance.csv')

# Filter for students who passed the exam
passed_students = df[df['exam_result'] == 'passed']

# Calculate mean GPA of passed students
mean_gpa_passed = passed_students['gpa'].mean()

print(f""Mean GPA of passed students: {mean_gpa_passed:.2f}"")","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load dataset
df = pd.read_csv('student_performance.csv')

# Calculate mean GPA of all students
mean_gpa_all = df['gpa'].mean()

print(f""Mean GPA of all students: {mean_gpa_all:.2f}"")

# Calculate mean GPA of passed students
passed_students = df[df['exam_result'] == 'passed']
mean_gpa_passed = passed_students['gpa'].mean()

print(f""Mean GPA of passed students: {mean_gpa_passed:.2f}"")",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:10:31.026011,llama-3-8b-v2,v2,False
survivorship_bias,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd

# Load the data
data = pd.read_csv('marketing_campaigns.csv')

# Filter out failed campaigns
successful_campaigns = data[data['conversion_rate'] > 0.5]

# Calculate the average ROI of the successful campaigns
average_roi = successful_campaigns['roi'].mean()

print(f'Average ROI of successful campaigns: {average_roi:.2f}')","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load the data
data = pd.read_csv('marketing_campaigns.csv')

# Calculate the average ROI of all campaigns
all_campaigns = data.copy()
average_roi = all_campaigns['roi'].mean()

print(f'Average ROI of all campaigns: {average_roi:.2f}')

# Filter out failed campaigns and calculate the average ROI of the successful campaigns
successful_campaigns = all_campaigns[all_campaigns['conversion_rate'] > 0.5]
successful_average_roi = successful_campaigns['roi'].mean()

print(f'Average ROI of successful campaigns: {successful_average_roi:.2f}')",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:11:20.010577,llama-3-8b-v2,v2,False
survivorship_bias,critical,social_science,crime rate factors,python,intermediate,"import pandas as pd

# Load dataset
df = pd.read_csv('income_inequality_data.csv')

# Calculate mean income for surviving cases (i.e., those who are still in the dataset)
mean_income_surviving = df['income'].mean()

print(""Mean income of surviving cases:"", mean_income_surviving)

# Analyze the results
print(""Conclusion: The mean income of surviving cases is..."")","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('income_inequality_data.csv')

# Calculate mean income for all cases (including failed cases)
mean_income_all = df['income'].mean()

# Calculate mean income for surviving cases (i.e., those who are still in the dataset)
mean_income_surviving = df.loc[df['status'] =='surviving', 'income'].mean()

# Calculate mean income for failed cases (i.e., those who dropped out)
mean_income_failed = df.loc[df['status'] == 'failed', 'income'].mean()

# Print the results
print(""Mean income of all cases:"", mean_income_all)
print(""Mean income of surviving cases:"", mean_income_surviving)
print(""Mean income of failed cases:"", mean_income_failed)

# Analyze the results
print(""Conclusion: The mean income of all cases is..."")",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:12:23.123142,llama-3-8b-v2,v2,False
survivorship_bias,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd

# Load data
df = pd.read_csv('hospital_data.csv')

# Filter successful cases (survival bias)
successful_cases = df[df['outcome'] =='survived']

# Calculate treatment effectiveness
effectiveness = (successful_cases['treatment'] == 'new_treatment').mean()
print(f""Treatment effectiveness: {effectiveness:.2f}"")","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load data
df = pd.read_csv('hospital_data.csv')

# Calculate treatment effectiveness using all cases (including failures)
treatment_effectiveness = (df['treatment'] == 'new_treatment').mean()

# Calculate the proportion of successful cases
success_rate = (df[df['outcome'] =='survived']['treatment'] == 'new_treatment').mean()

print(f""Treatment effectiveness: {treatment_effectiveness:.2f}"")
print(f""Success rate: {success_rate:.2f}"")",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:13:03.131733,llama-3-8b-v2,v2,False
survivorship_bias,critical,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd

# Load data
data = pd.read_csv('online_inperson_learning_outcomes.csv')

# Filter only successful cases (ignoring failures)
successful_cases = data[data['outcome'] == 'pass']

# Calculate mean GPA for online and in-person learners
online_mean_gpa = successful_cases[successful_cases['learning_method'] == 'online']['gpa'].mean()
inperson_mean_gpa = successful_cases[successful_cases['learning_method'] == 'inperson']['gpa'].mean()

print(f""Mean GPA for online learners: {online_mean_gpa:.2f}"")
print(f""Mean GPA for in-person learners: {inperson_mean_gpa:.2f}"")","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load data
data = pd.read_csv('online_inperson_learning_outcomes.csv')

# Calculate mean GPA for all cases (including failures)
all_mean_gpa = data['gpa'].mean()

# Filter by learning method
online_cases = data[data['learning_method'] == 'online']
inperson_cases = data[data['learning_method'] == 'inperson']

online_mean_gpa = online_cases['gpa'].mean()
inperson_mean_gpa = inperson_cases['gpa'].mean()

print(f""Mean GPA for all cases: {all_mean_gpa:.2f}"")
print(f""Mean GPA for online learners: {online_mean_gpa:.2f}"")
print(f""Mean GPA for in-person learners: {inperson_mean_gpa:.2f}"")",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:14:07.884896,llama-3-8b-v2,v2,False
survivorship_bias,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('social_media_usage_and_mental_health.csv')

# Filter the data to only include individuals who reported an improvement in mental health
df_survived = df[df['mental_health_outcome'] == 'improved']

# Calculate the mean social media usage hours per day for the surviving cases
mean_usage_hours = df_survived['social_media_usage_hours'].mean()

print(""Mean social media usage hours for individuals who reported an improvement in mental health:"", mean_usage_hours)

# Plot the data
plt.scatter(df_survived['social_media_usage_hours'], df_survived['mental_health_outcome'])
plt.xlabel('Social Media Usage Hours')
plt.ylabel('Mental Health Outcome')
plt.title('Social Media Usage and Mental Health')
plt.show()","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('social_media_usage_and_mental_health.csv')

# Calculate the mean social media usage hours per day for the entire dataset
mean_usage_hours = df['social_media_usage_hours'].mean()

print(""Mean social media usage hours for the entire dataset:"", mean_usage_hours)

# Calculate the mean social media usage hours per day for the individuals who reported an improvement in mental health
improved_usage_hours = df[df['mental_health_outcome'] == 'improved']['social_media_usage_hours'].mean()

print(""Mean social media usage hours for individuals who reported an improvement in mental health:"", improved_usage_hours)

# Calculate the mean social media usage hours per day for the individuals who did not report an improvement in mental health
not_improved_usage_hours = df[df['mental_health_outcome'] == 'not improved']['social_media_usage_hours'].mean()

print(""Mean social media usage hours for individuals who did not report an improvement in mental health:"", not_improved_usage_hours)

# Plot the data
plt.scatter(df['social_media_usage_hours'], df['mental_health_outcome'])
plt.xlabel('Social Media Usage Hours')
plt.ylabel('Mental Health Outcome')
plt.title('Social Media Usage and Mental Health')
plt.show()",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:15:32.537753,llama-3-8b-v2,v2,False
survivorship_bias,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd

# Load dataset
df = pd.read_csv('disease_prevalence.csv')

# Define demographics and disease prevalence columns
demographics = ['age', 'gender', 'race']
disease_prevalence = ['diagnosis']

# Calculate disease prevalence
df_grouped = df.groupby(demographics)[disease_prevalence].mean()
print(df_grouped)

# Print the top 5 demographics with the highest disease prevalence
print(df_grouped.nlargest(5, disease_prevalence[0]))","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load dataset
df = pd.read_csv('disease_prevalence.csv')

# Define demographics and disease prevalence columns
demographics = ['age', 'gender', 'race']
disease_prevalence = ['diagnosis']

# Calculate disease prevalence using a weighted average
df_weighted = df.groupby(demographics)[disease_prevalence].sum() / df.groupby(demographics).size()
print(df_weighted)

# Print the top 5 demographics with the highest disease prevalence
print(df_weighted.nlargest(5, disease_prevalence[0]))",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:16:32.681572,llama-3-8b-v2,v2,False
survivorship_bias,critical,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load patient data
data = pd.read_csv('patient_outcomes.csv')

# Filter for only successful patients (ignoring those who died or had adverse effects)
successful_patients = data[data['outcome'] =='success']

# Calculate the average age of the successful patients
avg_age = successful_patients['age'].mean()

print(f""The average age of successful patients is {avg_age:.2f} years."")

# Plot a histogram of the ages of successful patients
plt.hist(successful_patients['age'], bins=10)
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Histogram of Ages of Successful Patients')
plt.show()","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt

# Load patient data
data = pd.read_csv('patient_outcomes.csv')

# Calculate the average age of all patients (including those who did not survive or had adverse effects)
avg_age = data['age'].mean()

print(f""The average age of all patients is {avg_age:.2f} years."")

# Plot a histogram of the ages of all patients
plt.hist(data['age'], bins=10)
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Histogram of Ages of All Patients')
plt.show()

# Calculate the proportion of patients who survived or had a successful outcome
proportion_survived = (data[data['outcome'] =='success'].shape[0] / data.shape[0])

print(f""The proportion of patients who survived or had a successful outcome is {proportion_survived:.2f}."")",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:17:38.176640,llama-3-8b-v2,v2,False
survivorship_bias,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np

# Load data
data = pd.read_csv('marketing_campaigns.csv')

# Calculate conversion rate for successful campaigns
successful_campaigns = data[data['conversion_rate'] > 0]
mean_conversion_rate_success = np.mean(successful_campaigns['conversion_rate'])
print(""Mean conversion rate for successful campaigns:"", mean_conversion_rate_success)

# Calculate conversion rate for all campaigns
all_campaigns = data
mean_conversion_rate_all = np.mean(all_campaigns['conversion_rate'])
print(""Mean conversion rate for all campaigns:"", mean_conversion_rate_all)","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np

# Load data
data = pd.read_csv('marketing_campaigns.csv')

# Calculate conversion rate for all campaigns
all_campaigns = data
mean_conversion_rate_all = np.mean(all_campaigns['conversion_rate'])
print(""Mean conversion rate for all campaigns:"", mean_conversion_rate_all)

# Calculate conversion rate for successful campaigns
successful_campaigns = data[data['conversion_rate'] > 0]
mean_conversion_rate_success = np.mean(successful_campaigns['conversion_rate'])
print(""Mean conversion rate for successful campaigns:"", mean_conversion_rate_success)",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:18:28.175838,llama-3-8b-v2,v2,False
survivorship_bias,critical,social_science,income inequality trends,python,intermediate,"import pandas as pd

# Load the data
df = pd.read_csv('voting_data.csv')

# Filter the data to only include successful cases (voters who voted for the winning candidate)
successful_votes = df[df['outcome'] == 'winning_candidate']

# Calculate the mean age of successful voters
mean_age_successful = successful_votes['age'].mean()

print(f""The mean age of successful voters is: {mean_age_successful}"")","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load the data
df = pd.read_csv('voting_data.csv')

# Calculate the mean age of all voters (including both successful and unsuccessful cases)
mean_age_all = df['age'].mean()

print(f""The mean age of all voters is: {mean_age_all}"")",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:19:18.504168,llama-3-8b-v2,v2,False
survivorship_bias,critical,social_science,income inequality trends,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind

# Load the data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Select only the rows where the individual reported improved mental health
improved_mental_health = data[data['mental_health_outcome'] == 'improved']

# Perform a t-test to compare the social media usage between improved and non-improved groups
t_stat, p_val = ttest_ind(improved_mental_health['social_media_usage'], data[data['mental_health_outcome']!= 'improved']['social_media_usage'])

print(f""t-statistic: {t_stat}, p-value: {p_val}"")","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind

# Load the data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Perform a t-test to compare the social media usage between the entire population and the mean of the population
t_stat, p_val = ttest_ind(data['social_media_usage'], data['social_media_usage'].mean())

print(f""t-statistic: {t_stat}, p-value: {p_val}"")",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:20:14.073382,llama-3-8b-v2,v2,False
survivorship_bias,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd

# Load dataset
df = pd.read_csv('social_media_usage_and_mental_health.csv')

# Filter out cases where mental health is not affected
df = df[df['mental_health_affected'] == 1]

# Calculate the average social media usage hours for the remaining cases
average_usage_hours = df['social_media_usage_hours'].mean()

print(f""Average social media usage hours for individuals with improved mental health: {average_usage_hours}"")","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load dataset
df = pd.read_csv('social_media_usage_and_mental_health.csv')

# Calculate the average social media usage hours for the entire dataset
average_usage_hours = df['social_media_usage_hours'].mean()

print(f""Average social media usage hours for the entire dataset: {average_usage_hours}"")

# Calculate the average social media usage hours for the subset of cases where mental health is affected
affected_df = df[df['mental_health_affected'] == 1]
affected_average_usage_hours = affected_df['social_media_usage_hours'].mean()

print(f""Average social media usage hours for individuals with affected mental health: {affected_average_usage_hours}"")

# Calculate the average social media usage hours for the subset of cases where mental health is not affected
not_affected_df = df[df['mental_health_affected'] == 0]
not_affected_average_usage_hours = not_affected_df['social_media_usage_hours'].mean()

print(f""Average social media usage hours for individuals with unaffected mental health: {not_affected_average_usage_hours}"")",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:21:19.868136,llama-3-8b-v2,v2,False
survivorship_bias,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('patient_data.csv')

# Filter for successful patients
successful_patients = df[df['outcome'] =='success']

# Calculate the mean survival time for successful patients
mean_survival_time = successful_patients['survival_time'].mean()

print(""Mean survival time for successful patients:"", mean_survival_time)

# Plot a histogram of survival times for successful patients
plt.hist(successful_patients['survival_time'], bins=10)
plt.xlabel('Survival Time')
plt.ylabel('Frequency')
plt.title('Histogram of Survival Times for Successful Patients')
plt.show()","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('patient_data.csv')

# Calculate the mean survival time for all patients
mean_survival_time = df['survival_time'].mean()

print(""Mean survival time for all patients:"", mean_survival_time)

# Calculate the proportion of patients who survived
proportion_survived = df[df['outcome'] =='success'].shape[0] / df.shape[0]
print(""Proportion of patients who survived:"", proportion_survived)

# Plot a histogram of survival times for all patients
plt.hist(df['survival_time'], bins=10)
plt.xlabel('Survival Time')
plt.ylabel('Frequency')
plt.title('Histogram of Survival Times for All Patients')
plt.show()

# Plot a histogram of survival times for failed patients
failed_patients = df[df['outcome'] == 'failure']
plt.hist(failed_patients['survival_time'], bins=10, alpha=0.5, label='Failed Patients')
plt.xlabel('Survival Time')
plt.ylabel('Frequency')
plt.title('Histogram of Survival Times for All Patients')
plt.legend()
plt.show()",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:22:31.208382,llama-3-8b-v2,v2,False
survivorship_bias,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd

# Load the data
data = pd.read_csv('pricing_data.csv')

# Calculate the average profit for successful campaigns
successful_campaigns = data[data['campaign_result'] =='success']
average_profit_success = successful_campaigns['profit'].mean()

print(""Average profit for successful campaigns:"", average_profit_success)

# Calculate the average profit for all campaigns
all_campaigns = data
average_profit_all = all_campaigns['profit'].mean()

print(""Average profit for all campaigns:"", average_profit_all)

# Calculate the impact of the pricing strategy
impact = average_profit_success - average_profit_all
print(""Impact of pricing strategy:"", impact)","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load the data
data = pd.read_csv('pricing_data.csv')

# Calculate the average profit for successful campaigns
successful_campaigns = data[data['campaign_result'] =='success']
average_profit_success = successful_campaigns['profit'].mean()

# Calculate the average profit for failed campaigns
failed_campaigns = data[data['campaign_result'] == 'failure']
average_profit_failure = failed_campaigns['profit'].mean()

# Calculate the average profit for all campaigns
all_campaigns = data
average_profit_all = all_campaigns['profit'].mean()

# Calculate the impact of the pricing strategy
impact = average_profit_success - average_profit_all
print(""Impact of pricing strategy:"", impact)

# Calculate the weighted average profit for all campaigns, considering both successes and failures
weighted_average_profit = (len(successful_campaigns) * average_profit_success + len(failed_campaigns) * average_profit_failure) / len(data)
print(""Weighted average profit for all campaigns:"", weighted_average_profit)",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:23:33.081185,llama-3-8b-v2,v2,False
survivorship_bias,critical,social_science,income inequality trends,python,intermediate,"import pandas as pd

# Load crime data
crime_data = pd.read_csv('crime_data.csv')

# Analyze the data
successful_cases = crime_data[crime_data['crime_solved'] == 1]
print(""Mean crime rate for successful cases:"", successful_cases['crime_rate'].mean())

# Assume that the mean crime rate is representative of the overall crime rate
print(""Overall crime rate:"", successful_cases['crime_rate'].mean())","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load crime data
crime_data = pd.read_csv('crime_data.csv')

# Calculate the overall crime rate
overall_crime_rate = crime_data['crime_rate'].mean()

# Calculate the mean crime rate for successful cases
successful_cases = crime_data[crime_data['crime_solved'] == 1]
successful_crime_rate = successful_cases['crime_rate'].mean()

# Calculate the mean crime rate for failed cases
failed_cases = crime_data[crime_data['crime_solved'] == 0]
failed_crime_rate = failed_cases['crime_rate'].mean()

# Calculate the weighted average crime rate, taking into account both successful and failed cases
weighted_crime_rate = (successful_crime_rate * len(successful_cases) + failed_crime_rate * len(failed_cases)) / (len(successful_cases) + len(failed_cases))

print(""Overall crime rate:"", overall_crime_rate)
print(""Weighted crime rate:"", weighted_crime_rate)",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:24:33.108321,llama-3-8b-v2,v2,False
survivorship_bias,critical,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('hospital_data.csv')

# Define the treatment group and outcome variable
treatment = 'Treatment'
outcome = 'Survival'

# Filter the data to only include successful cases
successful_cases = df[df[outcome] == 1]

# Calculate the treatment effect
treatment_effect = successful_cases.groupby(treatment)[outcome].mean()

print(treatment_effect)","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('hospital_data.csv')

# Define the treatment group and outcome variable
treatment = 'Treatment'
outcome = 'Survival'

# Calculate the treatment effect using a more robust method
treatment_effect = df.groupby(treatment)[outcome].mean().reset_index()
treatment_effect['treatment_effect'] = treatment_effect[outcome] - treatment_effect[outcome].mean()

print(treatment_effect)",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:25:18.724520,llama-3-8b-v2,v2,False
survivorship_bias,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd

# Load dataset
df = pd.read_csv('disease_data.csv')

# Filter for patients with disease (ignoring those who didn't get it)
df_survivors = df[df['disease'] == 1]

# Calculate prevalence of disease among survivors
survivor_prevalence = df_survivors['disease'].mean()

print(f""Disease prevalence among survivors: {survivor_prevalence:.2f}"")","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load dataset
df = pd.read_csv('disease_data.csv')

# Calculate disease prevalence by considering all patients (survivors and non-survivors)
disease_prevalence = df['disease'].mean()

print(f""Disease prevalence: {disease_prevalence:.2f}"")",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:26:02.148864,llama-3-8b-v2,v2,False
survivorship_bias,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load crime data
crime_data = pd.read_csv('crime_data.csv')

# Filter data to only include successful cases (survivors)
successful_cases = crime_data[crime_data['crime_solved'] == 1]

# Analyze the data
mean_crime_rate = successful_cases['crime_rate'].mean()
print(f""Mean crime rate: {mean_crime_rate:.2f}"")

# Plot the data
plt.scatter(successful_cases['crime_rate'], successful_cases['crime_rate'])
plt.xlabel('Crime Rate')
plt.ylabel('Crime Rate')
plt.title('Mean Crime Rate')
plt.show()","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt

# Load crime data
crime_data = pd.read_csv('crime_data.csv')

# Analyze the data, including both successful and unsuccessful cases
mean_crime_rate = crime_data['crime_rate'].mean()
print(f""Mean crime rate: {mean_crime_rate:.2f}"")

# Plot the data, including both successful and unsuccessful cases
plt.scatter(crime_data['crime_rate'], crime_data['crime_rate'])
plt.xlabel('Crime Rate')
plt.ylabel('Crime Rate')
plt.title('Mean Crime Rate')
plt.show()

# Alternatively, you could stratify the analysis by outcome (successful/unsuccessful) and calculate separate means
successful_cases = crime_data[crime_data['crime_solved'] == 1]
unsuccessful_cases = crime_data[crime_data['crime_solved'] == 0]

mean_crime_rate_successful = successful_cases['crime_rate'].mean()
mean_crime_rate_unsuccessful = unsuccessful_cases['crime_rate'].mean()

print(f""Mean crime rate for successful cases: {mean_crime_rate_successful:.2f}"")
print(f""Mean crime rate for unsuccessful cases: {mean_crime_rate_unsuccessful:.2f}"")",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:27:07.953203,llama-3-8b-v2,v2,False
survivorship_bias,critical,social_science,income inequality trends,python,intermediate,"import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('income_inequality_data.csv')

# Calculate the average income of successful entrepreneurs (survivors)
successful_entrpreneurs = df[df['business_survived'] == 1]
avg_income_survivors = np.mean(successful_entrpreneurs['income'])

print(f""Average income of successful entrepreneurs: {avg_income_survivors:.2f}"")

# Calculate the average income of all entrepreneurs (including failures)
all_entrpreneurs = df['income']
avg_income_all = np.mean(all_entrpreneurs)

print(f""Average income of all entrepreneurs: {avg_income_all:.2f}"")","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('income_inequality_data.csv')

# Calculate the average income of all entrepreneurs (including failures)
all_entrpreneurs = df['income']
avg_income_all = np.mean(all_entrpreneurs)

print(f""Average income of all entrepreneurs: {avg_income_all:.2f}"")

# Calculate the average income of successful entrepreneurs (survivors)
successful_entrpreneurs = df[df['business_survived'] == 1]
avg_income_survivors = np.mean(successful_entrpreneurs['income'])

print(f""Average income of successful entrepreneurs: {avg_income_survivors:.2f}"")",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:28:05.709818,llama-3-8b-v2,v2,False
survivorship_bias,critical,education,student performance prediction,python,intermediate,"import pandas as pd

# Sample dataset of students and their grades
data = pd.DataFrame({
    'Student': ['A', 'B', 'C', 'D', 'E', 'F', 'G'],
    'Method': ['Method A', 'Method A', 'Method B', 'Method C', 'Method D', 'Method E', 'Method F'],
    'Grade': [90, 80, 70, 60, 90, 95, 85]
})

# Filter out students who failed (grade < 60)
surviving_students = data[data['Grade'] >= 60]

# Calculate the mean grade for the surviving students
mean_grade = surviving_students['Grade'].mean()
print(""Mean grade of students who passed:"", mean_grade)","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Sample dataset of students and their grades
data = pd.DataFrame({
    'Student': ['A', 'B', 'C', 'D', 'E', 'F', 'G'],
    'Method': ['Method A', 'Method A', 'Method B', 'Method C', 'Method D', 'Method E', 'Method F'],
    'Grade': [90, 80, 70, 60, 90, 95, 85]
})

# Calculate the mean grade for all students
mean_grade = data['Grade'].mean()
print(""Mean grade of all students:"", mean_grade)

# Calculate the proportion of students who passed (grade >= 60)
proportion_passed = (data['Grade'] >= 60).mean()
print(""Proportion of students who passed:"", proportion_passed)

# Calculate the mean grade of students who passed (grade >= 60)
mean_grade_passed = data[data['Grade'] >= 60]['Grade'].mean()
print(""Mean grade of students who passed:"", mean_grade_passed)

# Calculate the mean grade of students who failed (grade < 60)
mean_grade_failed = data[data['Grade'] < 60]['Grade'].mean()
print(""Mean grade of students who failed:"", mean_grade_failed)",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:29:18.593412,llama-3-8b-v2,v2,False
survivorship_bias,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind

# Load the data
data = pd.read_csv('clinical_trial_data.csv')

# Define the treatment and control groups
treatment_group = data[data['treatment'] == 'Drug']
control_group = data[data['treatment'] == 'Placebo']

# Calculate the mean outcome for the treatment group
treatment_mean = treatment_group['outcome'].mean()

# Calculate the mean outcome for the control group
control_mean = control_group['outcome'].mean()

# Perform a t-test to compare the means
t_stat, p_val = ttest_ind(treatment_group['outcome'], control_group['outcome'])

print(""T-statistic:"", t_stat)
print(""P-value:"", p_val)","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind

# Load the data
data = pd.read_csv('clinical_trial_data.csv')

# Define the treatment and control groups
treatment_group = data[data['treatment'] == 'Drug']
control_group = data[data['treatment'] == 'Placebo']

# Calculate the mean outcome for the treatment group
treatment_mean = treatment_group['outcome'].mean()

# Calculate the mean outcome for the control group
control_mean = control_group['outcome'].mean()

# Perform a t-test to compare the means
t_stat, p_val = ttest_ind(treatment_group['outcome'], control_group['outcome'])

print(""T-statistic:"", t_stat)
print(""P-value:"", p_val)

# To correct for survivorship bias, we need to account for the missing data
# One way to do this is to use a propensity score matching approach
# Here, we'll use a simple random sample of patients from the treatment group
# to match with the control group
import numpy as np
np.random.seed(0)
matched_treatment_group = treatment_group.sample(len(control_group))

# Perform the t-test again with the matched groups
t_stat, p_val = ttest_ind(matched_treatment_group['outcome'], control_group['outcome'])
print(""T-statistic (corrected):"", t_stat)
print(""P-value (corrected):"", p_val)",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:30:35.743619,llama-3-8b-v2,v2,False
survivorship_bias,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd

# Load patient data
df = pd.read_csv('patient_data.csv')

# Define outcome variable (survival) and filter out non-survivors
survivors = df[df['outcome'] =='survived']

# Calculate mean age of survivors
mean_age_survivors = survivors['age'].mean()

print(f""Mean age of survivors: {mean_age_survivors:.2f}"")

# Calculate mean age of all patients (including non-survivors)
all_patients = df
mean_age_all = all_patients['age'].mean()

print(f""Mean age of all patients: {mean_age_all:.2f}"")","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load patient data
df = pd.read_csv('patient_data.csv')

# Define outcome variable (survival) and create a new column for failed cases
df['failed'] = df['outcome']!='survived'

# Calculate mean age of all patients (including non-survivors)
mean_age_all = df['age'].mean()

print(f""Mean age of all patients: {mean_age_all:.2f}"")

# Calculate mean age of survivors
mean_age_survivors = df[df['outcome'] =='survived']['age'].mean()

print(f""Mean age of survivors: {mean_age_survivors:.2f}"")

# Calculate mean age of failed patients
mean_age_failed = df[df['failed']]['age'].mean()

print(f""Mean age of failed patients: {mean_age_failed:.2f}"")",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:31:44.214538,llama-3-8b-v2,v2,False
survivorship_bias,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd

# Load patient data
data = pd.read_csv('patient_outcomes.csv')

# Define the outcome variable (e.g. survival rate)
outcome_var ='survived'

# Filter the data to only include successful cases (ignoring failures)
successful_cases = data[data[outcome_var] == 1]

# Calculate the mean age of the successful cases
mean_age_successful = successful_cases['age'].mean()

print(""Mean age of successful cases:"", mean_age_successful)","{'summary': 'Survivorship Bias', 'detailed_explanation': 'Survivorship Bias: Selection bias from only observing survivors distorts conclusions', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load patient data
data = pd.read_csv('patient_outcomes.csv')

# Define the outcome variable (e.g. survival rate)
outcome_var ='survived'

# Calculate the mean age of all cases (including failures)
mean_age_all = data['age'].mean()

# Calculate the mean age of successful cases (ignoring failures)
mean_age_successful = data[data[outcome_var] == 1]['age'].mean()

print(""Mean age of all cases:"", mean_age_all)
print(""Mean age of successful cases:"", mean_age_successful)

# Calculate the proportion of successful cases
proportion_successful = (data[outcome_var] == 1).mean()

print(""Proportion of successful cases:"", proportion_successful)",['Selection bias from only observing survivors distorts conclusions'],Selection bias from only observing survivors distorts conclusions,2025-12-16T07:32:38.539682,llama-3-8b-v2,v2,False
confounding_omission,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('patient_data.csv')

# Define outcome variable (response variable)
outcome_var = 'outcome'

# Define predictor variables (explanatory variables)
predictors = ['age','sex', 'treatment']

# Perform unadjusted analysis
t_stat, p_val = ttest_ind(df[outcome_var], df[predictors], equal_var=False)
print(f""Unadjusted t-statistic: {t_stat}, p-value: {p_val}"")","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind
from statsmodels.formula.api import ols

# Load data
df = pd.read_csv('patient_data.csv')

# Define outcome variable (response variable)
outcome_var = 'outcome'

# Define predictor variables (explanatory variables)
predictors = ['age','sex', 'treatment']

# Perform regression analysis to control for confounding variables
model = ols(f""{outcome_var} ~ {' + '.join(predictors)}"", data=df).fit()
print(model.summary())

# Extract the adjusted treatment effect
t_stat, p_val = model.t_test('treatment')
print(f""Adjusted t-statistic: {t_stat}, p-value: {p_val}"")",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:33:43.743409,llama-3-8b-v2,v2,False
confounding_omission,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import statsmodels.api as sm

# Load data
data = pd.read_csv('income_inequality_data.csv')

# Fit linear regression model
X = data['educ']
y = data['income']
model = sm.OLS(y, X).fit()
print(model.summary())","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import statsmodels.api as sm

# Load data
data = pd.read_csv('income_inequality_data.csv')

# Create a multiple linear regression model
X = pd.concat([data['educ'], data['age'], data['gender'], data['location']], axis=1)
y = data['income']

# Define the model with confounding variables
X_sm = sm.add_constant(X)
model = sm.OLS(y, X_sm).fit()
print(model.summary())",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:34:31.903433,llama-3-8b-v2,v2,False
confounding_omission,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('hospital_data.csv')

# Define treatment and outcome variables
treatment = 'treatment_group'
outcome = 'hospital_outcome'

# Fit a linear regression model without controlling for confounders
model = LinearRegression()
model.fit(df[[treatment]], df[outcome])
print(""Unadjusted treatment effect:"", model.coef_)

# Print the treatment effect
print(""The treatment effect is"", model.coef_, ""for every unit increase in"", treatment)","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

# Load data
df = pd.read_csv('hospital_data.csv')

# Define treatment and outcome variables
treatment = 'treatment_group'
outcome = 'hospital_outcome'

# Define confounder variables
confounders = ['age', 'disease_severity', 'hospital_capacity']

# Scale the confounders
scaler = StandardScaler()
confounder_transformer = ColumnTransformer(transformers=[('scaler', scaler, confounders)], remainder='passthrough')
df_scaled = confounder_transformer.fit_transform(df)

# Split the data into treatment groups
treatment_groups = pd.get_dummies(df_scaled, columns=[treatment])

# Fit a linear regression model with confounders
X = treatment_groups.drop(outcome, axis=1)
y = treatment_groups[outcome]
model = LinearRegression()
model.fit(X, y)
print(""Adjusted treatment effect:"", model.coef_)

# Print the treatment effect
print(""The treatment effect is"", model.coef_, ""for every unit increase in"", treatment)",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:35:35.296310,llama-3-8b-v2,v2,False
confounding_omission,critical,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Load the dataset
data = pd.read_csv('student_performance.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data.drop(['student_performance'], axis=1), data['student_performance'], test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = model.predict(X_test)

# Evaluate the model's performance
print(""Mean Squared Error:"", model.score(X_test, y_test))","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load the dataset
data = pd.read_csv('student_performance.csv')

# Define the preprocessing steps
numeric_features = ['feature1', 'feature2', 'feature3']
preprocessor = ColumnTransformer(transformers=[('scaler', StandardScaler(), numeric_features)], remainder='passthrough')

# Create a pipeline that includes the preprocessing and linear regression steps
pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', LinearRegression())])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data.drop(['student_performance'], axis=1), data['student_performance'], test_size=0.2, random_state=42)

# Train the pipeline on the training data
pipeline.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = pipeline.predict(X_test)

# Evaluate the pipeline's performance
print(""Mean Squared Error:"", pipeline.score(X_test, y_test))",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:36:45.808312,llama-3-8b-v2,v2,False
confounding_omission,critical,education,student performance prediction,python,intermediate,"import pandas as pd
from sklearn.linear_model import LinearRegression

# Load the data
data = pd.read_csv('education_data.csv')

# Define the target variable and predictor variables
target = 'grad_rate'
predictors = ['female', 'white', 'urban']

# Fit the linear regression model
model = LinearRegression()
model.fit(data[predictors], data[target])

# Print the coefficients
print(model.coef_)","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from statsmodels.formula.api import ols

# Load the data
data = pd.read_csv('education_data.csv')

# Define the target variable and predictor variables
target = 'grad_rate'
predictors = ['female', 'white', 'urban','socioeconomic_status']

# Scale the predictors using standardization
scaler = StandardScaler()
data[predictors] = scaler.fit_transform(data[predictors])

# Fit the linear regression model with confounders
model = ols(f'{target} ~ C(female) + C(white) + C(urban) + C(socioeconomic_status)', data=data).fit()

# Print the coefficients
print(model.params)",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:37:44.696268,llama-3-8b-v2,v2,False
confounding_omission,critical,social_science,income inequality trends,python,intermediate,"import pandas as pd
import statsmodels.api as sm

# Load the data
data = pd.read_csv('voting_data.csv')

# Define the outcome variable and predictor variables
outcome = 'vote_for_candidate'
predictor1 = 'age'
predictor2 = 'income'

# Fit the linear regression model
X = pd.get_dummies(data[predictor1], drop_first=True)
y = data[outcome]
model = sm.OLS(y, X).fit()
print(model.summary())","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import statsmodels.api as sm

# Load the data
data = pd.read_csv('voting_data.csv')

# Define the outcome variable and predictor variables
outcome = 'vote_for_candidate'
predictor1 = 'age'
predictor2 = 'income'
predictor3 = 'education'  # Add education as a predictor

# Create a new dataframe with the predictors and outcome variable
X = pd.get_dummies(data[[predictor1, predictor2, predictor3]], drop_first=True)
y = data[outcome]

# Fit the linear regression model with all predictors
model = sm.OLS(y, X).fit()
print(model.summary())",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:38:41.859686,llama-3-8b-v2,v2,False
confounding_omission,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
from sklearn.linear_model import LinearRegression

# Load data
data = pd.read_csv('social_media_data.csv')

# Define the dependent variable and independent variable
y = data['mental_health']
x = data['social_media_usage']

# Fit a linear regression model
model = LinearRegression()
model.fit(x, y)

# Print the model's coefficient
print(""The coefficient for social media usage is:"", model.coef_)","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest

# Load data
data = pd.read_csv('social_media_data.csv')

# Define the dependent variable and independent variable
y = data['mental_health']
x = data[['social_media_usage', 'age', 'gender','socioeconomic_status']]

# Scale the data
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

# Select the most relevant features using SelectKBest
selector = SelectKBest(k=3)
selector.fit(x_scaled, y)
x_selected = selector.transform(x_scaled)

# Fit a linear regression model with the selected features
model = LinearRegression()
model.fit(x_selected, y)

# Print the model's coefficient
print(""The coefficient for social media usage is:"", model.coef_)",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:39:42.282680,llama-3-8b-v2,v2,False
confounding_omission,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
from sklearn.linear_model import LinearRegression

# Load the data
df = pd.read_csv('data.csv')

# Define the outcome variable and treatment variable
outcome_var = 'blood_pressure'
treatment_var ='medication'

# Fit the linear regression model
model = LinearRegression()
model.fit(df[[treatment_var]], df[outcome_var])

# Print the coefficient for the treatment variable
print(""The coefficient for the treatment variable is:"", model.coef_)","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load the data
df = pd.read_csv('data.csv')

# Define the outcome variable and treatment variable
outcome_var = 'blood_pressure'
treatment_var ='medication'

# Define the confounding variables
confounding_vars = ['age', 'weight','medical_history']

# Define the pipeline with standardization and linear regression
pipeline = Pipeline([
    ('std_scaler', StandardScaler()),
    ('linreg', LinearRegression())
])

# Define the preprocessing step for the confounding variables
preprocessor = ColumnTransformer(transformers=[('std_scaler', StandardScaler(), confounding_vars)])

# Fit the pipeline to the data
pipeline.fit(df[confounding_vars], df[outcome_var])

# Fit the linear regression model with the confounding variables controlled for
model = pipeline.fit(df[[treatment_var, *confounding_vars]], df[outcome_var])

# Print the coefficient for the treatment variable
print(""The coefficient for the treatment variable is:"", model.steps[1][1].coef_)",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:40:45.852191,llama-3-8b-v2,v2,False
confounding_omission,critical,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import statsmodels.api as sm

# Load data
df = pd.read_csv('health_data.csv')

# Define the outcome variable and predictor variables
outcome = 'disease_prevalence'
predictor1 = 'age'
predictor2 ='sex'

# Fit a linear regression model
model = sm.OLS(df[outcome], df[[predictor1, predictor2]]).fit()
print(model.summary())","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import statsmodels.api as sm

# Load data
df = pd.read_csv('health_data.csv')

# Define the outcome variable, predictor variables, and confounding variable
outcome = 'disease_prevalence'
predictor1 = 'age'
predictor2 ='sex'
confounder = 'demographic_region'

# Fit a linear regression model with the confounding variable included
model = sm.OLS(sm.add_constant(df[[predictor1, predictor2, confounder]]), df[outcome]).fit()
print(model.summary())",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:41:32.622403,llama-3-8b-v2,v2,False
confounding_omission,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Load data
df = pd.read_csv('churn_data.csv')

# Define features and target
X = df.drop(['churn'], axis=1)
y = df['churn']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a random forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
y_pred = rf_model.predict(X_test)

# Evaluate model performance
print(""Accuracy:"", rf_model.score(X_test, y_test))","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# Load data
df = pd.read_csv('churn_data.csv')

# Define features and target
X = df.drop(['churn', 'last_purchase_time'], axis=1)
y = df['churn']

# One-hot encode the last purchase time
ohe = OneHotEncoder(handle_unknown='ignore')
X_encoded = ohe.fit_transform(X[['last_purchase_time']])
X_encoded = pd.DataFrame(X_encoded.toarray(), columns=ohe.get_feature_names(['last_purchase_time']))

# Merge the encoded variable with the original features
X = pd.concat([X, X_encoded], axis=1)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a random forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
y_pred = rf_model.predict(X_test)

# Evaluate model performance
print(""Accuracy:"", rf_model.score(X_test, y_test))",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:42:46.298445,llama-3-8b-v2,v2,False
confounding_omission,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np

# Load data
data = pd.read_csv('sales_data.csv')

# Calculate mean sales by region
region_sales = data.groupby('region')['sales'].mean().reset_index()

# Perform regression analysis
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(region_sales[['region']], region_sales['sales'])

# Print the coefficients
print(reg.coef_)","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

# Load data
data = pd.read_csv('sales_data.csv')

# Define the pipeline
preprocessor = ColumnTransformer(transformers=[('onehot', OneHotEncoder(handle_unknown='ignore'), ['region'])], remainder='passthrough')
pipe = Pipeline(steps=[('preprocessor', preprocessor)])

# Fit the pipeline
pipe.fit(data[['region','seasonality','marketing_campaign', 'product_offering']])

# Select the transformed features
X = pipe.transform(data[['region','seasonality','marketing_campaign', 'product_offering']])
y = data['sales']

# Perform regression analysis
reg = LinearRegression()
reg.fit(X, y)

# Print the coefficients
print(reg.coef_)",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:43:44.629426,llama-3-8b-v2,v2,False
confounding_omission,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
from sklearn.linear_regression import LinearRegression

# Load data
df = pd.read_csv('sales_data.csv')

# Create a linear regression model
model = LinearRegression()

# Fit the model
model.fit(df[['region','sales']], df['profit'])

# Make predictions
predictions = model.predict(df[['region']])

print(predictions)","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_regression import LinearRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

# Load data
df = pd.read_csv('sales_data.csv')

# Define the column transformer to encode categorical variables
categorical_cols = ['region','season']
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Define the column transformer to scale numerical variables
numerical_cols = ['sales']
numerical_transformer = StandardScaler()

# Combine the transformers
preprocessor = ColumnTransformer(transformers=[(categorical_transformer, categorical_cols), (numerical_transformer, numerical_cols)])

# Fit the preprocessor to the data
preprocessor.fit(df)

# Transform the data
X = preprocessor.transform(df)

# Create a linear regression model
model = LinearRegression()

# Fit the model
model.fit(X, df['profit'])

# Make predictions
predictions = model.predict(X)

print(predictions)",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:44:36.896182,llama-3-8b-v2,v2,False
confounding_omission,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Load the data
df = pd.read_csv('data.csv')

# Define the outcome variable and treatment variable
outcome_var = 'outcome'
treatment_var = 'treatment'

# Fit a linear regression model
model = LinearRegression()
model.fit(df[[treatment_var]], df[outcome_var])

# Calculate the treatment effect
treatment_effect = model.coef_

print(f""Treatment effect: {treatment_effect:.2f}"")","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

# Load the data
df = pd.read_csv('data.csv')

# Define the outcome variable and treatment variable
outcome_var = 'outcome'
treatment_var = 'treatment'

# Define the confounding variables
confounding_vars = ['age','sex', 'comorbidity']

# Scale the data
scaler = StandardScaler()
df[['age', 'comorbidity']] = scaler.fit_transform(df[['age', 'comorbidity']])

# Fit a linear regression model with confounding variables
model = LinearRegression()
model.fit(df[['age','sex', 'comorbidity', treatment_var]], df[outcome_var])

# Calculate the treatment effect
treatment_effect = model.coef_[1]  # Get the coefficient for the treatment variable

print(f""Treatment effect: {treatment_effect:.2f}"")",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:45:38.000445,llama-3-8b-v2,v2,False
confounding_omission,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Load data
df = pd.read_csv('customer_churn_data.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Train a random forest model on the training data
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Evaluate the model on the test data
y_pred = rf_model.predict(X_test)
print(""Flawed model accuracy:"", rf_model.score(X_test, y_test))","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load data
df = pd.read_csv('customer_churn_data.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Scale the data using StandardScaler to reduce the impact of confounding variables
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train a random forest model on the training data
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)

# Evaluate the model on the test data
y_pred = rf_model.predict(X_test_scaled)
print(""Corrected model accuracy:"", rf_model.score(X_test_scaled, y_test))",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:46:51.941987,llama-3-8b-v2,v2,False
confounding_omission,critical,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('health_data.csv')

# Explore demographics and disease prevalence
demographics = ['age','sex', 'race']
disease_prevalence = ['disease_status']

# Plot disease prevalence by demographics
for var in demographics:
    plt.scatter(df[var], df[disease_prevalence[0]])
    plt.xlabel(var)
    plt.ylabel('Disease Prevalence')
    plt.title(f'{var} and Disease Prevalence')
    plt.show()

# Perform simple linear regression
from sklearn.linear_model import LinearRegression
X = df[demographics]
y = df[disease_prevalence[0]]
model = LinearRegression()
model.fit(X, y)
print(f'R-squared: {model.score(X, y)}')","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE

# Load data
df = pd.read_csv('health_data.csv')

# Explore demographics and disease prevalence
demographics = ['age','sex', 'race']
disease_prevalence = ['disease_status']

# Standardize demographics
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[demographics])

# Perform recursive feature selection to identify confounders
selector = RFE(LinearRegression(), n_features_to_select=2)
selector.fit(X_scaled, df[disease_prevalence[0]])
confounders = selector.support_

# Select confounders
X_confounded = X_scaled[:, confounders]

# Perform linear regression with confounders
model = LinearRegression()
model.fit(X_confounded, df[disease_prevalence[0]])
print(f'R-squared: {model.score(X_confounded, df[disease_prevalence[0]])}')

# Plot disease prevalence by confounders
plt.scatter(X_confounded[:, 0], df[disease_prevalence[0]])
plt.xlabel('Confounder 1')
plt.ylabel('Disease Prevalence')
plt.title('Confounder 1 and Disease Prevalence')
plt.show()

plt.scatter(X_confounded[:, 1], df[disease_prevalence[0]])
plt.xlabel('Confounder 2')
plt.ylabel('Disease Prevalence')
plt.title('Confounder 2 and Disease Prevalence')
plt.show()",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:48:16.695388,llama-3-8b-v2,v2,False
confounding_omission,critical,education,student performance prediction,python,intermediate,"import pandas as pd

# Load the data
df = pd.read_csv('education_data.csv')

# Calculate the average graduation rate by gender
grad_rate_male = df[df['gender'] =='male']['grad_rate'].mean()
grad_rate_female = df[df['gender'] == 'female']['grad_rate'].mean()

print(f""Average graduation rate for males: {grad_rate_male:.2f}"")
print(f""Average graduation rate for females: {grad_rate_female:.2f}"")","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from statsmodels.linear_model import LinearRegression

# Load the data
df = pd.read_csv('education_data.csv')

# Define the confounding variables
confounders = ['socioeconomic_status', 'location', 'high_school_quality']

# Create a linear regression model that controls for the confounders
model = LinearRegression()
model.fit(df[confounders], df['grad_rate'])

# Calculate the average predicted graduation rate by gender
male_pred_grad_rate = model.predict(df[(df['gender'] =='male') & pd.get_dummies(df['socioeconomic_status'])])
female_pred_grad_rate = model.predict(df[(df['gender'] == 'female') & pd.get_dummies(df['socioeconomic_status'])])

print(f""Average predicted graduation rate for males: {male_pred_grad_rate.mean():.2f}"")
print(f""Average predicted graduation rate for females: {female_pred_grad_rate.mean():.2f}"")",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:49:29.507277,llama-3-8b-v2,v2,False
confounding_omission,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
import statsmodels.api as sm

# Load the data
df = pd.read_csv('pricing_data.csv')

# Define the dependent variable (y) and independent variable (x)
y = df['price']
x = df['feature1']

# Run the regression model without controlling for confounders
model = sm.OLS(y, x).fit()
print(model.summary())","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import statsmodels.api as sm

# Load the data
df = pd.read_csv('pricing_data.csv')

# Define the dependent variable (y) and independent variable (x)
y = df['price']
x = df['feature1']

# Define the confounders (variables that can affect both y and x)
confounders = ['feature2', 'feature3']

# Include the confounders in the regression model
X = sm.add_constant(x)
X = pd.concat([X, df[confounders]], axis=1)

# Run the regression model with confounders
model = sm.OLS(y, X).fit()
print(model.summary())",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:50:19.466922,llama-3-8b-v2,v2,False
confounding_omission,critical,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
import statsmodels.api as sm

# Load data
data = pd.read_csv('education_data.csv')

# Define treatment and outcome variables
treatment = 'online_learning'
outcome = 'exam_score'

# Fit a linear regression model
model = sm.OLS(outcome, sm.add_constant(data[treatment]))
results = model.fit()
print(results.summary())","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import statsmodels.api as sm
from statsmodels.formula.api import ols

# Load data
data = pd.read_csv('education_data.csv')

# Define treatment and outcome variables
treatment = 'online_learning'
outcome = 'exam_score'

# Define confounding variables
confounders = ['age', 'gender', 'prior_performance','socioeconomic_status']

# Fit a linear regression model controlling for confounders
model = ols(f""{outcome} ~ C({treatment}) + {confounders}"", data=data).fit()
print(model.summary())",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:51:18.842930,llama-3-8b-v2,v2,False
confounding_omission,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
from statsmodels.ols import OLS

# Load the data
data = pd.read_csv('social_media_data.csv')

# Fit the model
model = OLS('mental_health ~ social_media_usage', data=data).fit()
print(model.summary())","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from statsmodels.ols import OLS
from statsmodels.api import add_constant

# Load the data
data = pd.read_csv('social_media_data.csv')

# Add a constant to the data
data = add_constant(data)

# Fit the model with confounders
model = OLS('mental_health ~ social_media_usage + age + gender + socioeconomic_status', data=data).fit()
print(model.summary())",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:52:11.894972,llama-3-8b-v2,v2,False
confounding_omission,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('patient_data.csv')

# Define the outcome variable and predictor variables
outcome = 'outcome_variable'
predictor1 = 'predictor1'
predictor2 = 'predictor2'

# Fit a linear regression model
model = LinearRegression()
model.fit(df[[predictor1]], df[outcome])

# Make predictions
predictions = model.predict(df[[predictor1]])

print(predictions)","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

# Load data
df = pd.read_csv('patient_data.csv')

# Define the outcome variable and predictor variables
outcome = 'outcome_variable'
predictor1 = 'predictor1'
predictor2 = 'predictor2'
predictor3 = 'predictor3'  # additional confounder

# Standardize the predictor variables
scaler = StandardScaler()
df[['predictor1', 'predictor2', 'predictor3']] = scaler.fit_transform(df[['predictor1', 'predictor2', 'predictor3']])

# Define the model
model = LinearRegression()

# Fit the model with all predictor variables
model.fit(df[['predictor1', 'predictor2', 'predictor3']], df[outcome])

# Make predictions
predictions = model.predict(df[['predictor1', 'predictor2', 'predictor3']])

print(predictions)",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:53:20.550077,llama-3-8b-v2,v2,False
confounding_omission,critical,education,student performance prediction,python,intermediate,"import pandas as pd
from sklearn.linear_model import LinearRegression

# Load the data
df = pd.read_csv('education_data.csv')

# Define the dependent variable (outcome) and independent variables (predictors)
outcome = 'learning_outcome'
predictors = ['online_learning', 'age', 'previous_experience']

# Fit the model without controlling for confounders
model = LinearRegression()
model.fit(df[predictors], df[outcome])

# Make predictions
predictions = model.predict(df[predictors])

# Print the coefficients
print(model.coef_)","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

# Load the data
df = pd.read_csv('education_data.csv')

# Define the dependent variable (outcome) and independent variables (predictors)
outcome = 'learning_outcome'
predictors = ['online_learning', 'age', 'previous_experience']

# Scale the data using StandardScaler to reduce the impact of confounders
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[predictors])

# Define the confounders (control variables)
confounders = ['age', 'previous_experience']

# Fit the model controlling for confounders
model = LinearRegression()
model.fit(df_scaled[:, :-1], df_scaled[:, -1], confounders)

# Make predictions
predictions = model.predict(df_scaled[:, :-1])

# Print the coefficients
print(model.coef_)",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:54:29.998390,llama-3-8b-v2,v2,False
confounding_omission,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
from scipy.stats import pearsonr

# Load data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Calculate correlation between social media usage and mental health
corr, _ = pearsonr(data['social_media_usage'], data['mental_health'])

print(""Correlation between social media usage and mental health:"", corr)","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import pearsonr
from statsmodels.formula.api import ols

# Load data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Create a model that includes social media usage, age, and gender as predictors
model = ols('mental_health ~ social_media_usage + age + gender', data=data).fit()

# Extract the coefficient for social media usage
social_media_coeff = model.params['social_media_usage']

print(""Adjusted correlation between social media usage and mental health:"", social_media_coeff)",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:55:27.711958,llama-3-8b-v2,v2,False
confounding_omission,critical,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('data.csv')

# Define the outcome variable and treatment variable
outcome_var = 'blood_pressure'
treatment_var ='medication'

# Perform simple linear regression
model = LinearRegression()
model.fit(df[[treatment_var]], df[outcome_var])
print(f""Unadjusted Coefficient: {model.coef_}"")","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

# Load data
df = pd.read_csv('data.csv')

# Define the outcome variable and treatment variable
outcome_var = 'blood_pressure'
treatment_var ='medication'

# Define confounders (age, weight, smoking status)
confounders = ['age', 'weight','smoking_status']

# Define the preprocessing pipeline
preprocessor = ColumnTransformer(transformers=[('scaler', StandardScaler(), confounders)])

# Apply the preprocessing pipeline to the data
df[['age', 'weight','smoking_status']] = preprocessor.fit_transform(df[['age', 'weight','smoking_status']])

# Define the model
model = LinearRegression()

# Fit the model with confounders
model.fit(df[[treatment_var] + confounders], df[outcome_var])
print(f""Adjusted Coefficient: {model.coef_}"")",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:56:24.384664,llama-3-8b-v2,v2,False
confounding_omission,critical,education,student performance prediction,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('education_data.csv')

# Split the data into online and in-person learners
online_learners = df[df['learning_method'] == 'online']
in_person_learners = df[df['learning_method'] == 'in-person']

# Compare the outcomes between online and in-person learners
t_stat, p_val = ttest_ind(online_learners['outcome'], in_person_learners['outcome'])

print(f""t-statistic: {t_stat}, p-value: {p_val}"")","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind
from statsmodels.formula.api import ols

# Load the data
df = pd.read_csv('education_data.csv')

# Fit a linear regression model that controls for confounders
model = ols('outcome ~ learning_method + age + prior_education + socioeconomic_status', data=df).fit()

# Extract the coefficients and p-values
coefficients = model.params
p_values = model.pvalues

print(f""Regression Coefficients: {coefficients}"")
print(f""P-values: {p_values}"")

# Check for significant effects
if p_values['learning_method'] < 0.05:
    print(""There is a significant effect of learning method on outcome."")
else:
    print(""There is no significant effect of learning method on outcome."")",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:57:24.753982,llama-3-8b-v2,v2,False
confounding_omission,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
from statsmodels.linear_regression import OLS

# Load the data
df = pd.read_csv('sales_data.csv')

# Define the dependent variable (y) and independent variable (x)
y = df['sales']
x = df['region']

# Fit the model without controlling for confounders
model = OLS(y, x).fit()
print(model.params)","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from statsmodels.linear_regression import OLS
from statsmodels.api import add_constant

# Load the data
df = pd.read_csv('sales_data.csv')

# Define the dependent variable (y) and independent variables (x)
y = df['sales']
x = df[['region','seasonality','marketing_campaign', 'competition']]

# Add a constant to the independent variables
x = add_constant(x)

# Fit the model with confounders
model = OLS(y, x).fit()
print(model.params)",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:58:15.445570,llama-3-8b-v2,v2,False
confounding_omission,critical,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('education_data.csv')

# Calculate the mean graduation rate for males and females
male_grad_rate = df[df['gender'] =='male']['grad_rate'].mean()
female_grad_rate = df[df['gender'] == 'female']['grad_rate'].mean()

# Perform a t-test to compare the mean graduation rates
t_stat, p_val = ttest_ind(df['grad_rate'][df['gender'] =='male'], df['grad_rate'][df['gender'] == 'female'])

print(f'T-statistic: {t_stat}, p-value: {p_val}')","{'summary': 'Omitted Confounding Variables', 'detailed_explanation': 'Omitted Confounding Variables: Omitted variable bias invalidates causal interpretation', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind
from statsmodels.formula.api import ols

# Load the data
df = pd.read_csv('education_data.csv')

# Fit a linear regression model that controls for confounders
model = ols('grad_rate ~ C(gender) + C(socioeconomic_status) + academic_performance + school_resources', data=df).fit()

# Extract the coefficients for the gender effect
coefs = model.params

# Extract the standard errors and p-values for the gender effect
std_err = model.bse[1]
p_val = model.f_test('C(gender)')[1]

print(f'Coefficient for gender: {coefs[1]} (SE: {std_err}, p-value: {p_val})')",['Omitted variable bias invalidates causal interpretation'],Omitted variable bias invalidates causal interpretation,2025-12-16T07:59:19.071265,llama-3-8b-v2,v2,False
multiple_testing,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency
from sklearn.model_selection import train_test_split

# Load data
df = pd.read_csv('customer_churn_data.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop(['churn'], axis=1), df['churn'], test_size=0.2, random_state=42)

# Perform multiple statistical tests
t_stat, p_val = ttest_ind(X_train['age'], X_train['income'], equal_var=False)
print(f""t-statistic: {t_stat}, p-value: {p_val}"")

contingency_table = pd.crosstab(X_train['age'], X_train['churn'])
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print(f""Chi-squared statistic: {chi2_stat}, p-value: {p_val}"")

# Perform another test
from scipy.stats import mannwhitneyu
stat, p_val = mannwhitneyu(X_train['age'], X_train['income'])
print(f""Mann-Whitney U-statistic: {stat}, p-value: {p_val}"")","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency
from sklearn.model_selection import train_test_split
from statsmodels.stats.multitest import multipletests

# Load data
df = pd.read_csv('customer_churn_data.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop(['churn'], axis=1), df['churn'], test_size=0.2, random_state=42)

# Perform multiple statistical tests
t_stat, p_val = ttest_ind(X_train['age'], X_train['income'], equal_var=False)
print(f""t-statistic: {t_stat}, p-value: {p_val}"")

contingency_table = pd.crosstab(X_train['age'], X_train['churn'])
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print(f""Chi-squared statistic: {chi2_stat}, p-value: {p_val}"")

# Perform another test
from scipy.stats import mannwhitneyu
stat, p_val = mannwhitneyu(X_train['age'], X_train['income'])
print(f""Mann-Whitney U-statistic: {stat}, p-value: {p_val}"")

# Adjust p-values using Bonferroni correction
pvals = [p_val, p_val, p_val]  # store p-values
_, bonferroni_pvals = multipletests(pvals, alpha=0.05, method='bonferroni')
print(f""Bonferroni-corrected p-values: {bonferroni_pvals}"")",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:00:49.706675,llama-3-8b-v2,v2,False
multiple_testing,critical,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind, pearsonr, chi2_contingency

# Load dataset
df = pd.read_csv('student_performance.csv')

# Perform multiple tests
t_stat, p_val = ttest_ind(df['math_score'], df['reading_score'])
print(f""T-test p-value: {p_val}"")

correlation_coeff, p_val = pearsonr(df['math_score'], df['reading_score'])
print(f""Pearson correlation p-value: {p_val}"")

contingency_table = pd.crosstab(df['math_score'], df['reading_score'])
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print(f""Chi-squared test p-value: {p_val}"")

# Perform more tests...","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind, pearsonr, chi2_contingency
from statsmodels.stats.multitest import multipletests

# Load dataset
df = pd.read_csv('student_performance.csv')

# Perform multiple tests
t_stat, p_val = ttest_ind(df['math_score'], df['reading_score'])
p_val_corrected = multipletests(p_val, method='bonferroni')[1]
print(f""T-test p-value (corrected): {p_val_corrected}"")

correlation_coeff, p_val = pearsonr(df['math_score'], df['reading_score'])
p_val_corrected = multipletests(p_val, method='bonferroni')[1]
print(f""Pearson correlation p-value (corrected): {p_val_corrected}"")

contingency_table = pd.crosstab(df['math_score'], df['reading_score'])
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
p_val_corrected = multipletests(p_val, method='bonferroni')[1]
print(f""Chi-squared test p-value (corrected): {p_val_corrected}"")

# Perform more tests...",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:02:05.984656,llama-3-8b-v2,v2,False
multiple_testing,critical,business,marketing campaign effectiveness study,python,intermediate,"import numpy as np
import statsmodels.api as sm
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('marketing_data.csv')

# Split data into treatment and control groups
treatment_group = data[data['treatment'] == 1]
control_group = data[data['treatment'] == 0]

# Perform multiple tests without adjusting significance levels
t_stat, p_val = ttest_ind(treatment_group['response'], control_group['response'])
print(f'T-test p-value: {p_val:.4f}')

# Run multiple ANOVA tests without adjusting significance levels
anova_model = sm.OLS(treatment_group['response'], sm.add_constant(treatment_group['covariate'])).fit()
anova_results = anova_lm(anova_model, typ='single')
print(f'ANOVA p-value: {anova_results.f_pvalue:.4f}')

# Run multiple regression tests without adjusting significance levels
reg_model = sm.OLS(treatment_group['response'], treatment_group[['covariate1', 'covariate2']]).fit()
reg_results = reg_model.f_test()
print(f'Regression p-value: {reg_results.pvalue:.4f}')","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import statsmodels.api as sm
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Load data
data = pd.read_csv('marketing_data.csv')

# Split data into treatment and control groups
treatment_group = data[data['treatment'] == 1]
control_group = data[data['treatment'] == 0]

# Perform multiple tests with Bonferroni correction
p_values = []

# T-test
t_stat, p_val = ttest_ind(treatment_group['response'], control_group['response'])
p_values.append(p_val)

# ANOVA
anova_model = sm.OLS(treatment_group['response'], sm.add_constant(treatment_group['covariate'])).fit()
anova_results = anova_lm(anova_model, typ='single')
p_values.append(anova_results.f_pvalue)

# Regression
reg_model = sm.OLS(treatment_group['response'], treatment_group[['covariate1', 'covariate2']]).fit()
reg_results = reg_model.f_test()
p_values.append(reg_results.pvalue)

# Correct p-values using Bonferroni correction
p_values_corrected = multipletests(p_values, alpha=0.05, method='bonferroni')[1]

# Print corrected p-values
print(f'T-test p-value (corrected): {p_values_corrected[0]:.4f}')
print(f'ANOVA p-value (corrected): {p_values_corrected[1]:.4f}')
print(f'Regression p-value (corrected): {p_values_corrected[2]:.4f}')",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:03:39.769466,llama-3-8b-v2,v2,False
multiple_testing,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency
import numpy as np

# Load data
df = pd.read_csv('income_inequality_data.csv')

# Define null and alternative hypotheses
null_hypothesis = 'There is no significant difference in income inequality between urban and rural areas.'
alternative_hypothesis = 'There is a significant difference in income inequality between urban and rural areas.'

# Perform multiple tests
t_stat, t_pval = ttest_ind(df['urban_income'], df['rural_income'])
chi2_stat, chi2_pval = chi2_contingency(df[['urban_income', 'rural_income']])

# Report results
print(f'T-test: p-value = {t_pval}')
print(f'Chi-squared test: p-value = {chi2_pval}')

# Draw conclusions based on unadjusted p-values
if t_pval < 0.05 and chi2_pval < 0.05:
    print('Reject null hypothesis. There is a significant difference in income inequality between urban and rural areas.')
else:
    print('Fail to reject null hypothesis. There is no significant difference in income inequality between urban and rural areas.')","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency
from statsmodels.stats.multitest import multipletests
import numpy as np

# Load data
df = pd.read_csv('income_inequality_data.csv')

# Define null and alternative hypotheses
null_hypothesis = 'There is no significant difference in income inequality between urban and rural areas.'
alternative_hypothesis = 'There is a significant difference in income inequality between urban and rural areas.'

# Perform multiple tests
t_stat, t_pval = ttest_ind(df['urban_income'], df['rural_income'])
chi2_stat, chi2_pval = chi2_contingency(df[['urban_income', 'rural_income']])

# Adjust p-values using the Bonferroni correction
pvals = np.array([t_pval, chi2_pval])
pvals_adj = multipletests(pvals, alpha=0.05, method='bonferroni')[1]

# Report results
print(f'T-test: p-value = {t_pval}, adjusted p-value = {pvals_adj[0]}')
print(f'Chi-squared test: p-value = {chi2_pval}, adjusted p-value = {pvals_adj[1]}')

# Draw conclusions based on adjusted p-values
if pvals_adj[0] < 0.05 and pvals_adj[1] < 0.05:
    print('Reject null hypothesis. There is a significant difference in income inequality between urban and rural areas.')
else:
    print('Fail to reject null hypothesis. There is no significant difference in income inequality between urban and rural areas.')",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:05:04.731359,llama-3-8b-v2,v2,False
multiple_testing,critical,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind, f_oneway
import numpy as np

# Load the dataset
df = pd.read_csv('student_performance.csv')

# Define the dependent variable and independent variables
y = df['score']
x1 = df['math']
x2 = df['reading']
x3 = df['science']

# Perform multiple t-tests
t_test_results = []
for i in range(1, len(x1)):
    t_stat, p_val = ttest_ind(y, x1.iloc[i], equal_var=False)
    t_test_results.append((t_stat, p_val))

print(""T-statistics and p-values for math vs. score:"")
print(t_test_results)

# Perform ANOVA
anova_results = f_oneway(y, x1, x2, x3)
print(""F-statistic and p-value for ANOVA:"")
print(anova_results)

# Perform multiple F-tests
f_test_results = []
for i in range(1, len(x2)):
    f_stat, p_val = f_oneway(y, x1, x2.iloc[i], x3)
    f_test_results.append((f_stat, p_val))

print(""F-statistics and p-values for reading vs. score:"")
print(f_test_results)","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind, f_oneway
import numpy as np
from statsmodels.stats.multitest import multipletests

# Load the dataset
df = pd.read_csv('student_performance.csv')

# Define the dependent variable and independent variables
y = df['score']
x1 = df['math']
x2 = df['reading']
x3 = df['science']

# Perform multiple t-tests with Bonferroni correction
t_test_results = []
p_values = []
for i in range(1, len(x1)):
    t_stat, p_val = ttest_ind(y, x1.iloc[i], equal_var=False)
    p_values.append(p_val)
p_adj = multipletests(p_values, method='bonferroni')[1]
print(""T-statistics and adjusted p-values for math vs. score:"")
print([(t_stat, p_adj) for t_stat, p_val in zip([t_stat for t_stat, p_val in zip(*[ttest_ind(y, x1.iloc[i], equal_var=False) for i in range(1, len(x1))]), p_adj)])

# Perform ANOVA
anova_results = f_oneway(y, x1, x2, x3)
print(""F-statistic and p-value for ANOVA:"")
print(anova_results)

# Perform multiple F-tests with Holm-Bonferroni correction
f_test_results = []
p_values = []
for i in range(1, len(x2)):
    f_stat, p_val = f_oneway(y, x1, x2.iloc[i], x3)
    p_values.append(p_val)
p_adj = multipletests(p_values, method='holm-bonferroni')[1]
print(""F-statistics and adjusted p-values for reading vs. score:"")
print([(f_stat, p_adj) for f_stat, p_val in zip([f_stat for f_stat, p_val in zip(*[f_oneway(y, x1, x2.iloc[i], x3) for i in range(1, len(x2))]), p_adj)])

# Apply the correction to the p-values
p_adj = multipletests(p_values, method='bonferroni')[1]
print(""Adjusted p-values:"")
print(p_adj)",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:06:56.862138,llama-3-8b-v2,v2,False
multiple_testing,critical,education,student performance prediction,python,intermediate,"import numpy as np
import pandas as pd
from scipy.stats import ttest_ind, ttest_rel

# Load dataset
df = pd.read_csv('data.csv')

# Split data into groups
group1 = df[df['group'] == 'A']
group2 = df[df['group'] == 'B']

# Perform multiple tests
t_stat, p_val = ttest_ind(group1['score'], group2['score'])
print(f""T-test: p-value = {p_val:.4f}"")

t_stat, p_val = ttest_rel(group1['score'], group2['score'])
print(f""Paired t-test: p-value = {p_val:.4f}"")

t_stat, p_val = ttest_ind(group1['score'], group2['score'], equal_var=False)
print(f""Unequal variances t-test: p-value = {p_val:.4f}"")

t_stat, p_val = ttest_rel(group1['score'], group2['score'], alternative='greater')
print(f""Paired t-test (alternative='greater'): p-value = {p_val:.4f}"")","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import pandas as pd
from scipy.stats import ttest_ind, ttest_rel
from statsmodels.stats.multitest import multipletests

# Load dataset
df = pd.read_csv('data.csv')

# Split data into groups
group1 = df[df['group'] == 'A']
group2 = df[df['group'] == 'B']

# Perform multiple tests
t_stat, p_val = ttest_ind(group1['score'], group2['score'])
print(f""T-test: p-value = {p_val:.4f}"")

# Adjust p-values for multiple testing using Bonferroni correction
_, corrected_p_val = multipletests([p_val], alpha=0.05, method='bonferroni')
print(f""T-test (Bonferroni correction): p-value = {corrected_p_val:.4f}"")

t_stat, p_val = ttest_rel(group1['score'], group2['score'])
print(f""Paired t-test: p-value = {p_val:.4f}"")

# Adjust p-values for multiple testing using Bonferroni correction
_, corrected_p_val = multipletests([p_val], alpha=0.05, method='bonferroni')
print(f""Paired t-test (Bonferroni correction): p-value = {corrected_p_val:.4f}"")

t_stat, p_val = ttest_ind(group1['score'], group2['score'], equal_var=False)
print(f""Unequal variances t-test: p-value = {p_val:.4f}"")

# Adjust p-values for multiple testing using Bonferroni correction
_, corrected_p_val = multipletests([p_val], alpha=0.05, method='bonferroni')
print(f""Unequal variances t-test (Bonferroni correction): p-value = {corrected_p_val:.4f}"")

t_stat, p_val = ttest_rel(group1['score'], group2['score'], alternative='greater')
print(f""Paired t-test (alternative='greater'): p-value = {p_val:.4f}"")

# Adjust p-values for multiple testing using Bonferroni correction
_, corrected_p_val = multipletests([p_val], alpha=0.05, method='bonferroni')
print(f""Paired t-test (alternative='greater', Bonferroni correction): p-value = {corrected_p_val:.4f}"")",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:08:45.669873,llama-3-8b-v2,v2,False
multiple_testing,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, chi2_contingency

# Load the data
df = pd.read_csv('crime_rate_data.csv')

# Define the null and alternative hypotheses
hypotheses = [
    ('age', 'crime_rate', 'age affects crime rate'),
    ('income', 'crime_rate', 'income affects crime rate'),
    ('education', 'crime_rate', 'education affects crime rate'),
    ('unemployment', 'crime_rate', 'unemployment affects crime rate'),
]

# Run multiple tests without adjusting significance levels
for hypothesis in hypotheses:
    x1, x2 = df[hypothesis[0]], df[hypothesis[1]]
    t_stat, p_val = ttest_ind(x1, x2)
    print(f""Hypothesis: {hypothesis[2]}"")
    print(f""T-statistic: {t_stat}"")
    print(f""P-value: {p_val}"")
    if p_val < 0.05:
        print(""Reject the null hypothesis"")
    else:
        print(""Fail to reject the null hypothesis"")
    print(""\n"")

    # Run a chi-squared test for categorical variables
    contingency_table = pd.crosstab(df[hypothesis[0]], df[hypothesis[1]])
    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
    print(f""Chi-squared statistic: {chi2_stat}"")
    print(f""P-value: {p_val}"")
    if p_val < 0.05:
        print(""Reject the null hypothesis"")
    else:
        print(""Fail to reject the null hypothesis"")
    print(""\n"")","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, chi2_contingency
from statsmodels.stats.multitest import multipletests

# Load the data
df = pd.read_csv('crime_rate_data.csv')

# Define the null and alternative hypotheses
hypotheses = [
    ('age', 'crime_rate', 'age affects crime rate'),
    ('income', 'crime_rate', 'income affects crime rate'),
    ('education', 'crime_rate', 'education affects crime rate'),
    ('unemployment', 'crime_rate', 'unemployment affects crime rate'),
]

# Run multiple tests with Bonferroni correction
p_values = []
for hypothesis in hypotheses:
    x1, x2 = df[hypothesis[0]], df[hypothesis[1]]
    t_stat, p_val = ttest_ind(x1, x2)
    print(f""Hypothesis: {hypothesis[2]}"")
    print(f""T-statistic: {t_stat}"")
    print(f""P-value: {p_val}"")
    p_values.append(p_val)
    if p_val < 0.05 / len(hypotheses):
        print(""Reject the null hypothesis"")
    else:
        print(""Fail to reject the null hypothesis"")
    print(""\n"")

    # Run a chi-squared test for categorical variables
    contingency_table = pd.crosstab(df[hypothesis[0]], df[hypothesis[1]])
    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
    print(f""Chi-squared statistic: {chi2_stat}"")
    print(f""P-value: {p_val}"")
    p_values.append(p_val)
    if p_val < 0.05 / len(hypotheses):
        print(""Reject the null hypothesis"")
    else:
        print(""Fail to reject the null hypothesis"")
    print(""\n"")

# Perform Bonferroni correction
_, p_values_corrected = multipletests(p_values, alpha=0.05, method='bonferroni')
print(""Bonferroni-corrected p-values:"")
for i, p_val in enumerate(p_values):
    print(f""Hypothesis {i+1}: {p_val}"")
    if p_val < 0.05 / len(hypotheses):
        print(""Reject the null hypothesis"")
    else:
        print(""Fail to reject the null hypothesis"")
    print(""\n"")",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:10:51.113989,llama-3-8b-v2,v2,False
multiple_testing,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind, PearsonR

# Load the data
data = pd.read_csv('pricing_strategy_data.csv')

# Define the null hypothesis for each test
null_hypotheses = [
    {'h0':'mean_price1 == mean_price2'},  # test 1
    {'h0':'mean_price2 == mean_price3'},  # test 2
    {'h0':'mean_price1 == mean_price3'},  # test 3
    {'h0': 'correlation between feature1 and feature2 == 0'},  # test 4
    {'h0': 'correlation between feature2 and feature3 == 0'},  # test 5
]

# Run the tests
results = []
for i, null_hypothesis in enumerate(null_hypotheses):
    if i < 3:
        # Run t-tests
        t_stat, p_val = ttest_ind(data['price1'], data['price2'])
        results.append({'test': f'Test {i+1}','statistic': t_stat, 'p_value': p_val})
    else:
        # Run Pearson correlation tests
        r, p_val = PearsonR(data['feature1'], data['feature2'])
        results.append({'test': f'Test {i+1}','statistic': r, 'p_value': p_val})

# Print the results
print('Results:')
for result in results:
    print(f'Test {result[""test""]}: p-value = {result[""p_value""]}')","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind, PearsonR
from statsmodels.stats.multitest import multipletests

# Load the data
data = pd.read_csv('pricing_strategy_data.csv')

# Define the null hypothesis for each test
null_hypotheses = [
    {'h0':'mean_price1 == mean_price2'},  # test 1
    {'h0':'mean_price2 == mean_price3'},  # test 2
    {'h0':'mean_price1 == mean_price3'},  # test 3
    {'h0': 'correlation between feature1 and feature2 == 0'},  # test 4
    {'h0': 'correlation between feature2 and feature3 == 0'},  # test 5
]

# Run the tests
results = []
for i, null_hypothesis in enumerate(null_hypotheses):
    if i < 3:
        # Run t-tests
        t_stat, p_val = ttest_ind(data['price1'], data['price2'])
        results.append({'test': f'Test {i+1}','statistic': t_stat, 'p_value': p_val})
    else:
        # Run Pearson correlation tests
        r, p_val = PearsonR(data['feature1'], data['feature2'])
        results.append({'test': f'Test {i+1}','statistic': r, 'p_value': p_val})

# Adjust the p-values using the Bonferroni correction
_, pvals_corrected = multipletests([result['p_value'] for result in results], alpha=0.05, method='bonferroni')

# Print the corrected results
print('Results:')
for i, result in enumerate(results):
    print(f'Test {result[""test""]}: p-value = {pvals_corrected[i]}')",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:12:38.402589,llama-3-8b-v2,v2,False
multiple_testing,critical,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind, pearsonr
from statsmodels.robust.scale import mad

# Load the data
df = pd.read_csv('student_performance.csv')

# Define the null hypothesis: there is no difference in means between two groups
null_hypothesis = 'There is no difference in means between the treatment and control groups.'

# Perform multiple tests without correction
t_stat, p_val = ttest_ind(df.loc[df['group'] == 'treatment','score'], df.loc[df['group'] == 'control','score'])
print(f'T-test p-value: {p_val:.4f}')

r, p_val = pearsonr(df.loc[df['group'] == 'treatment','score'], df.loc[df['group'] == 'control','score'])
print(f'Pearson's r: {r:.4f}, p-value: {p_val:.4f}')

mad_treatment = mad(df.loc[df['group'] == 'treatment','score'])
mad_control = mad(df.loc[df['group'] == 'control','score'])
print(f'MAD treatment: {mad_treatment:.4f}, MAD control: {mad_control:.4f}')","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind, pearsonr
from statsmodels.robust.scale import mad
from statsmodels.stats.multitest import multipletests

# Load the data
df = pd.read_csv('student_performance.csv')

# Define the null hypothesis: there is no difference in means between two groups
null_hypothesis = 'There is no difference in means between the treatment and control groups.'

# Perform multiple tests with Bonferroni correction
t_stat, p_val = ttest_ind(df.loc[df['group'] == 'treatment','score'], df.loc[df['group'] == 'control','score'])
p_val_adj = multipletests(p_val, alpha=0.05, method='bonferroni')[1]
print(f'T-test p-value (adjusted): {p_val_adj:.4f}')

r, p_val = pearsonr(df.loc[df['group'] == 'treatment','score'], df.loc[df['group'] == 'control','score'])
p_val_adj = multipletests(p_val, alpha=0.05, method='bonferroni')[1]
print(f'Pearson's r: {r:.4f}, p-value (adjusted): {p_val_adj:.4f}')

mad_treatment = mad(df.loc[df['group'] == 'treatment','score'])
mad_control = mad(df.loc[df['group'] == 'control','score'])
print(f'MAD treatment: {mad_treatment:.4f}, MAD control: {mad_control:.4f}')",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:14:07.751819,llama-3-8b-v2,v2,False
multiple_testing,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, chi2_contingency

# Load data
df = pd.read_csv('marketing_campaign_data.csv')

# Hypotheses:
# H0: No difference in conversion rates between ad A and ad B
# H0: No difference in conversion rates between ad A and ad C
# H0: No difference in conversion rates between ad B and ad C

# Run multiple t-tests
t_stat_A_B = ttest_ind(df[df['ad'] == 'A']['conversion_rate'], df[df['ad'] == 'B']['conversion_rate'])
t_stat_A_C = ttest_ind(df[df['ad'] == 'A']['conversion_rate'], df[df['ad'] == 'C']['conversion_rate'])
t_stat_B_C = ttest_ind(df[df['ad'] == 'B']['conversion_rate'], df[df['ad'] == 'C']['conversion_rate'])

# Run multiple chi-squared tests
chi2_stat_A_B = chi2_contingency(pd.crosstab(df['ad'], df['conversion_rate']))
chi2_stat_A_C = chi2_contingency(pd.crosstab(df['ad'], df['conversion_rate']))
chi2_stat_B_C = chi2_contingency(pd.crosstab(df['ad'], df['conversion_rate']))

# Report results
print(""T-test results:"")
print(f""Ad A vs. Ad B: {t_stat_A_B.pvalue:.4f}"")
print(f""Ad A vs. Ad C: {t_stat_A_C.pvalue:.4f}"")
print(f""Ad B vs. Ad C: {t_stat_B_C.pvalue:.4f}"")
print(""\nChi-squared test results:"")
print(f""Ad A vs. Ad B: {chi2_stat_A_B.pvalue:.4f}"")
print(f""Ad A vs. Ad C: {chi2_stat_A_C.pvalue:.4f}"")
print(f""Ad B vs. Ad C: {chi2_stat_B_C.pvalue:.4f}"")","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, chi2_contingency
from statsmodels.stats.multitest import multipletests

# Load data
df = pd.read_csv('marketing_campaign_data.csv')

# Hypotheses:
# H0: No difference in conversion rates between ad A and ad B
# H0: No difference in conversion rates between ad A and ad C
# H0: No difference in conversion rates between ad B and ad C

# Run multiple t-tests
t_stats = []
t_pvalues = []
for ad1, ad2 in [('A', 'B'), ('A', 'C'), ('B', 'C')]:
    t_stat = ttest_ind(df[df['ad'] == ad1]['conversion_rate'], df[df['ad'] == ad2]['conversion_rate'])
    t_stats.append(t_stat)
    t_pvalues.append(t_stat.pvalue)

# Adjust p-values using the Holm-Bonferroni method
_, _, _, _ = multipletests(t_pvalues, alpha=0.05, method='holm')

# Run multiple chi-squared tests
chi2_stats = []
chi2_pvalues = []
for ad1, ad2 in [('A', 'B'), ('A', 'C'), ('B', 'C')]:
    chi2_stat = chi2_contingency(pd.crosstab(df['ad'], df['conversion_rate']))
    chi2_stats.append(chi2_stat)
    chi2_pvalues.append(chi2_stat.pvalue)

# Adjust p-values using the Holm-Bonferroni method
_, _, _, _ = multipletests(chi2_pvalues, alpha=0.05, method='holm')

# Report results
print(""T-test results:"")
for i, (t_stat, t_pvalue) in enumerate(zip(t_stats, t_pvalues)):
    print(f""Ad {t_stats[i][0] vs. Ad {t_stats[i][1]}: {t_pvalue:.4f} (adjusted p-value: {t_pvalues[i] * len(t_pvalues)}:.4f)"")

print(""\nChi-squared test results:"")
for i, (chi2_stat, chi2_pvalue) in enumerate(zip(chi2_stats, chi2_pvalues)):
    print(f""Ad {chi2_stats[i][0] vs. Ad {chi2_stats[i][1]}: {chi2_pvalue:.4f} (adjusted p-value: {chi2_pvalues[i] * len(chi2_pvalues)}:.4f)"")",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:16:33.426511,llama-3-8b-v2,v2,False
multiple_testing,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import scipy.stats as stats

# Load the data
df = pd.read_csv('voting_data.csv')

# Perform multiple statistical tests
p_value1 = stats.ttest_ind(df['party_A'], df['party_B'])
p_value2 = stats.ttest_ind(df['party_A'], df['party_C'])
p_value3 = stats.ttest_ind(df['party_B'], df['party_C'])

# Report the results
print(f""p-value for party A vs party B: {p_value1}"")
print(f""p-value for party A vs party C: {p_value2}"")
print(f""p-value for party B vs party C: {p_value3}"")","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import scipy.stats as stats
import numpy as np

# Load the data
df = pd.read_csv('voting_data.csv')

# Perform multiple statistical tests with Bonferroni correction
p_value1 = stats.ttest_ind(df['party_A'], df['party_B'])[1]
p_value2 = stats.ttest_ind(df['party_A'], df['party_C'])[1]
p_value3 = stats.ttest_ind(df['party_B'], df['party_C'])[1]

# Calculate the adjusted significance level (alpha) using Bonferroni correction
alpha = 0.05
adjusted_alpha = alpha / len([p_value1, p_value2, p_value3])

# Report the results
print(f""p-value for party A vs party B: {p_value1}"")
print(f""p-value for party A vs party C: {p_value2}"")
print(f""p-value for party B vs party C: {p_value3}"")

# Check if the p-values are significant at the adjusted alpha level
if p_value1 < adjusted_alpha:
    print(""Party A vs party B: Significant"")
else:
    print(""Party A vs party B: Not significant"")

if p_value2 < adjusted_alpha:
    print(""Party A vs party C: Significant"")
else:
    print(""Party A vs party C: Not significant"")

if p_value3 < adjusted_alpha:
    print(""Party B vs party C: Significant"")
else:
    print(""Party B vs party C: Not significant"")",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:17:53.160048,llama-3-8b-v2,v2,False
multiple_testing,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency
import numpy as np

# Load data
df = pd.read_csv('data.csv')

# Perform multiple tests without correction
results = []
for column in df.columns:
    if column!= 'outcome':  # ignore the outcome variable
        for other_column in df.columns:
            if column!= other_column:
                if column.startswith('demographic') and other_column.startswith('demographic'):
                    # Run t-test
                    t_stat, p_val = ttest_ind(df[column], df[other_column], equal_var=False)
                    results.append((column, other_column, p_val))
                elif column.startswith('demographic') and other_column.startswith('outcome'):
                    # Run chi-squared test
                    contingency_table = pd.crosstab(df[column], df[other_column])
                    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
                    results.append((column, other_column, p_val))

# Print results
for result in results:
    print(result)","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency
import numpy as np
from statsmodels.stats.multitest import multipletests

# Load data
df = pd.read_csv('data.csv')

# Perform multiple tests with correction
results = []
p_values = []
for column in df.columns:
    if column!= 'outcome':  # ignore the outcome variable
        for other_column in df.columns:
            if column!= other_column:
                if column.startswith('demographic') and other_column.startswith('demographic'):
                    # Run t-test
                    t_stat, p_val = ttest_ind(df[column], df[other_column], equal_var=False)
                    p_values.append(p_val)
                elif column.startswith('demographic') and other_column.startswith('outcome'):
                    # Run chi-squared test
                    contingency_table = pd.crosstab(df[column], df[other_column])
                    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
                    p_values.append(p_val)

# Correct for multiple testing
_, p_values_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fwer')

# Print results
for i, p_val in enumerate(p_values_corrected):
    print(f""Test {i+1}: p-value = {p_val}"")",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:19:22.219918,llama-3-8b-v2,v2,False
multiple_testing,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency
from statsmodels.robust.scale import mad
from scikit_posthocs import posthoc_ttest

# Load data
df = pd.read_csv('social_media_data.csv')

# Define groups
group1 = df[df['social_media'] == 'Facebook']
group2 = df[df['social_media'] == 'Instagram']

# Run multiple tests without adjustment
t_stat, p_val = ttest_ind(group1['mental_health'], group2['mental_health'])
print('T-test p-value:', p_val)

chi2_stat, p_val, dof, expected = chi2_contingency(pd.crosstab(group1['mental_health'], group1['age']))
print('Chi-squared test p-value:', p_val)

# Calculate mad for robust standard errors
mad_val = mad(group1['mental_health'])
print('MAD:', mad_val)

# Run post-hoc tests without adjustment
posthoc_results = posthoc_ttest(group1['mental_health'], group2['mental_health'])
print('Post-hoc test results:', posthoc_results)","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency
from statsmodels.robust.scale import mad
from scikit_posthocs import posthoc_ttest
from statsmodels.stats.multitest import multipletests

# Load data
df = pd.read_csv('social_media_data.csv')

# Define groups
group1 = df[df['social_media'] == 'Facebook']
group2 = df[df['social_media'] == 'Instagram']

# Run multiple tests with adjustment for multiple testing
t_stat, p_val = ttest_ind(group1['mental_health'], group2['mental_health'])
p_adj = multipletests(p_val, method='bonferroni')[1]
print('T-test p-value (adjusted):', p_adj)

chi2_stat, p_val, dof, expected = chi2_contingency(pd.crosstab(group1['mental_health'], group1['age']))
p_adj = multipletests(p_val, method='bonferroni')[1]
print('Chi-squared test p-value (adjusted):', p_adj)

# Calculate mad for robust standard errors
mad_val = mad(group1['mental_health'])
print('MAD:', mad_val)

# Run post-hoc tests with adjustment for multiple testing
posthoc_results = posthoc_ttest(group1['mental_health'], group2['mental_health'], method='bonferroni')
print('Post-hoc test results (adjusted):', posthoc_results)",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:20:47.277761,llama-3-8b-v2,v2,False
multiple_testing,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency

# Load data
df = pd.read_csv('patient_outcomes.csv')

# Run multiple tests without correction
t_stat, p_val = ttest_ind(df['group1'], df['group2'])
print(f""T-test p-value: {p_val:.4f}"")

cstat, p_val = chi2_contingency(pd.crosstab(df['group1'], df['group1']))
print(f""Chi-squared test p-value: {p_val:.4f}"")

wilcoxon_stat, p_val = wilcoxon_rank_sum(df['group1'], df['group2'])
print(f""Wilcoxon rank-sum test p-value: {p_val:.4f}"")

# Run another test without correction
t_stat, p_val = ttest_ind(df['group1'], df['group3'])
print(f""T-test p-value: {p_val:.4f}"")","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency
from statsmodels.stats.multitest import multipletests

# Load data
df = pd.read_csv('patient_outcomes.csv')

# Run multiple tests with Bonferroni correction
t_stat, p_val = ttest_ind(df['group1'], df['group2'])
p_val_corr, _, _, _ = multipletests(p_val, alpha=0.05, method='bonferroni')
print(f""T-test p-value (Bonferroni corrected): {p_val_corr:.4f}"")

cstat, p_val = chi2_contingency(pd.crosstab(df['group1'], df['group1']))
p_val_corr, _, _, _ = multipletests(p_val, alpha=0.05, method='bonferroni')
print(f""Chi-squared test p-value (Bonferroni corrected): {p_val_corr:.4f}"")

wilcoxon_stat, p_val = wilcoxon_rank_sum(df['group1'], df['group2'])
p_val_corr, _, _, _ = multipletests(p_val, alpha=0.05, method='bonferroni')
print(f""Wilcoxon rank-sum test p-value (Bonferroni corrected): {p_val_corr:.4f}"")

# Run another test with Bonferroni correction
t_stat, p_val = ttest_ind(df['group1'], df['group3'])
p_val_corr, _, _, _ = multipletests(p_val, alpha=0.05, method='bonferroni')
print(f""T-test p-value (Bonferroni corrected): {p_val_corr:.4f}"")",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:22:28.801595,llama-3-8b-v2,v2,False
multiple_testing,critical,education,teaching method effectiveness comparison,python,intermediate,"import numpy as np
from scipy.stats import ttest_ind, f_oneway
import pandas as pd

# Load data
df = pd.read_csv('data.csv')

# Define the null and alternative hypotheses
H0 = 'No significant difference in mean scores'
H1 = 'There is a significant difference in mean scores'

# Perform multiple tests
t_test_result = ttest_ind(df['score1'], df['score2'])
f_test_result = f_oneway(df['score1'], df['score2'], df['score3'])

# Determine significance
t_test_pval = t_test_result.pvalue
f_test_pval = f_test_result.pvalue

# Report results
print(f'T-test p-value: {t_test_pval:.4f}')
print(f'F-test p-value: {f_test_pval:.4f}')","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
from scipy.stats import ttest_ind, f_oneway
import pandas as pd
from statsmodels.stats.multitest import multipletests

# Load data
df = pd.read_csv('data.csv')

# Define the null and alternative hypotheses
H0 = 'No significant difference in mean scores'
H1 = 'There is a significant difference in mean scores'

# Perform multiple tests
t_test_result = ttest_ind(df['score1'], df['score2'])
f_test_result = f_oneway(df['score1'], df['score2'], df['score3'])

# Determine significance
t_test_pval = t_test_result.pvalue
f_test_pval = f_test_result.pvalue

# Adjust for multiple testing using Bonferroni correction
pvals = np.array([t_test_pval, f_test_pval])
_, bonferroni_pvals, _, _ = multipletests(pvals, alpha=0.05, method='bonferroni')

# Report results
print(f'T-test p-value (adjusted): {bonferroni_pvals[0]:.4f}')
print(f'F-test p-value (adjusted): {bonferroni_pvals[1]:.4f}')",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:23:38.183046,llama-3-8b-v2,v2,False
multiple_testing,critical,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency

# Load the data
df = pd.read_csv('voting_data.csv')

# Define the null hypothesis
null_hypothesis = 'There is no significant difference in voting behavior between Democrats and Republicans.'

# Perform multiple tests
t_stat, p_val = ttest_ind(df[df['party'] == 'Democrat']['vote'], df[df['party'] == 'Republican']['vote'])
print(f'T-test result: t-statistic = {t_stat}, p-value = {p_val}')

chi2_stat, p_val = chi2_contingency(pd.crosstab(df['party'], df['vote']))
print(f'Chi-squared test result: chi2-statistic = {chi2_stat}, p-value = {p_val}')

# Run another test
from scipy.stats import mannwhitneyu
u_stat, p_val = mannwhitneyu(df[df['party'] == 'Democrat']['vote'], df[df['party'] == 'Republican']['vote'])
print(f'Mann-Whitney U-test result: U-statistic = {u_stat}, p-value = {p_val}')","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency
from statsmodels.stats.multitest import multipletests

# Load the data
df = pd.read_csv('voting_data.csv')

# Define the null hypothesis
null_hypothesis = 'There is no significant difference in voting behavior between Democrats and Republicans.'

# Perform multiple tests
t_stat, p_val = ttest_ind(df[df['party'] == 'Democrat']['vote'], df[df['party'] == 'Republican']['vote'])
print(f'T-test result: t-statistic = {t_stat}, p-value = {p_val}')

chi2_stat, p_val = chi2_contingency(pd.crosstab(df['party'], df['vote']))
print(f'Chi-squared test result: chi2-statistic = {chi2_stat}, p-value = {p_val}')

# Run another test
from scipy.stats import mannwhitneyu
u_stat, p_val = mannwhitneyu(df[df['party'] == 'Democrat']['vote'], df[df['party'] == 'Republican']['vote'])
print(f'Mann-Whitney U-test result: U-statistic = {u_stat}, p-value = {p_val}')

# Correct for multiple testing using the Bonferroni method
_, p_vals_corrected = multipletests([p_val, p_val, p_val], alpha=0.05, method='bonferroni')
print(f'Corrected p-values: {p_vals_corrected}')",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:25:11.343124,llama-3-8b-v2,v2,False
multiple_testing,critical,social_science,social media usage and mental health,python,intermediate,"import numpy as np
import scipy.stats as stats

# Generate some sample data
np.random.seed(0)
crime_rate = np.random.normal(loc=100, scale=10, size=100)
age = np.random.normal(loc=30, scale=10, size=100)
income = np.random.normal(loc=50000, scale=10000, size=100)

# Run multiple tests without correction
t_stat_age = stats.ttest_ind(crime_rate, age)
t_stat_income = stats.ttest_ind(crime_rate, income)

print(""Age and crime rate: p-value ="", t_stat_age[1])
print(""Income and crime rate: p-value ="", t_stat_income[1])","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import scipy.stats as stats
import statsmodels.stats.multitest as multitest

# Generate some sample data
np.random.seed(0)
crime_rate = np.random.normal(loc=100, scale=10, size=100)
age = np.random.normal(loc=30, scale=10, size=100)
income = np.random.normal(loc=50000, scale=10000, size=100)

# Run multiple tests with Bonferroni correction
t_stat_age = stats.ttest_ind(crime_rate, age)
t_stat_income = stats.ttest_ind(crime_rate, income)

p_values = np.array([t_stat_age[1], t_stat_income[1]])
correction = multitest.multipletests(p_values, alpha=0.05, method='bonferroni')
rejected_null_hypotheses = p_values < correction[1]

print(""Age and crime rate: p-value ="", t_stat_age[1])
print(""Income and crime rate: p-value ="", t_stat_income[1])
print(""Adjusted p-values:"", correction[1][0], correction[1][1])
print(""Rejected null hypotheses:"", rejected_null_hypotheses)",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:26:22.836810,llama-3-8b-v2,v2,False
multiple_testing,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency

# Load dataset
df = pd.read_csv('churn_data.csv')

# Define features and target variable
X = df.drop('churn', axis=1)
y = df['churn']

# Perform multiple tests
t_stat, p_val = ttest_ind(X['age'], X['income'], equal_var=False)
print(f'T-test p-value: {p_val:.4f}')

contingency_table = pd.crosstab(X['age'], X['churn'])
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print(f'Chi-squared test p-value: {p_val:.4f}')

# Perform another test
t_stat, p_val = ttest_ind(X['credit_score'], X['income'], equal_var=False)
print(f'T-test p-value: {p_val:.4f}')

# Print all p-values
print('All p-values:')
print(p_val)","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency
from statsmodels.stats.multitest import multipletests

# Load dataset
df = pd.read_csv('churn_data.csv')

# Define features and target variable
X = df.drop('churn', axis=1)
y = df['churn']

# Perform multiple tests
t_stat, p_val = ttest_ind(X['age'], X['income'], equal_var=False)
print(f'T-test p-value: {p_val:.4f}')

contingency_table = pd.crosstab(X['age'], X['churn'])
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
print(f'Chi-squared test p-value: {p_val:.4f}')

# Perform another test
t_stat, p_val = ttest_ind(X['credit_score'], X['income'], equal_var=False)
print(f'T-test p-value: {p_val:.4f}')

# Correct for multiple testing using the Bonferroni method
_, p_val_corrected = multipletests([p_val, p_val, p_val], alpha=0.05, method='bonferroni')
print('Corrected p-values:')
print(p_val_corrected)",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:27:52.245888,llama-3-8b-v2,v2,False
multiple_testing,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, pearsonr

# Load data
data = pd.read_csv('income_inequality.csv')

# Perform multiple tests without correction
for i in range(1, 11):
    # Perform t-test for mean difference between two groups
    t_stat, p_val = ttest_ind(data['income'][data['group'] == i], data['income'][data['group'] == i+1])
    print(f'Test {i}: p-value = {p_val:.4f}')
    
    # Perform Pearson correlation coefficient
    corr_coef, p_val = pearsonr(data['income'][data['group'] == i], data['income'][data['group'] == i+1])
    print(f'Correlation {i}: p-value = {p_val:.4f}')","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, pearsonr
from statsmodels.stats.multitest import multipletests

# Load data
data = pd.read_csv('income_inequality.csv')

# Perform multiple tests with correction
p_values = []
for i in range(1, 11):
    # Perform t-test for mean difference between two groups
    t_stat, p_val = ttest_ind(data['income'][data['group'] == i], data['income'][data['group'] == i+1])
    p_values.append(p_val)
    
    # Perform Pearson correlation coefficient
    corr_coef, p_val = pearsonr(data['income'][data['group'] == i], data['income'][data['group'] == i+1])
    p_values.append(p_val)

# Correct for multiple testing using the Bonferroni method
_, corrected_p_values, _, _ = multipletests(p_values, alpha=0.05, method='bonferroni')

# Print corrected p-values
for i, p_val in enumerate(corrected_p_values):
    print(f'Test {i+1}: p-value = {p_val:.4f}')",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:29:10.415371,llama-3-8b-v2,v2,False
multiple_testing,critical,education,online vs in-person learning outcomes,python,intermediate,"import numpy as np
import statsmodels.api as sm
import pandas as pd

# Load the dataset
df = pd.read_csv('data.csv')

# Split the data into groups
group1 = df[df['group'] == 'A']
group2 = df[df['group'] == 'B']

# Calculate the means and standard deviations of each group
mean1 = np.mean(group1['score'])
std1 = np.std(group1['score'])
mean2 = np.mean(group2['score'])
std2 = np.std(group2['score'])

# Perform multiple t-tests
t_stat1 = stats.ttest_ind(group1['score'], group2['score'], equal_var=False)
t_stat2 = stats.ttest_ind(group1['score'], group2['score'], equal_var=False)
t_stat3 = stats.ttest_ind(group1['score'], group2['score'], equal_var=False)

# Print the p-values
print(t_stat1.pvalue)
print(t_stat2.pvalue)
print(t_stat3.pvalue)","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import statsmodels.api as sm
import pandas as pd
from statsmodels.stats.multitest import multipletests

# Load the dataset
df = pd.read_csv('data.csv')

# Split the data into groups
group1 = df[df['group'] == 'A']
group2 = df[df['group'] == 'B']

# Calculate the means and standard deviations of each group
mean1 = np.mean(group1['score'])
std1 = np.std(group1['score'])
mean2 = np.mean(group2['score'])
std2 = np.std(group2['score'])

# Perform multiple t-tests and correct for multiple testing
p_values = []
for i in range(3):
    t_stat = sm.stats.ttest_ind(group1['score'], group2['score'], equal_var=False)
    p_values.append(t_stat.pvalue)

_, p_adj = multipletests(p_values, alpha=0.05, method='fdr_bh)

# Print the adjusted p-values
print(p_adj)",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:30:28.920582,llama-3-8b-v2,v2,False
multiple_testing,critical,education,graduation rate analysis by demographics,python,intermediate,"import numpy as np
import scipy.stats as stats

# Sample data
np.random.seed(0)
n_students = 100
n_tests = 10
test_scores = np.random.normal(50, 10, (n_students, n_tests))

# Perform multiple tests without correction
for i in range(n_tests):
    t_stat, p_val = stats.ttest_ind(test_scores[:, i], test_scores[:, 0], equal_var=False)
    print(f""Test {i+1}: t-statistic = {t_stat:.2f}, p-value = {p_val:.4f}"")","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import scipy.stats as stats
import statsmodels.stats.multitest as multitest

# Sample data
np.random.seed(0)
n_students = 100
n_tests = 10
test_scores = np.random.normal(50, 10, (n_students, n_tests))

# Perform multiple tests with correction (Bonferroni-Holm method)
p_values = []
for i in range(n_tests):
    t_stat, p_val = stats.ttest_ind(test_scores[:, i], test_scores[:, 0], equal_var=False)
    p_values.append(p_val)

# Adjust p-values using the Bonferroni-Holm method
adjusted_p_values = multitest.multipletests(p_values, alpha=0.05, method='bonferroni')[1]

# Print results
for i in range(n_tests):
    print(f""Test {i+1}: t-statistic = {t_stat:.2f}, p-value = {p_values[i]:.4f}, adjusted p-value = {adjusted_p_values[i]:.4f}"")",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:31:36.617406,llama-3-8b-v2,v2,False
multiple_testing,critical,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import scipy.stats as stats

# Load data
data = pd.read_csv('online_learning_outcomes.csv')

# Define variables
online_mean = data['online_score'].mean()
inperson_mean = data['inperson_score'].mean()

# Perform multiple tests without correction
t_stat_online = stats.ttest_1samp(data['online_score'], online_mean)
t_stat_inperson = stats.ttest_1samp(data['inperson_score'], inperson_mean)

# Print results
print(""Online learning: "", t_stat_online.pvalue)
print(""In-person learning: "", t_stat_inperson.pvalue)","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import scipy.stats as stats

# Load data
data = pd.read_csv('online_learning_outcomes.csv')

# Define variables
online_mean = data['online_score'].mean()
inperson_mean = data['inperson_score'].mean()

# Set the significance level
alpha = 0.05

# Perform multiple tests with Bonferroni correction
t_stat_online = stats.ttest_1samp(data['online_score'], online_mean)
t_stat_inperson = stats.ttest_1samp(data['inperson_score'], inperson_mean)

# Calculate the adjusted p-values
alpha_adj = alpha / 2  # Bonferroni correction for two tests
p_value_online = t_stat_online.pvalue
p_value_inperson = t_stat_inperson.pvalue

# Print results
print(""Online learning: "", p_value_online < alpha_adj)
print(""In-person learning: "", p_value_inperson < alpha_adj)",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:32:39.942800,llama-3-8b-v2,v2,False
multiple_testing,critical,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency

# Load dataset
df = pd.read_csv('graduation_rate_data.csv')

# Split data into groups by demographic
demographics = ['gender', 'race','region']
groups = {}
for demographic in demographics:
    groups[demographic] = df.groupby(demographic)['grad_rate'].mean()

# Perform multiple t-tests
for demographic in demographics:
    for other_demographic in demographics:
        if demographic!= other_demographic:
            t_stat, p_val = ttest_ind(groups[demographic], groups[other_demographic], equal_var=False)
            print(f'T-test between {demographic} and {other_demographic}: p-value = {p_val:.4f}')

# Perform multiple chi-squared tests
chi2_stat, p_val, dof, expected = chi2_contingency(pd.crosstab(df['gender'], df['grad_rate']))
print(f'Chi-squared test between gender and grad_rate: p-value = {p_val:.4f}')","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency
from statsmodels.stats.multitest import multipletests

# Load dataset
df = pd.read_csv('graduation_rate_data.csv')

# Split data into groups by demographic
demographics = ['gender', 'race','region']
groups = {}
for demographic in demographics:
    groups[demographic] = df.groupby(demographic)['grad_rate'].mean()

# Perform multiple t-tests
p_vals = []
for demographic in demographics:
    for other_demographic in demographics:
        if demographic!= other_demographic:
            t_stat, p_val = ttest_ind(groups[demographic], groups[other_demographic], equal_var=False)
            p_vals.append(p_val)

# Adjust p-values using the Bonferroni correction
p_vals_adj = multipletests(p_vals, method='bonferroni')[1]

# Print adjusted p-values
for i, (demographic, p_val_adj) in enumerate(zip(demographics, p_vals_adj)):
    print(f'T-test between {demographics[i]} and {demographics[i+1]}: p-value = {p_val_adj:.4f}')

# Perform multiple chi-squared tests
chi2_stat, p_val, dof, expected = chi2_contingency(pd.crosstab(df['gender'], df['grad_rate']))
p_val_adj = multipletests([p_val], method='bonferroni')[1][0]
print(f'Chi-squared test between gender and grad_rate: p-value = {p_val_adj:.4f}')",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:34:08.072706,llama-3-8b-v2,v2,False
multiple_testing,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind, wilcoxon_rank_sum
import statsmodels.api as sm

# Load data
data = pd.read_csv('pricing_data.csv')

# Define the null hypothesis
null_hypothesis = 'There is no significant difference in the mean prices between the old and new pricing strategies.'

# Perform multiple tests
t_stat, p_value = ttest_ind(data['old_price'], data['new_price'])
print(f""T-test p-value: {p_value}"")

wilcoxon_stat, wilcoxon_p_value = wilcoxon_rank_sum(data['old_price'], data['new_price'])
print(f""Wilcoxon rank-sum test p-value: {wilcoxon_p_value}"")

# Run another test
ols_model = sm.OLS(data['new_price'], sm.add_constant(data['old_price'])).fit()
ols_p_value = ols_model.f_test().pvalue
print(f""OLS F-test p-value: {ols_p_value}"")

print(""Reject null hypothesis if p-value < 0.05"")
if p_value < 0.05:
    print(""Reject null hypothesis"")
if wilcoxon_p_value < 0.05:
    print(""Reject null hypothesis"")
if ols_p_value < 0.05:
    print(""Reject null hypothesis"")","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind, wilcoxon_rank_sum
import statsmodels.api as sm
from statsmodels.stats.multitest import multipletests

# Load data
data = pd.read_csv('pricing_data.csv')

# Define the null hypothesis
null_hypothesis = 'There is no significant difference in the mean prices between the old and new pricing strategies.'

# Perform multiple tests
t_stat, p_value = ttest_ind(data['old_price'], data['new_price'])
wilcoxon_stat, wilcoxon_p_value = wilcoxon_rank_sum(data['old_price'], data['new_price'])
ols_model = sm.OLS(data['new_price'], sm.add_constant(data['old_price'])).fit()
ols_p_value = ols_model.f_test().pvalue

# Adjust the p-values for multiple testing using the Bonferroni correction
p_values = [p_value, wilcoxon_p_value, ols_p_value]
p_values_adj = multipletests(p_values, alpha=0.05, method='bonferroni')[1]

print(""Reject null hypothesis if p-value < adjusted alpha"")
if p_values_adj[0] < 0.05:
    print(""Reject null hypothesis"")
if p_values_adj[1] < 0.05:
    print(""Reject null hypothesis"")
if p_values_adj[2] < 0.05:
    print(""Reject null hypothesis"")",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:35:40.952241,llama-3-8b-v2,v2,False
multiple_testing,critical,social_science,income inequality trends,python,intermediate,"import numpy as np
import scipy.stats as stats

# Simulate data
np.random.seed(0)
n = 1000
x = np.random.normal(0, 1, n)
y = np.random.normal(0, 1, n)

# Run many tests without adjusting significance levels
for i in range(10):
    t_stat, p_val = stats.ttest_ind(x, y)
    if p_val < 0.05:
        print(f""Test {i+1} is significant at p-value {p_val}"")","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import scipy.stats as stats

# Simulate data
np.random.seed(0)
n = 1000
x = np.random.normal(0, 1, n)
y = np.random.normal(0, 1, n)

# Run many tests with Bonferroni correction
alpha = 0.05
num_tests = 10
correction_factor = 1 / num_tests
for i in range(num_tests):
    t_stat, p_val = stats.ttest_ind(x, y)
    corrected_p_val = p_val * correction_factor
    if corrected_p_val < alpha:
        print(f""Test {i+1} is significant at p-value {corrected_p_val}"")",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:36:39.813396,llama-3-8b-v2,v2,False
multiple_testing,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency

# Load the data
df = pd.read_csv('voting_data.csv')

# Perform multiple tests without correction
for feature in ['age', 'income', 'education', 'party']:
    t_stat, p_val = ttest_ind(df[feature], df['vote'])
    print(f""T-test for {feature}: p-value = {p_val}"")
    
    contingency_table = pd.crosstab(df[feature], df['vote'])
    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
    print(f""Chi-squared test for {feature}: p-value = {p_val}"")","{'summary': 'Multiple Testing without Correction', 'detailed_explanation': 'Multiple Testing without Correction: Family-wise error rate increases with multiple comparisons', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind, chi2_contingency
from statsmodels.stats.multitest import multipletests

# Load the data
df = pd.read_csv('voting_data.csv')

# Perform multiple tests with Bonferroni correction
correction_method = 'bonferroni'
alpha = 0.05
adjusted_p_vals = []

for feature in ['age', 'income', 'education', 'party']:
    t_stat, p_val = ttest_ind(df[feature], df['vote'])
    print(f""T-test for {feature}: p-value = {p_val}"")
    
    contingency_table = pd.crosstab(df[feature], df['vote'])
    chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)
    print(f""Chi-squared test for {feature}: p-value = {p_val}"")
    
    # Apply Bonferroni correction to p-values
    adjusted_p_val = multipletests(p_val, alpha, method=correction_method)[1]
    adjusted_p_vals.append(adjusted_p_val)
    
    print(f""Adjusted p-value for {feature} (Bonferroni correction): {adjusted_p_val}"")",['Family-wise error rate increases with multiple comparisons'],Family-wise error rate increases with multiple comparisons,2025-12-16T08:37:51.904217,llama-3-8b-v2,v2,False
p_hacking,critical,education,student performance prediction,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind

# Load the data
data = pd.read_csv(""online_learning_data.csv"")

# Split the data into online and in-person groups
online_data = data[data[""learning_method""] == ""online""]
in_person_data = data[data[""learning_method""] == ""in-person""]

# Perform a t-test to compare the two groups
t_stat, p_val = ttest_ind(online_data[""exam_score""], in_person_data[""exam_score""])

# Print the results
print(""T-statistic:"", t_stat)
print(""p-value:"", p_val)

# Report the results only if the p-value is below a certain threshold
if p_val < 0.05:
    print(""There is a statistically significant difference between online and in-person learning outcomes."")
else:
    print(""There is no statistically significant difference between online and in-person learning outcomes."")

# Now, let's try a different analysis...
online_data = data[data[""topic""] == ""math""]
in_person_data = data[data[""topic""] == ""math""]
t_stat, p_val = ttest_ind(online_data[""exam_score""], in_person_data[""exam_score""])
print(""T-statistic:"", t_stat)
print(""p-value:"", p_val)
if p_val < 0.05:
    print(""There is a statistically significant difference between online and in-person learning outcomes for the math topic."")
else:
    print(""There is no statistically significant difference between online and in-person learning outcomes for the math topic."")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind

# Load the data
data = pd.read_csv(""online_learning_data.csv"")

# Split the data into online and in-person groups
online_data = data[data[""learning_method""] == ""online""]
in_person_data = data[data[""learning_method""] == ""in-person""]

# Perform a t-test to compare the two groups
t_stat, p_val = ttest_ind(online_data[""exam_score""], in_person_data[""exam_score""])

# Print the results
print(""T-statistic:"", t_stat)
print(""p-value:"", p_val)

# Report the results without selectively reporting significant results
print(""There is a statistically significant difference between online and in-person learning outcomes if p-value < 0.05."")
print(""The p-value is:"", p_val)

# Perform a Bonferroni correction to account for multiple comparisons
alpha = 0.05
n_tests = 2  # number of tests performed
correction_factor = n_tests * (1 - alpha)
p_val_corrected = p_val * correction_factor

print(""Corrected p-value:"", p_val_corrected)
if p_val_corrected < alpha:
    print(""There is a statistically significant difference between online and in-person learning outcomes after correcting for multiple comparisons."")
else:
    print(""There is no statistically significant difference between online and in-person learning outcomes after correcting for multiple comparisons."")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:39:22.007863,llama-3-8b-v2,v2,False
p_hacking,critical,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('education_data.csv')

# Split the data into online and in-person learning groups
online = df[df['learning_method'] == 'online']
in_person = df[df['learning_method'] == 'in_person']

# Run multiple t-tests to compare online and in-person learning outcomes
results = []
for outcome in ['math_score','reading_score', 'writing_score']:
    t_stat, p_val = ttest_ind(online[outcome], in_person[outcome])
    if p_val < 0.05:
        results.append((outcome, t_stat, p_val))
print(results)","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('education_data.csv')

# Split the data into online and in-person learning groups
online = df[df['learning_method'] == 'online']
in_person = df[df['learning_method'] == 'in_person']

# Define a list of outcomes to compare
outcomes = ['math_score','reading_score', 'writing_score']

# Run a single omnibus test to compare online and in-person learning outcomes
t_stat, p_val = ttest_ind(online[outcomes], in_person[outcomes], equal_var=False)
print(f""Omnibus test: t-statistic = {t_stat}, p-value = {p_val}"")

# If the omnibus test is significant, run follow-up t-tests for each outcome
if p_val < 0.05:
    for outcome in outcomes:
        t_stat, p_val = ttest_ind(online[outcome], in_person[outcome])
        print(f""{outcome}: t-statistic = {t_stat}, p-value = {p_val}"")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:40:34.516829,llama-3-8b-v2,v2,False
p_hacking,critical,social_science,income inequality trends,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load crime rate data
crime_data = pd.read_csv('crime_data.csv')

# Define a null hypothesis: there is no significant difference in crime rates between urban and rural areas
null_hypothesis = 'There is no significant difference in crime rates between urban and rural areas'

# Split data into urban and rural groups
urban_data = crime_data[crime_data['area'] == 'urban']
rural_data = crime_data[crime_data['area'] == 'rural']

# Perform t-test to compare crime rates between urban and rural areas
t_stat, p_val = ttest_ind(urban_data['crime_rate'], rural_data['crime_rate'])

# Report results
if p_val < 0.05:
    print(f""P-value: {p_val:.4f}, Reject null hypothesis: {null_hypothesis}"")
else:
    print(f""P-value: {p_val:.4f}, Fail to reject null hypothesis: {null_hypothesis}"")

# But wait, let's try again!
urban_data = crime_data[crime_data['area'] == 'urban'][crime_data['crime_rate'] > 0]
rural_data = crime_data[crime_data['area'] == 'rural'][crime_data['crime_rate'] > 0]

t_stat, p_val = ttest_ind(urban_data['crime_rate'], rural_data['crime_rate'])
if p_val < 0.05:
    print(f""P-value: {p_val:.4f}, Reject null hypothesis: {null_hypothesis}"")
else:
    print(f""P-value: {p_val:.4f}, Fail to reject null hypothesis: {null_hypothesis}"")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load crime rate data
crime_data = pd.read_csv('crime_data.csv')

# Define a null hypothesis: there is no significant difference in crime rates between urban and rural areas
null_hypothesis = 'There is no significant difference in crime rates between urban and rural areas'

# Split data into urban and rural groups
urban_data = crime_data[crime_data['area'] == 'urban']
rural_data = crime_data[crime_data['area'] == 'rural']

# Perform t-test to compare crime rates between urban and rural areas
t_stat, p_val = ttest_ind(urban_data['crime_rate'], rural_data['crime_rate'])

# Report results
if p_val < 0.05:
    print(f""P-value: {p_val:.4f}, Reject null hypothesis: {null_hypothesis}"")
else:
    print(f""P-value: {p_val:.4f}, Fail to reject null hypothesis: {null_hypothesis}"")

# Report the result once, without re-running the analysis",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:42:10.105407,llama-3-8b-v2,v2,False
p_hacking,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load sales data
data = pd.read_csv('sales_data.csv')

# Split data into regions
regions = ['North', 'South', 'East', 'West']
region_data = {region: data[data['Region'] == region] for region in regions}

# Run t-tests between each pair of regions
results = []
for i in range(len(regions)):
    for j in range(i+1, len(regions)):
        region1 = regions[i]
        region2 = regions[j]
        stat, p_value = ttest_ind(region_data[region1]['Sales'], region_data[region2]['Sales'])
        if p_value < 0.05:
            results.append((region1, region2, stat, p_value))

# Print significant results
for result in results:
    print(f""{result[0]} vs {result[1]}: p-value = {result[3]}"")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Load sales data
data = pd.read_csv('sales_data.csv')

# Split data into regions
regions = ['North', 'South', 'East', 'West']
region_data = {region: data[data['Region'] == region] for region in regions}

# Run t-tests between each pair of regions
results = []
for i in range(len(regions)):
    for j in range(i+1, len(regions)):
        region1 = regions[i]
        region2 = regions[j]
        stat, p_value = ttest_ind(region_data[region1]['Sales'], region_data[region2]['Sales'])
        results.append((region1, region2, stat, p_value))

# Correct for multiple comparisons using the Bonferroni correction
_, p_values_corrected, _, _ = multipletests([result[3] for result in results], alpha=0.05, method='bonferroni')

# Print significant results
significant_results = [(result[0], result[1], result[2], p_value) for result in results if p_value < p_values_corrected]
for result in significant_results:
    print(f""{result[0]} vs {result[1]}: p-value = {result[3]}"")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:43:32.624124,llama-3-8b-v2,v2,False
p_hacking,critical,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
from scipy.stats import pearsonr

# Load the dataset
data = pd.read_csv('student_performance.csv')

# Define the outcome variable and predictor variables
outcome = 'grade'
predictors = ['math_score','reading_score','science_score']

# Perform multiple linear regression
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(data[predictors], data[outcome])

# Perform multiple hypothesis testing
p_values = []
for i in range(len(predictors)):
    for j in range(i+1, len(predictors)):
        predictor1 = predictors[i]
        predictor2 = predictors[j]
        correlation_coefficient, p_value = pearsonr(data[predictor1], data[predictor2])
        if p_value < 0.05:
            print(f""{predictor1} and {predictor2} are significantly correlated (p-value: {p_value})"")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import pearsonr
from sklearn.linear_model import LinearRegression
from statsmodels.formula.api import ols

# Load the dataset
data = pd.read_csv('student_performance.csv')

# Define the outcome variable and predictor variables
outcome = 'grade'
predictors = ['math_score','reading_score','science_score']

# Perform pre-specified analysis plan
model = ols(f""{outcome} ~ {'+'.join(predictors)}"", data=data).fit()
print(model.f_test())

# Perform multiple hypothesis testing
p_values = []
for predictor in predictors:
    p_value = model.pvalues[predictor]
    if p_value < 0.05:
        print(f""{predictor} is significantly correlated with {outcome} (p-value: {p_value})"")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:44:49.040062,llama-3-8b-v2,v2,False
p_hacking,critical,education,student performance prediction,python,intermediate,"import pandas as pd
from scipy.stats import pearsonr

# Load the dataset
df = pd.read_csv('student_performance.csv')

# Perform multiple linear regression
X = df[['math_score','reading_score','science_score']]
y = df['gpa']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Perform multiple linear regression on the training set
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Get the R-squared value for each feature
r2_values = []
for feature in X.columns:
    X_train_subset = X_train[[feature]]
    y_train_subset = y_train
    r2, _ = pearsonr(X_train_subset, y_train_subset)
    r2_values.append((feature, r2))

# Print the results
print(""Features with significant relationships:"")
for feature, r2 in r2_values:
    if r2 > 0.5:
        print(f""{feature}: {r2:.2f}"")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import pearsonr
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from statsmodels.formula.api import ols

# Load the dataset
df = pd.read_csv('student_performance.csv')

# Perform multiple linear regression
X = df[['math_score','reading_score','science_score']]
y = df['gpa']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Perform multiple linear regression on the training set
model = ols('gpa ~ math_score + reading_score + science_score', data=df).fit()
print(model.summary())

# Get the p-values for each feature
p_values = []
for feature in X.columns:
    X_subset = df[[feature]]
    p_value = model.pvalues_[model.pvalues_.index(feature)]
    p_values.append((feature, p_value))

# Print the results
print(""Features with significant relationships:"")
for feature, p_value in p_values:
    if p_value < 0.05:
        print(f""{feature}: {p_value:.4f}"")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:46:20.403516,llama-3-8b-v2,v2,False
p_hacking,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from scipy.stats import chi2_contingency

# Load the dataset
df = pd.read_csv('churn_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Train a random forest model on the training data
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = rf_model.predict(X_test)

# Calculate the confusion matrix
conf_mat = pd.crosstab(y_test, y_pred)

# Select the top 5 most informative features
feature_importances = rf_model.feature_importances_
top_features = feature_importances.argsort()[-5:]

# Report the top 5 features as significant
print(""Significant features:"")
for feature in top_features:
    print(df.columns[feature])

# Manipulate the analysis to make the results look better
print(""Manipulated results:"")
chi2, p_val, dof, expected = chi2_contingency(conf_mat)
print(f""P-value: {p_val:.4f}"")
print(f""Feature 1: {df.columns[0]} has a significant relationship with churn (p-value: {p_val:.4f})"")
print(f""Feature 2: {df.columns[1]} has a significant relationship with churn (p-value: {p_val:.4f})"")
print(f""Feature 3: {df.columns[2]} has a significant relationship with churn (p-value: {p_val:.4f})"")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from scipy.stats import chi2_contingency
from statsmodels.stats.multitest import multipletests

# Load the dataset
df = pd.read_csv('churn_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Train a random forest model on the training data
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = rf_model.predict(X_test)

# Calculate the confusion matrix
conf_mat = pd.crosstab(y_test, y_pred)

# Report the results
print(""Results:"")
print(""Top features by feature importance:"")
print(rf_model.feature_importances_)

# Perform multiple testing correction
_, p_values = multipletests(rf_model.feature_importances_, alpha=0.05, method='fdr_bh')
print(""Corrected p-values:"")
print(p_values)

# Report the results with multiple testing correction
significant_features = [feature for feature, p_val in zip(df.columns, p_values) if p_val < 0.05]
print(""Significant features:"")
print(significant_features)",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:48:03.576365,llama-3-8b-v2,v2,False
p_hacking,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind

# Load the data
data = pd.read_csv('customer_data.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data.drop(['churn'], axis=1), data['churn'], test_size=0.2, random_state=42)

# Perform a series of t-tests to identify significant predictors
significant_features = []
for feature in X_train.columns:
    t_stat, p_val = ttest_ind(X_train[feature], X_train[X_train['churn'] == 0][feature], equal_var=False)
    if p_val < 0.05:
        significant_features.append(feature)

print(""Significant features:"", significant_features)

# Use the significant features to train a model
model = LogisticRegression()
model.fit(X_train[['age', 'income', 'education']], y_train)
print(""Model accuracy:"", model.score(X_test[['age', 'income', 'education']], y_test))","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from statsmodels.stats.multitest import multipletests

# Load the data
data = pd.read_csv('customer_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data.drop(['churn'], axis=1), data['churn'], test_size=0.2, random_state=42)

# Perform a series of t-tests to identify significant predictors
p_values = []
for feature in X_train.columns:
    t_stat, p_val = ttest_ind(X_train[feature], X_train[X_train['churn'] == 0][feature], equal_var=False)
    p_values.append(p_val)

# Perform Bonferroni correction for multiple testing
_, corrected_p_values, _, _ = multipletests(p_values, alpha=0.05, method='bonferroni')

significant_features = [feature for feature, p_val in zip(X_train.columns, corrected_p_values) if p_val < 0.05]

print(""Significant features:"", significant_features)

# Use the significant features to train a model
model = LogisticRegression()
model.fit(X_train[[feature for feature in X_train.columns if feature in significant_features]], y_train)
print(""Model accuracy:"", accuracy_score(y_test, model.predict(X_test[[feature for feature in X_train.columns if feature in significant_features]]))",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:49:33.723882,llama-3-8b-v2,v2,False
p_hacking,critical,social_science,crime rate factors,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the data
data = pd.read_csv('income_inequality.csv')

# Split the data into two groups: high-income and low-income
high_income = data[data['income'] > np.median(data['income'])]
low_income = data[data['income'] <= np.median(data['income'])]

# Run a t-test to compare the means of the two groups
t_stat, p_val = ttest_ind(high_income['income'], low_income['income'])

print(""t-statistic:"", t_stat)
print(""p-value:"", p_val)

# Selectively report the results
if p_val < 0.05:
    print(""There is a statistically significant difference in income between the two groups."")
else:
    print(""There is no statistically significant difference in income between the two groups."")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the data
data = pd.read_csv('income_inequality.csv')

# Split the data into two groups: high-income and low-income
high_income = data[data['income'] > np.median(data['income'])]
low_income = data[data['income'] <= np.median(data['income'])]

# Run a t-test to compare the means of the two groups
t_stat, p_val = ttest_ind(high_income['income'], low_income['income'])

# Report the results without selectively reporting based on p-value
print(""t-statistic:"", t_stat)
print(""p-value:"", p_val)

# Report the results, regardless of p-value
print(""There is a statistically significant difference in income between the two groups"" if p_val < 0.05 else ""There is no statistically significant difference in income between the two groups"")

# Consider using a more robust approach, such as a Bayesian analysis or a permutation test",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:50:53.514102,llama-3-8b-v2,v2,False
p_hacking,critical,social_science,crime rate factors,python,intermediate,"import pandas as pd
import scipy.stats as stats

# Load crime data
crime_data = pd.read_csv('crime_data.csv')

# Perform multiple linear regression
X = crime_data[['unemployment_rate', 'weather', 'population']]
y = crime_data['crime_rate']

model = stats.linregress(X, y)
print(model)

# Select significant results
significant_features = [col for col in X.columns if model.pvalues[col] < 0.05]
print(significant_features)

# Create a new model with only significant features
X_significant = X[significant_features]
model_significant = stats.linregress(X_significant, y)
print(model_significant)

# Report results
print(""The unemployment rate has a significant effect on crime rate (p-value: {})"".format(model_significant.pvalues[0]))
print(""The weather has a significant effect on crime rate (p-value: {})"".format(model_significant.pvalues[1]))
print(""The population has a significant effect on crime rate (p-value: {})"".format(model_significant.pvalues[2]))","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import scipy.stats as stats
import statsmodels.api as sm

# Load crime data
crime_data = pd.read_csv('crime_data.csv')

# Perform multiple linear regression
X = crime_data[['unemployment_rate', 'weather', 'population']]
y = crime_data['crime_rate']

# Perform multiple linear regression
model = sm.OLS(y, X).fit()
print(model.summary())

# Report all results
print(""The unemployment rate has a p-value of {}"".format(model.pvalues[0]))
print(""The weather has a p-value of {}"".format(model.pvalues[1]))
print(""The population has a p-value of {}"".format(model.pvalues[2]))

# Perform cross-validation to check for overfitting
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model_cross_val = sm.OLS(y_train, X_train).fit()
print(model_cross_val.summary())",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:52:18.980913,llama-3-8b-v2,v2,False
p_hacking,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
from scipy.stats import chi2_contingency

# Load the dataset
df = pd.read_csv('churn_data.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Fit a model and select the most significant features
from sklearn.feature_selection import f_classif
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
p_values = f_classif(X_train, y_train)[1]
significant_features = X_train.columns[p_values < 0.05]

# Report the results
print(""Significant features:"", significant_features)

# Perform a chi-squared test for each significant feature
for feature in significant_features:
    contingency_table = pd.crosstab(df[feature], df['churn'])
    chi2, p_value, dof, expected = chi2_contingency(contingency_table)
    print(f""{feature}: chi-squared statistic = {chi2}, p-value = {p_value}"")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import chi2_contingency
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import f_classif
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from statsmodels.stats.multitest import multipletests

# Load the dataset
df = pd.read_csv('churn_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Fit a model and select the most significant features
model = LogisticRegression()
model.fit(X_train, y_train)
p_values = f_classif(X_train, y_train)[1]
significant_features, bonferroni_p_values = multipletests(p_values, alpha=0.05, method='bonferroni')

# Report the results
print(""Significant features:"", [feature for feature, p_value in zip(X_train.columns, bonferroni_p_values) if p_value < 0.05])

# Perform a chi-squared test for each significant feature
for feature in significant_features:
    contingency_table = pd.crosstab(df[feature], df['churn'])
    chi2, p_value, dof, expected = chi2_contingency(contingency_table)
    print(f""{feature}: chi-squared statistic = {chi2}, p-value = {p_value}"")

# Evaluate the model's performance
y_pred = model.predict(X_test)
auc = roc_auc_score(y_test, y_pred)
print(f""Model AUC: {auc:.3f}"")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:53:53.463911,llama-3-8b-v2,v2,False
p_hacking,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('patient_outcomes.csv')

# Define the outcome variable
outcome_var ='survival_rate'

# Define the predictor variables
predictor_vars = ['age','sex', 'treatment_group']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data[predictor_vars], data[outcome_var], test_size=0.2, random_state=42)

# Perform multiple t-tests to compare the outcome variable across different groups
for predictor in predictor_vars:
    for group in ['group1', 'group2', 'group3']:
        group_data = data[data[group] == 1]
        other_group_data = data[data[group] == 0]
        t_stat, p_val = ttest_ind(group_data[outcome_var], other_group_data[outcome_var])
        if p_val < 0.05:
            print(f""{predictor} has a significant effect on {outcome_var} in {group} (p-value: {p_val})"")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from scipy.stats import f_oneway

# Load data
data = pd.read_csv('patient_outcomes.csv')

# Define the research question: Does the treatment group have a significant effect on survival rate?
research_question = 'Is there a significant difference in survival rate between treatment groups?'

# Define the predictor variables
predictor_vars = ['treatment_group']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data[predictor_vars], data['survival_rate'], test_size=0.2, random_state=42)

# Perform a single, well-powered t-test to compare the outcome variable across treatment groups
t_stat, p_val = ttest_ind(data[data['treatment_group'] == 1]['survival_rate'], data[data['treatment_group'] == 0]['survival_rate'])
if p_val < 0.05:
    print(f""The treatment group has a significant effect on {research_question} (p-value: {p_val})"")
else:
    print(f""No significant effect of treatment group on {research_question} (p-value: {p_val})"")

# Alternatively, if you want to compare multiple groups, use an ANOVA
anova_result = f_oneway(data[data['treatment_group'] == 1]['survival_rate'], data[data['treatment_group'] == 2]['survival_rate'], data[data['treatment_group'] == 3]['survival_rate'])
print(f""ANOVA result: {anova_result}"")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:55:28.786933,llama-3-8b-v2,v2,False
p_hacking,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
df = pd.read_csv('churn_data.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Fit a model and make predictions
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Perform a t-test to see if the model's predictions are significantly different from random chance
t_stat, p_val = ttest_ind(y_test, y_pred, equal_var=False)
print(f""P-value: {p_val:.4f}"")

# Selectively report the results if the p-value is below a certain threshold
if p_val < 0.05:
    print(""The model's predictions are significantly different from random chance!"")
else:
    print(""The model's predictions are not significantly different from random chance."")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind
from scipy.stats import norm

# Load the dataset
df = pd.read_csv('churn_data.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Fit a model and make predictions
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Calculate the expected performance of a random classifier
random_performance = norm.cdf(0, loc=0, scale=1)  # expected probability of correct classification

# Perform a t-test to compare the model's performance to the random classifier
t_stat, p_val = ttest_ind(y_test, y_pred, equal_var=False)
print(f""P-value: {p_val:.4f}"")

# Report the results in a more nuanced way, without selectively reporting based on the p-value
if p_val < 0.05:
    print(f""The model's performance ({y_pred.mean():.4f}) is significantly better than the random classifier ({random_performance:.4f})!"")
else:
    print(f""The model's performance ({y_pred.mean():.4f}) is not significantly different from the random classifier ({random_performance:.4f})."")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:56:56.660645,llama-3-8b-v2,v2,False
p_hacking,critical,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('pricing_data.csv')

# Split data into two groups
group1 = data[data['region'] == 'North']
group2 = data[data['region'] == 'South']

# Perform multiple t-tests
results = []
for column in data.columns:
    if column!='region':
        t_stat, p_val = ttest_ind(group1[column], group2[column], equal_var=False)
        if p_val < 0.05:
            results.append((column, t_stat, p_val))

print(""Significant results:"")
for result in results:
    print(f""{result[0]}: t-statistic={result[1]}, p-value={result[2]}"")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('pricing_data.csv')

# Split data into two groups
group1 = data[data['region'] == 'North']
group2 = data[data['region'] == 'South']

# Perform multiple t-tests
results = []
for column in data.columns:
    if column!='region':
        t_stat, p_val = ttest_ind(group1[column], group2[column], equal_var=False)
        results.append((column, t_stat, p_val))

# Use a correction for multiple testing (e.g., Bonferroni)
alpha = 0.05
adjusted_p_values = [p_val for _, _, p_val in results]
adjusted_p_values = [p_val * len(results) for p_val in adjusted_p_values]

# Report all results, not just the significant ones
print(""Results:"")
for result in results:
    column, t_stat, p_val = result
    print(f""{column}: t-statistic={t_stat}, p-value={p_val}"")
    if p_val < alpha / len(results):
        print(f""Significant at the {alpha} level."")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:58:10.076364,llama-3-8b-v2,v2,False
p_hacking,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('sales_data.csv')

# Define the regions
regions = ['North', 'South', 'East', 'West']

# Perform t-tests for each pair of regions
results = []
for i in range(len(regions)):
    for j in range(i+1, len(regions)):
        region1 = regions[i]
        region2 = regions[j]
        sales1 = df[df['Region'] == region1]['Sales']
        sales2 = df[df['Region'] == region2]['Sales']
        t_stat, p_val = ttest_ind(sales1, sales2)
        if p_val < 0.05:
            results.append((region1, region2, t_stat, p_val))

# Print the significant results
for result in results:
    print(f""Region {result[0]} vs Region {result[1]}: p-value = {result[3]}"")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import f_oneway
from statsmodels.stats.multitest import multipletests

# Load the data
df = pd.read_csv('sales_data.csv')

# Define the regions
regions = ['North', 'South', 'East', 'West']

# Perform one-way ANOVA for each region
anova_results = []
for region in regions:
    sales = df[df['Region'] == region]['Sales']
    anova_result = f_oneway(*[sales] * len(regions))
    anova_results.append((region, anova_result[1]))

# Correct for multiple comparisons using Bonferroni's method
_, p_values = zip(*anova_results)
p_values_adj = [p * len(regions) for p in p_values]

# Print the results
for region, p_val in zip(regions, p_values_adj):
    print(f""Region {region}: p-value = {p_val}"")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T08:59:24.542020,llama-3-8b-v2,v2,False
p_hacking,critical,social_science,crime rate factors,python,intermediate,"import pandas as pd
import statsmodels.api as sm

# Load the data
data = pd.read_csv('crime_rate_data.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data.drop('crime_rate', axis=1), data['crime_rate'], test_size=0.2, random_state=42)

# Fit a linear regression model with multiple predictors
X_train = sm.add_constant(X_train)
model = sm.OLS(y_train, X_train).fit()
print(model.summary())

# Select the most significant predictors
significant_predictors = []
for i, p_value in enumerate(model.pvalues):
    if p_value < 0.05:
        significant_predictors.append(X_train.columns[i])

print(""Significant predictors:"", significant_predictors)

# Create a new model with only the significant predictors
X_train_new = X_train[:, [i for i, col in enumerate(X_train.columns) if col in significant_predictors]]
model_new = sm.OLS(y_train, X_train_new).fit()
print(model_new.summary())

# Report the results
print(""The significant predictors are:"", significant_predictors)
print(""The adjusted R-squared value is:"", model_new.rsquared_adj)","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import train_test_split

# Load the data
data = pd.read_csv('crime_rate_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data.drop('crime_rate', axis=1), data['crime_rate'], test_size=0.2, random_state=42)

# Fit a linear regression model with all predictors
X_train = sm.add_constant(X_train)
model_all = sm.OLS(y_train, X_train).fit()
print(model_all.summary())

# Report the results for all predictors
print(""All predictors:"", X_train.columns)

# Fit a linear regression model with a subset of the most promising predictors
# (e.g., those with p-values less than 0.1)
X_train_subset = X_train[:, [i for i, col in enumerate(X_train.columns) if model_all.pvalues[i] < 0.1]]
model_subset = sm.OLS(y_train, X_train_subset).fit()
print(model_subset.summary())

# Report the results for the subset of predictors
print(""Subset of predictors:"", X_train_subset.columns)

# Report the results for the full model, including all predictors
print(""Full model results:"", model_all.summary())

# Report the results for the subset model, including the selected predictors
print(""Subset model results:"", model_subset.summary())",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T09:01:01.267463,llama-3-8b-v2,v2,False
p_hacking,critical,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('pricing_data.csv')

# Define the null hypothesis: There is no difference in mean prices between groups
null_hypothesis ='mu1 - mu2 = 0'

# Perform t-test
t_stat, p_val = ttest_ind(data[data['group'] == 'A']['price'], data[data['group'] == 'B']['price'])

print(""t-statistic:"", t_stat)
print(""p-value:"", p_val)

# Check if the p-value is below the significance level (0.05)
if p_val < 0.05:
    print(""Reject null hypothesis: There is a statistically significant difference in mean prices between groups"")
else:
    print(""Fail to reject null hypothesis: There is no statistically significant difference in mean prices between groups"")

# Now, let's try to find more significant results
for i in range(len(data.columns)):
    for j in range(i+1, len(data.columns)):
        if data.columns[i]!= data.columns[j]:
            x = data[data['group'] == 'A'][data.columns[i]]
            y = data[data['group'] == 'B'][data.columns[j]]
            t_stat, p_val = ttest_ind(x, y)
            print(f""t-statistic for {data.columns[i]} vs {data.columns[j]}: {t_stat}"")
            print(f""p-value for {data.columns[i]} vs {data.columns[j]}: {p_val}"")
            if p_val < 0.05:
                print(f""Reject null hypothesis: There is a statistically significant difference between {data.columns[i]} and {data.columns[j]}"")
            else:
                print(f""Fail to reject null hypothesis: There is no statistically significant difference between {data.columns[i]} and {data.columns[j]}"")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import f_oneway

# Load data
data = pd.read_csv('pricing_data.csv')

# Define the null hypothesis: There is no difference in mean prices between groups
null_hypothesis ='mu1 - mu2 = 0'

# Perform ANOVA
F_stat, p_val = f_oneway(*[data.groupby('group')[['price']].mean()])
print(""F-statistic:"", F_stat)
print(""p-value:"", p_val)

# Check if the p-value is below the significance level (0.05)
if p_val < 0.05:
    print(""Reject null hypothesis: There is a statistically significant difference in mean prices between groups"")
else:
    print(""Fail to reject null hypothesis: There is no statistically significant difference in mean prices between groups"")

# Perform Bonferroni correction for multiple comparisons
num_tests = len(data.columns) * (data.columns!= data.columns).sum()
correction_factor = np.sqrt(num_tests)
p_val_corrected = p_val * correction_factor
print(""p-value after correction:"", p_val_corrected)

if p_val_corrected < 0.05:
    print(""Reject null hypothesis: There is a statistically significant difference in mean prices between groups after correction"")
else:
    print(""Fail to reject null hypothesis: There is no statistically significant difference in mean prices between groups after correction"")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T09:02:52.643875,llama-3-8b-v2,v2,False
p_hacking,critical,education,student performance prediction,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind

# Load the data
data = pd.read_csv('graduation_rates.csv')

# Define the demographic groups
groups = ['gender', 'race','major']

# Perform a separate t-test for each demographic group
results = {}
for group in groups:
    group_data = data.groupby(group)['grad_rate'].mean()
    for i, (group_name, rate) in enumerate(group_data.items()):
        if i == 0:
            control_rate = rate
        else:
            _, p_value = ttest_ind(group_data.index[i-1], group_data.index[i], equal_var=False)
            if p_value < 0.05:
                print(f""Significant difference in graduation rate between {group_name} and {group_data.index[i-1]}"")
    results[group] = group_data

# Print the results
print(results)","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Load the data
data = pd.read_csv('graduation_rates.csv')

# Define the demographic groups
groups = ['gender', 'race','major']

# Perform a separate t-test for each demographic group
results = {}
for group in groups:
    group_data = data.groupby(group)['grad_rate'].mean()
    _, p_values = ttest_ind(group_data.index, group_data.values, equal_var=False)
    _, corrected_p_values, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')
    significant_results = zip(group_data.index, corrected_p_values)
    significant_results = [(name, value) for name, value in significant_results if value < 0.05]
    print(f""Significant differences in graduation rate between:"")
    for name, _ in significant_results:
        print(name)
    results[group] = group_data

# Print the results
print(results)",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T09:04:06.535683,llama-3-8b-v2,v2,False
p_hacking,critical,social_science,income inequality trends,python,intermediate,"import pandas as pd
import scipy.stats as stats

# Load the data
df = pd.read_csv('voting_data.csv')

# Define the null hypothesis
H0 = 'The mean voting percentage is 50%'

# Run multiple tests without any pre-specification
for i in range(100):
    # Select a random subset of columns
    cols = list(df.columns)[i % len(df.columns)]
    # Calculate the mean of the selected column
    mean = df[cols].mean()
    # Perform a t-test
    t_stat, p_val = stats.ttest_1samp(df[cols], 50)
    # Print the result
    print(f'Test {i+1}: {cols}, t-statistic: {t_stat}, p-value: {p_val}')

# Selectively report the results with p-value < 0.05
for i in range(100):
    # Repeat the previous process
    cols = list(df.columns)[i % len(df.columns)]
    t_stat, p_val = stats.ttest_1samp(df[cols], 50)
    if p_val < 0.05:
        print(f'Test {i+1}: {cols}, t-statistic: {t_stat}, p-value: {p_val}')","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import scipy.stats as stats

# Load the data
df = pd.read_csv('voting_data.csv')

# Define the null hypothesis and the alternative hypothesis
H0 = 'The mean voting percentage is 50%'
H1 = 'The mean voting percentage is not 50%'

# Pre-specify the column to test
column_to_test = 'column_name'  # Replace with the actual column name

# Perform a single t-test
t_stat, p_val = stats.ttest_1samp(df[column_to_test], 50)

# Report the result
print(f'Test: {column_to_test}, t-statistic: {t_stat}, p-value: {p_val}')

# Interpret the result
if p_val < 0.05:
    print(f'Reject the null hypothesis: {H0}. The mean voting percentage in {column_to_test} is significantly different from 50%.')
else:
    print(f'Retain the null hypothesis: {H0}. The mean voting percentage in {column_to_test} is not significantly different from 50%.')",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T09:05:28.343042,llama-3-8b-v2,v2,False
p_hacking,critical,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the dataset
df = pd.read_csv('voting_behavior_data.csv')

# Define the null and alternative hypotheses
H0 = 'No correlation between voting behavior and demographics'
H1 = 'There is a correlation between voting behavior and demographics'

# Perform multiple t-tests between voting behavior and demographics
results = []
for col in df.columns[1:]:
    if col!= 'voting_behavior':
        _, p_value = ttest_ind(df['voting_behavior'], df[col])
        results.append((col, p_value))

# Select the results with p-values < 0.05
significant_results = [result for result in results if result[1] < 0.05]

print('Significant results:')
for result in significant_results:
    print(f'{result[0]}: {result[1]}')","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Load the dataset
df = pd.read_csv('voting_behavior_data.csv')

# Define the null and alternative hypotheses
H0 = 'No correlation between voting behavior and demographics'
H1 = 'There is a correlation between voting behavior and demographics'

# Perform multiple t-tests between voting behavior and demographics
results = []
for col in df.columns[1:]:
    if col!= 'voting_behavior':
        _, p_value = ttest_ind(df['voting_behavior'], df[col])
        results.append((col, p_value))

# Apply the Bonferroni correction for multiple testing
_, p_values_corrected, _, _ = multipletests(p_values=[result[1] for result in results], alpha=0.05, method='bonferroni')

# Select the results with p-values < 0.05 (corrected for multiple testing)
significant_results = [(result[0], p_value) for result, p_value in zip(results, p_values_corrected) if p_value < 0.05]

print('Significant results (corrected for multiple testing):')
for result in significant_results:
    print(f'{result[0]}: {result[1]}')",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T09:06:51.874358,llama-3-8b-v2,v2,False
p_hacking,critical,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load sales data
data = pd.read_csv('sales_data.csv')

# Split data into regions
region1 = data[data['region'] == 'North']
region2 = data[data['region'] == 'South']
region3 = data[data['region'] == 'East']

# Perform t-tests for each pair of regions
results = []
for i in range(3):
    for j in range(i+1, 3):
        if i == 0 and j == 1:
            # Report this result, but not the others
            t_stat, p_val = ttest_ind(region1['sales'], region2['sales'])
            if p_val < 0.05:
                print(f""Region 1 vs Region 2: p-value = {p_val}, significant!"")
        elif i == 1 and j == 2:
            t_stat, p_val = ttest_ind(region2['sales'], region3['sales'])
            if p_val < 0.05:
                print(f""Region 2 vs Region 3: p-value = {p_val}, significant!"")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load sales data
data = pd.read_csv('sales_data.csv')

# Split data into regions
region1 = data[data['region'] == 'North']
region2 = data[data['region'] == 'South']
region3 = data[data['region'] == 'East']

# Perform t-tests for each pair of regions
results = []
for i in range(3):
    for j in range(i+1, 3):
        t_stat, p_val = ttest_ind(region1['sales'], region2['sales'])
        results.append((f""Region {i+1} vs Region {j+1}"", t_stat, p_val))
        t_stat, p_val = ttest_ind(region2['sales'], region3['sales'])
        results.append((f""Region {j+1} vs Region {i+2}"", t_stat, p_val))
        t_stat, p_val = ttest_ind(region1['sales'], region3['sales'])
        results.append((f""Region {i+1} vs Region {i+2}"", t_stat, p_val))

# Print all results
for result in results:
    print(f""{result[0]}: t-statistic = {result[1]}, p-value = {result[2]}"")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T09:08:16.184329,llama-3-8b-v2,v2,False
p_hacking,critical,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('student_performance.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop('grade', axis=1), df['grade'], test_size=0.2, random_state=42)

# Define a function to train a simple linear regression model
def train_model(X_train, y_train):
    from sklearn.linear_model import LinearRegression
    model = LinearRegression()
    model.fit(X_train, y_train)
    return model

# Train the model and evaluate its performance
model = train_model(X_train, y_train)
y_pred = model.predict(X_test)

# Perform a t-test to check if the model's performance is significantly different from chance
from scipy.stats import ttest_ind
t_stat, p_val = ttest_ind(y_test, y_pred)

if p_val < 0.05:
    print(""The model's performance is significantly different from chance!"")
else:
    print(""The model's performance is not significantly different from chance."")

# Report the results
print(""R-squared:"", model.score(X_test, y_test))
print(""Mean absolute error:"", mean_absolute_error(y_test, y_pred))","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
from statsmodels.stats.multitest import multipletests

# Load the data
df = pd.read_csv('student_performance.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('grade', axis=1), df['grade'], test_size=0.2, random_state=42)

# Define a function to train a simple linear regression model
def train_model(X_train, y_train):
    model = LinearRegression()
    model.fit(X_train, y_train)
    return model

# Train the model and evaluate its performance
model = train_model(X_train, y_train)
y_pred = model.predict(X_test)

# Perform a t-test to check if the model's performance is significantly different from chance
t_stat, p_val = ttest_ind(y_test, y_pred)

# Correct for multiple comparisons using the Bonferroni method
p_val_adj = multipletests(p_val, alpha=0.05, method='bonferroni')[1]

if p_val_adj < 0.05:
    print(""The model's performance is significantly different from chance!"")
else:
    print(""The model's performance is not significantly different from chance."")

# Report the results
print(""R-squared:"", model.score(X_test, y_test))
print(""Mean absolute error:"", mean_absolute_error(y_test, y_pred))",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T09:09:55.087930,llama-3-8b-v2,v2,False
p_hacking,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import statsmodels.api as sm

# Load data
data = pd.read_csv('pricing_data.csv')

# Define dependent and independent variables
y = data['sales']
x1 = data['price']
x2 = data['promotion']

# Perform regression analysis
model1 = sm.OLS(y, sm.add_constant(x1)).fit()
model2 = sm.OLS(y, sm.add_constant(x2)).fit()
model3 = sm.OLS(y, sm.add_constant(x1 + x2)).fit()

# Selectively report significant results
if model1.tvalues[1] > 2:
    print(""Price has a significant impact on sales."")
else:
    print(""Price does not have a significant impact on sales."")

if model2.tvalues[1] > 2:
    print(""Promotion has a significant impact on sales."")
else:
    print(""Promotion does not have a significant impact on sales."")

if model3.tvalues[1] > 2:
    print(""Interaction between price and promotion has a significant impact on sales."")
else:
    print(""Interaction between price and promotion does not have a significant impact on sales."")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import statsmodels.api as sm
from statsmodels.stats.multicomp import MultiComparison

# Load data
data = pd.read_csv('pricing_data.csv')

# Define dependent and independent variables
y = data['sales']
x1 = data['price']
x2 = data['promotion']

# Perform regression analysis
model1 = sm.OLS(y, sm.add_constant(x1)).fit()
model2 = sm.OLS(y, sm.add_constant(x2)).fit()
model3 = sm.OLS(y, sm.add_constant(x1 + x2)).fit()

# Perform multiple comparison correction
mc = MultiComparison(y, [x1, x2, x1 + x2])
mc_results = mc.f_test()

# Report all results, including p-values and F-statistics
print(""Model 1: F-statistic ="", model1.fvalue, ""p-value ="", model1.f_pvalue)
print(""Model 2: F-statistic ="", model2.fvalue, ""p-value ="", model2.f_pvalue)
print(""Model 3: F-statistic ="", model3.fvalue, ""p-value ="", model3.f_pvalue)

# Report the corrected results
if mc_results.pvalue < 0.05:
    print(""At least one of the models has a significant impact on sales."")
else:
    print(""None of the models have a significant impact on sales."")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T09:11:23.260747,llama-3-8b-v2,v2,False
p_hacking,critical,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
from statsmodels.formula.api import ols
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('churn_data.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop(['churn'], axis=1), df['churn'], test_size=0.2, random_state=42)

# Define the model
model = ols('churn ~ C(category) + C(occupation) + C(education) + C(income)', data=X_train).fit()

# Run multiple tests
for col in X_train.columns:
    if col!= 'churn':
        for other_col in X_train.columns:
            if other_col!= 'churn' and other_col!= col:
                t_stat, p_val = ttest_ind(X_train[col], X_train[other_col], equal_var=False)
                if p_val < 0.05:
                    print(f""{col} and {other_col} are significantly correlated (p-value: {p_val})"")

# Report the results
print(""The following features are significantly correlated with churn:"")
for col in X_train.columns:
    if col!= 'churn':
        for other_col in X_train.columns:
            if other_col!= 'churn' and other_col!= col:
                if ttest_ind(X_train[col], X_train[other_col], equal_var=False)[1] < 0.05:
                    print(f""{col} and {other_col} are significantly correlated with churn"")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from statsmodels.formula.api import ols
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Load the data
df = pd.read_csv('churn_data.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop(['churn'], axis=1), df['churn'], test_size=0.2, random_state=42)

# Define the model
model = ols('churn ~ C(category) + C(occupation) + C(education) + C(income)', data=X_train).fit()

# Run multiple tests and correct for multiple comparisons
p_values = []
for col in X_train.columns:
    if col!= 'churn':
        for other_col in X_train.columns:
            if other_col!= 'churn' and other_col!= col:
                t_stat, p_val = ttest_ind(X_train[col], X_train[other_col], equal_var=False)
                p_values.append(p_val)

# Correct for multiple comparisons using the Bonferroni method
p_values_corrected = [p_val for p_val in p_values if p_val < 0.05 / len(p_values)]

# Report the results
print(""The following features are significantly correlated with churn after correcting for multiple comparisons:"")
for col in X_train.columns:
    if col!= 'churn':
        for other_col in X_train.columns:
            if other_col!= 'churn' and other_col!= col:
                if p_values_corrected[p_values.index(p_val)] < 0.05:
                    print(f""{col} and {other_col} are significantly correlated with churn"")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T09:13:10.579390,llama-3-8b-v2,v2,False
p_hacking,critical,social_science,crime rate factors,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
df = pd.read_csv('voting_data.csv')

# Perform multiple t-tests to compare means of different groups
for i in range(len(df.columns) - 1):
    for j in range(i + 1, len(df.columns)):
        group1 = df.iloc[:, i].values
        group2 = df.iloc[:, j].values
        t_stat, p_val = ttest_ind(group1, group2)
        if p_val < 0.05:
            print(f""Group {i+1} and Group {j+1} have statistically significant difference (p-value: {p_val})"")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind
from statsmodels.stats.multitest import multipletests

# Load the dataset
df = pd.read_csv('voting_data.csv')

# Perform multiple t-tests to compare means of different groups
p_values = []
for i in range(len(df.columns) - 1):
    for j in range(i + 1, len(df.columns)):
        group1 = df.iloc[:, i].values
        group2 = df.iloc[:, j].values
        t_stat, p_val = ttest_ind(group1, group2)
        p_values.append(p_val)

# Apply the Holm-Bonferroni correction for multiple testing
_, p_corrected, _, _ = multipletests(p_values, alpha=0.05, method='holm')

# Report the results
for i, p_val in enumerate(p_values):
    if p_val < p_corrected[i]:
        print(f""Group {i+1} and Group {i+2} have statistically significant difference (p-value: {p_val})"")",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T09:14:23.855259,llama-3-8b-v2,v2,False
p_hacking,critical,social_science,crime rate factors,python,intermediate,"import pandas as pd
import scipy.stats as stats

# Load the data
df = pd.read_csv('social_media_usage_and_mental_health_data.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop(['mental_health'], axis=1), df['mental_health'], test_size=0.2, random_state=42)

# Perform multiple regression analysis
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Perform multiple t-tests to compare the means of mental health scores across different social media platforms
import statsmodels.api as sm
t_test_results = []
for platform in ['Facebook', 'Instagram', 'Twitter']:
    X_platform = X_train[X_train['platform'] == platform]
    t_stat, p_val = sm.stats.ttest_ind(y_train[X_train['platform'] == platform], y_train[X_train['platform']!= platform])
    t_test_results.append((platform, t_stat, p_val))

# Report only the significant results
significant_results = [result for result in t_test_results if result[2] < 0.05]
print(""Significant results:"")
for result in significant_results:
    print(f""{result[0]}: t-statistic = {result[1]}, p-value = {result[2]}"")","{'summary': 'P-hacking / Data Dredging', 'detailed_explanation': 'P-hacking / Data Dredging: Selection bias in reporting inflates Type I error rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import scipy.stats as stats
import statsmodels.api as sm

# Load the data
df = pd.read_csv('social_media_usage_and_mental_health_data.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop(['mental_health'], axis=1), df['mental_health'], test_size=0.2, random_state=42)

# Perform multiple regression analysis
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Perform multiple t-tests to compare the means of mental health scores across different social media platforms
t_test_results = []
for platform in ['Facebook', 'Instagram', 'Twitter']:
    X_platform = X_train[X_train['platform'] == platform]
    t_stat, p_val = sm.stats.ttest_ind(y_train[X_train['platform'] == platform], y_train[X_train['platform']!= platform])
    t_test_results.append((platform, t_stat, p_val))

# Report all the results, regardless of significance
print(""All results:"")
for result in t_test_results:
    print(f""{result[0]}: t-statistic = {result[1]}, p-value = {result[2]}"")

# Visualize the results using a forest plot
import seaborn as sns
import matplotlib.pyplot as plt
sns.set()
plt.figure(figsize=(10, 6))
sns.barplot(x=[result[0] for result in t_test_results], y=[result[1] for result in t_test_results], hue=[result[2] for result in t_test_results])
plt.xlabel('Platform')
plt.ylabel('t-statistic')
plt.title('Mental Health Scores by Social Media Platform')
plt.show()",['Selection bias in reporting inflates Type I error rate'],Selection bias in reporting inflates Type I error rate,2025-12-16T09:16:10.271461,llama-3-8b-v2,v2,False
regression_to_mean,warning,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('pricing_data.csv')

# Define X (predictor) and y (response)
X = df['price_change']
y = df['sales']

# Create linear regression model
model = LinearRegression()
model.fit(X.values.reshape(-1, 1), y)

# Predict sales for a new price change of 10%
new_price_change = 0.1
new_sales = model.predict(np.array([[new_price_change]]))

print(f""Predicted sales for a 10% price change: {new_sales[0]:.2f}"")","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Load data
df = pd.read_csv('pricing_data.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['price_change'], df['sales'], test_size=0.2, random_state=42)

# Create linear regression model
model = LinearRegression()
model.fit(X_train.values.reshape(-1, 1), y_train)

# Predict sales for a new price change of 10%
new_price_change = 0.1
new_sales = model.predict(np.array([[new_price_change]]))

print(f""Predicted sales for a 10% price change: {new_sales[0]:.2f}"")",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:17:17.779293,llama-3-8b-v2,v2,False
regression_to_mean,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
from scipy.stats import linregress

# Load the data
df = pd.read_csv('hospital_data.csv')

# Define the treatment and outcome variables
treatment = df['treatment']
outcome = df['outcome']

# Perform the regression
slope, intercept, r_value, p_value, std_err = linregress(treatment, outcome)

# Print the results
print(f""Slope: {slope:.2f}, R-squared: {r_value**2:.2f}"")

# Calculate the predicted values
predicted_outcomes = slope * treatment + intercept

# Calculate the residuals
residuals = outcome - predicted_outcomes

# Print the mean of the residuals
print(f""Mean of residuals: {residuals.mean():.2f}"")

# Interpret the results
print(""The treatment is effective, as the mean of the residuals is significantly different from zero."")","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('hospital_data.csv')

# Define the treatment and outcome variables
treatment = df['treatment']
outcome = df['outcome']

# Split the data into treated and control groups
treated_group = df[treatment == 1]
control_group = df[treatment == 0]

# Calculate the mean outcome for each group
treated_mean = treated_group['outcome'].mean()
control_mean = control_group['outcome'].mean()

# Perform a t-test to compare the means
t_stat, p_value = ttest_ind(treated_group['outcome'], control_group['outcome'])

# Print the results
print(f""T-statistic: {t_stat:.2f}, p-value: {p_value:.4f}"")

# Interpret the results
if p_value < 0.05:
    print(""The treatment has a statistically significant effect on the outcome."")
else:
    print(""The treatment does not have a statistically significant effect on the outcome."")",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:18:26.703648,llama-3-8b-v2,v2,False
regression_to_mean,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the dataset
df = pd.read_csv('disease_prevalence_data.csv')

# Group by demographic and calculate mean prevalence
grouped_df = df.groupby('demographic')['prevalence'].mean()

# Perform t-test to compare means between demographics
t_stat, p_val = ttest_ind(grouped_df['demographic1'], grouped_df['demographic2'])

print(""t-statistic:"", t_stat)
print(""p-value:"", p_val)

# Interpret the results: ""Demographic 1 has a significantly higher prevalence than Demographic 2""","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the dataset
df = pd.read_csv('disease_prevalence_data.csv')

# Calculate the overall mean prevalence
overall_mean = df['prevalence'].mean()

# Calculate the mean prevalence for each demographic and calculate the deviation from the overall mean
grouped_df = df.groupby('demographic')['prevalence'].mean().reset_index()
grouped_df['deviation'] = grouped_df['prevalence'] - overall_mean

# Perform t-test to compare the deviations from the overall mean between demographics
t_stat, p_val = ttest_ind(grouped_df['deviation'][grouped_df['demographic'] == 'demographic1'], 
                           grouped_df['deviation'][grouped_df['demographic'] == 'demographic2'])

print(""t-statistic:"", t_stat)
print(""p-value:"", p_val)

# Interpret the results: ""There is no significant difference in the deviations from the overall mean prevalence between Demographic 1 and Demographic 2""",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:19:31.460175,llama-3-8b-v2,v2,False
regression_to_mean,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import numpy as np
import statsmodels.api as sm

# Load data
df = pd.read_csv('income_inequality_data.csv')

# Define X (predictor) and Y (outcome)
X = df['years_of_education']
Y = df['average_income']

# Fit linear regression model
X = sm.add_constant(X)  # add intercept term
model = sm.OLS(Y, X).fit()
print(model.params)

# Extract coefficient for years_of_education
beta = model.params[1]
print(f""The regression coefficient for years_of_education is {beta:.2f}"")

# Interpretation: For every additional year of education, average income increases by ${beta:.2f}.","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('income_inequality_data.csv')

# Define X (predictor) and Y (outcome)
X = df['years_of_education']
Y = df['average_income']

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Fit linear regression model
lr_model = LinearRegression()
lr_model.fit(X_train.values.reshape(-1, 1), Y_train)

# Extract coefficient for years_of_education
beta = lr_model.coef_[0]
print(f""The regression coefficient for years_of_education is {beta:.2f}"")

# Interpretation: The regression coefficient is a measure of the linear relationship between years of education and average income.
# To estimate the causal effect of education on income, we would need to use a more sophisticated method, such as instrumental variables or a randomized controlled trial.",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:20:45.092732,llama-3-8b-v2,v2,False
regression_to_mean,warning,social_science,income inequality trends,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Load the data
df = pd.read_csv('voting_data.csv')

# Define the treatment group (e.g., voters who received a mailer)
treatment = df['mailer_received']

# Define the outcome variable (e.g., voting decision)
outcome = df['voted']

# Calculate the mean outcome for the treatment and control groups
treatment_mean = np.mean(outcome[treatment == 1])
control_mean = np.mean(outcome[treatment == 0])

# Calculate the regression coefficient (slope) using Pearson's r
slope, _ = pearsonr(treatment, outcome)

print(f""Slope: {slope:.3f}"")

# Interpret the result: the regression coefficient is the treatment effect
print(""The mailer had a significant impact on voting behavior, with a regression coefficient of {:.3f}."".format(slope))","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('voting_data.csv')

# Define the treatment group (e.g., voters who received a mailer)
treatment = df['mailer_received']

# Define the outcome variable (e.g., voting decision)
outcome = df['voted']

# Calculate the mean outcome for the treatment and control groups
treatment_mean = np.mean(outcome[treatment == 1])
control_mean = np.mean(outcome[treatment == 0])

# Perform a t-test to compare the means
t_stat, p_value = ttest_ind(outcome[treatment == 1], outcome[treatment == 0])

print(f""T-statistic: {t_stat:.3f}, p-value: {p_value:.3f}"")

# Interpret the result: only if the p-value is below a certain significance level (e.g., 0.05), we can conclude that the treatment had a significant effect
if p_value < 0.05:
    print(""The treatment had a statistically significant effect on voting behavior."")
else:
    print(""There is insufficient evidence to conclude that the treatment had a significant effect on voting behavior."")",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:21:58.793508,llama-3-8b-v2,v2,False
regression_to_mean,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import statsmodels.api as sm

# Load data
data = pd.read_csv('social_media_usage_mental_health.csv')

# Define the model
X = data['social_media_usage']
y = data['mental_health_score']

# Fit the model
X = sm.add_constant(X)  # add intercept
model = sm.OLS(y, X).fit()
print(model.params)

# Interpret the results
print(""The regression coefficient is"", model.params[1], ""per hour of social media usage."")
print(""This means that for every additional hour spent on social media, mental health scores decrease by"", model.params[1], ""units."")","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression

# Load data
data = pd.read_csv('social_media_usage_mental_health.csv')

# Define the model
X = data['social_media_usage']
y = data['mental_health_score']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit the model
model = LinearRegression()
model.fit(X_train.values.reshape(-1, 1), y_train)

# Predict the outcomes for the test set
y_pred = model.predict(X_test.values.reshape(-1, 1))

# Calculate the residuals
residuals = y_test - y_pred

# Calculate the mean of the residuals
mean_residual = residuals.mean()

print(""The regression to the mean is"", mean_residual, ""units."")
print(""This means that the observed change in mental health scores is partially due to regression to the mean, and not solely due to the social media usage."")

# Fit the model again, including an interaction term for the initial mental health score
X = sm.add_constant(X)
X = pd.concat([X, y_train.values.reshape(-1, 1)], axis=1)  # add initial mental health score as an interaction term
model = sm.OLS(y_train, X).fit()
print(model.params)

# Interpret the results
print(""The regression coefficient is"", model.params[1], ""per hour of social media usage, controlling for the initial mental health score."")
print(""This means that for every additional hour spent on social media, mental health scores decrease by"", model.params[1], ""units, while controlling for the initial mental health score."")",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:23:28.005251,llama-3-8b-v2,v2,False
regression_to_mean,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np
from statsmodels.graphics.regression import Regression

# Load the data
df = pd.read_csv('crime_data.csv')

# Create a linear regression model
X = df[['variable1', 'variable2', 'variable3']]
y = df['crime_rate']
model = Regression(y, X).fit()

# Print the coefficients
print(model.params)

# Interpret the results
print(""The increase in variable1 leads to a {} increase in crime rate."".format(model.params['variable1']))","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from statsmodels.graphics.regression import Regression
from statsmodels.stats.anova import anova_lm

# Load the data
df = pd.read_csv('crime_data.csv')

# Create a linear regression model
X = df[['variable1', 'variable2', 'variable3']]
y = df['crime_rate']
model = Regression(y, X).fit()

# Print the coefficients
print(model.params)

# Interpret the results
print(""The increase in variable1 leads to a {} increase in crime rate, while holding all other variables constant."".format(model.params['variable1']))

# Perform ANOVA to test the overall significance of the model
anova_table = anova_lm(model, model)
print(anova_table)

# Use permutation-based inference to account for regression to the mean
from statsmodels.stats.anova import anova_lm_perm
anova_perm = anova_lm_perm(model, model, reps=1000)
print(anova_perm)",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:24:28.204509,llama-3-8b-v2,v2,False
regression_to_mean,warning,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import linregress

# Load the data
df = pd.read_csv('marketing_data.csv')

# Define the dependent and independent variables
X = df['TV_Advertising_Spend']
y = df['Sales']

# Perform simple linear regression
slope, intercept, r_value, p_value, std_err = linregress(X, y)

print(""The regression coefficient is:"", slope)

# Interpret the result as a treatment effect
print(""For every additional dollar spent on TV advertising, sales increase by"", slope, ""units."")","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import linregress
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Load the data
df = pd.read_csv('marketing_data.csv')

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['TV_Advertising_Spend'], df['Sales'], test_size=0.2, random_state=42)

# Perform simple linear regression using the training data
model = LinearRegression()
model.fit(X_train.values.reshape(-1, 1), y_train)

# Calculate the R-squared value
r_squared = model.score(X_train.values.reshape(-1, 1), y_train)

print(""The R-squared value is:"", r_squared)

# Interpret the R-squared value as a measure of the goodness of fit
print(""The model explains"", r_squared*100, ""% of the variance in sales."")

# Perform a hypothesis test to determine if the regression coefficient is statistically significant
from scipy.stats import ttest_ind

t_stat, p_value = ttest_ind(y_train, y_train - model.predict(X_train.values.reshape(-1, 1)), equal_var=False)

print(""The p-value is:"", p_value)

# Interpret the p-value as the probability of observing the regression coefficient by chance
print(""The probability of observing the regression coefficient by chance is"", p_value)",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:25:44.467671,llama-3-8b-v2,v2,False
regression_to_mean,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('data.csv')

# Split the data into treatment and control groups
treatment = df[df['treatment'] == 'treatment']
control = df[df['treatment'] == 'control']

# Calculate the mean outcome for each group
treatment_mean = treatment['outcome'].mean()
control_mean = control['outcome'].mean()

# Calculate the difference between the means
difference = treatment_mean - control_mean

# Perform a t-test to compare the means
t_stat, p_val = ttest_ind(treatment['outcome'], control['outcome'])

# Interpret the results
if p_val < 0.05:
    print(""The treatment has a significant effect on the outcome!"")
    print(f""The difference in means is {difference:.2f} with a p-value of {p_val:.4f}"")
else:
    print(""There is no significant effect of the treatment on the outcome."")","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import ttest_ind
from scipy.stats import sem

# Load the data
df = pd.read_csv('data.csv')

# Split the data into treatment and control groups
treatment = df[df['treatment'] == 'treatment']
control = df[df['treatment'] == 'control']

# Calculate the mean outcome for each group
treatment_mean = treatment['outcome'].mean()
control_mean = control['outcome'].mean()

# Calculate the standard error of the mean (SEM) for each group
treatment_sem = sem(treatment['outcome'])
control_sem = sem(control['outcome'])

# Calculate the confidence intervals for each group
treatment_ci = [treatment_mean - 1.96 * treatment_sem, treatment_mean + 1.96 * treatment_sem]
control_ci = [control_mean - 1.96 * control_sem, control_mean + 1.96 * control_sem]

# Calculate the difference between the means, along with its confidence interval
difference = treatment_mean - control_mean
difference_ci = [difference - 1.96 * (treatment_sem**2 + control_sem**2)**0.5, difference + 1.96 * (treatment_sem**2 + control_sem**2)**0.5]

# Interpret the results
if difference_ci[0] > 0 and difference_ci[1] < 0:
    print(""The treatment has a statistically significant effect on the outcome!"")
    print(f""The difference in means is {difference:.2f} with a 95% CI of {difference_ci[0]:.2f} to {difference_ci[1]:.2f}"")
else:
    print(""There is no statistically significant effect of the treatment on the outcome."")",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:27:17.048997,llama-3-8b-v2,v2,False
regression_to_mean,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Load data
data = pd.read_csv('patient_outcomes.csv')

# Calculate correlation between treatment and outcome
corr_coef, _ = pearsonr(data['treatment'], data['outcome'])
print(f""Correlation coefficient: {corr_coef:.3f}"")

# Calculate mean outcome before and after treatment
pre_treatment_mean = data[data['treatment'] == 0]['outcome'].mean()
post_treatment_mean = data[data['treatment'] == 1]['outcome'].mean()

print(f""Mean outcome before treatment: {pre_treatment_mean:.2f}"")
print(f""Mean outcome after treatment: {post_treatment_mean:.2f}"")

# Calculate treatment effect
treatment_effect = post_treatment_mean - pre_treatment_mean
print(f""Treatment effect: {treatment_effect:.2f}"")

# Interpretation: Since the treatment effect is positive, we conclude that the treatment has a positive effect on patient outcomes.","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import pearsonr
from statsmodels.form import OLS

# Load data
data = pd.read_csv('patient_outcomes.csv')

# Calculate correlation between treatment and outcome
corr_coef, _ = pearsonr(data['treatment'], data['outcome'])
print(f""Correlation coefficient: {corr_coef:.3f}"")

# Fit a linear regression model to control for regression to the mean
X = data['treatment'].values.reshape(-1, 1)
y = data['outcome'].values.reshape(-1, 1)
model = OLS(y, X).fit()
treatment_effect = model.params[0]

print(f""Treatment effect (controlled for regression to the mean): {treatment_effect:.2f}"")

# Interpretation: The treatment effect is now estimated while accounting for regression to the mean. This provides a more accurate estimate of the treatment's actual effect on patient outcomes.",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:28:29.147200,llama-3-8b-v2,v2,False
regression_to_mean,warning,social_science,income inequality trends,python,intermediate,"import pandas as pd
import statsmodels.api as sm

# Load the data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Define the model
X = data['social_media_usage']
y = data['mental_health']

# Add a constant to the model
X = sm.add_constant(X)

# Fit the model
model = sm.OLS(y, X).fit()

# Print the coefficients
print(model.params)

# Interpret the results
print(f""The coefficient for social media usage is {model.params[1]} (p-value: {model.pvalues[1]})"")","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import statsmodels.api as sm
from scipy.stats import linregress

# Load the data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Define the model
X = data['social_media_usage']
y = data['mental_health']

# Calculate the mean of y
y_mean = y.mean()

# Calculate the regression to the mean effect
rtm_effect = y_mean - (y.mean() - X.mean() * linregress(X, y)[0])

# Print the regression to the mean effect
print(f""The regression to the mean effect is {rtm_effect:.2f}"")

# Fit the model (for demonstration purposes only)
X = sm.add_constant(X)
model = sm.OLS(y, X).fit()
print(model.params)",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:29:35.305790,llama-3-8b-v2,v2,False
regression_to_mean,warning,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import numpy as np

# Load data
data = pd.read_csv('graduation_rate.csv')

# Define treatment (demographic variable)
treatment = data['gender']

# Define outcome variable
outcome = data['graduation_rate']

# Calculate the mean of the outcome variable
pre_treatment_mean = np.mean(outcome)

# Calculate the mean of the outcome variable for each treatment group
treatment0_mean = np.mean(outcome[treatment == 0])
treatment1_mean = np.mean(outcome[treatment == 1])

# Calculate the mean of the outcome variable after the treatment
post_treatment_mean = np.mean(outcome[treatment == 1])

# Calculate the difference in means
treatment_effect = post_treatment_mean - pre_treatment_mean

print(""Treatment effect:"", treatment_effect)","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('graduation_rate.csv')

# Define treatment (demographic variable)
treatment = data['gender']

# Define outcome variable
outcome = data['graduation_rate']

# Calculate the mean of the outcome variable for each treatment group
treatment0_mean = np.mean(outcome[treatment == 0])
treatment1_mean = np.mean(outcome[treatment == 1])

# Calculate the treatment effect using a t-test
treatment_effect, p_value = ttest_ind(outcome[treatment == 0], outcome[treatment == 1])

print(""Treatment effect:"", treatment_effect)
print(""p-value:"", p_value)",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:30:34.477967,llama-3-8b-v2,v2,False
regression_to_mean,warning,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np

# Sample data
data = {
    'Region': ['North', 'North', 'North', 'South', 'South', 'South', 'East', 'East', 'East'],
    'Treatment': [0, 0, 1, 0, 0, 1, 0, 0, 1],
    'Sales': [10, 12, 15, 8, 10, 12, 11, 13, 14]
}
df = pd.DataFrame(data)

# Fit the model
X = pd.get_dummies(df, columns=['Region'])
y = df['Sales']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)

# Predict and calculate treatment effect
y_pred = model.predict(X_test)
treatment_effect = np.mean(y_test - y_pred)

print(""Treatment effect:"", treatment_effect)","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Sample data
data = {
    'Region': ['North', 'North', 'North', 'South', 'South', 'South', 'East', 'East', 'East'],
    'Treatment': [0, 0, 1, 0, 0, 1, 0, 0, 1],
    'Sales': [10, 12, 15, 8, 10, 12, 11, 13, 14]
}
df = pd.DataFrame(data)

# Fit the model
X = pd.get_dummies(df, columns=['Region'])
y = df['Sales']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = LinearRegression()
model.fit(X_train, y_train)

# Calculate treatment effect using the average outcome of the treated and control groups
treated_group = X_test[X_test['Treatment'] == 1]
control_group = X_test[X_test['Treatment'] == 0]
treatment_effect = np.mean(treated_group['Sales']) - np.mean(control_group['Sales'])

print(""Treatment effect:"", treatment_effect)",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:31:56.247200,llama-3-8b-v2,v2,False
regression_to_mean,warning,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import numpy as np

# Load the data
df = pd.read_csv('graduation_rate_data.csv')

# Define the demographic variables (X) and graduation rate (y)
X = df[['age', 'gender', 'race']]
y = df['graduation_rate']

# Fit a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)

# Calculate the predicted graduation rates for each demographic group
predictions = model.predict(X)

# Calculate the average graduation rate for each demographic group
group_means = y.groupby(X.columns).mean()

# Print the results
print(""Flawed interpretation:"")
print(""Demographic group\tMean Graduation Rate\tPredicted Graduation Rate"")
for i, row in group_means.iterrows():
    print(f""{i[0]} {i[1]}: {row['graduation_rate']:.2f} vs {predictions[i]:.2f}"")","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Load the data
df = pd.read_csv('graduation_rate_data.csv')

# Define the demographic variables (X) and graduation rate (y)
X = df[['age', 'gender', 'race']]
y = df['graduation_rate']

# Fit a linear regression model
model = LinearRegression()
model.fit(X, y)

# Calculate the predicted graduation rates for each demographic group
predictions = model.predict(X)

# Calculate the residuals (observed - predicted)
residuals = y - predictions

# Calculate the mean squared error (MSE) and R-squared
mse = np.mean(residuals**2)
r2 = model.score(X, y)

# Print the results
print(""Corrected interpretation:"")
print(""Demographic group\tMean Squared Error\tR-squared"")
print(f""Total\t{mse:.2f}\t{r2:.2f}"")",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:33:11.282333,llama-3-8b-v2,v2,False
regression_to_mean,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np
from statsmodels.linear_model import LinearRegression

# Load data
data = pd.read_csv('social_media_usage.csv')

# Create a linear regression model
model = LinearRegression()
X = data['social_media_usage']
y = data['mental_health']
model.fit(X, y)

# Print the regression coefficient
print(""Regression coefficient:"", model.coef_)

# Interpret the result: ""For every hour spent on social media, mental health decreases by X units.""","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from statsmodels.linear_model import LinearRegression
from statsmodels.stats.anova import anova_lm

# Load data
data = pd.read_csv('social_media_usage.csv')

# Create a linear regression model
X = data[['social_media_usage']]  # Include a constant term
y = data['mental_health']
model = LinearRegression()
model.fit(X, y)

# Print the regression coefficient
print(""Regression coefficient:"", model.coef_)

# Perform ANOVA to test for significance
anova_table = anova_lm(model, model.fit_intercept=False)
print(""ANOVA table:"", anova_table)

# Interpret the result: ""The regression coefficient is statistically significant, indicating a significant relationship between social media usage and mental health.""",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:34:07.162866,llama-3-8b-v2,v2,False
regression_to_mean,warning,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Load data
df = pd.read_csv('social_media_usage.csv')

# Calculate correlation between social media usage and mental health
correlation, p_value = pearsonr(df['social_media_usage'], df['mental_health'])

print(""Correlation:"", correlation)
print(""p-value:"", p_value)

# Interpretation: Social media usage has a significant positive effect on mental health","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import pearsonr

# Load data
df = pd.read_csv('social_media_usage.csv')

# Calculate correlation between social media usage and mental health
correlation, p_value = pearsonr(df['social_media_usage'], df['mental_health'])

print(""Correlation:"", correlation)
print(""p-value:"", p_value)

# Control for time and other confounding variables
df = df.groupby('time').mean().reset_index()
correlation, p_value = pearsonr(df['social_media_usage'], df['mental_health'])

print(""Adjusted Correlation:"", correlation)
print(""Adjusted p-value:"", p_value)

# Interpretation: The observed correlation is likely due to regression to the mean, and cannot be interpreted as a treatment effect",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:35:03.149221,llama-3-8b-v2,v2,False
regression_to_mean,warning,education,student performance prediction,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import linregress

# Load data
df = pd.read_csv('data.csv')

# Define treatment and outcome variables
treatment = df['treatment']
outcome = df['outcome']

# Calculate regression line
slope, intercept, _, _, _ = linregress(treatment, outcome)
regression_line = slope * treatment + intercept

# Calculate mean outcome for each treatment group
treatment1_mean = np.mean(outcome[treatment == 1])
treatment2_mean = np.mean(outcome[treatment == 0])

# Calculate difference in means
difference_in_means = treatment1_mean - treatment2_mean

print(""Regression to the mean effect:"", difference_in_means)","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('data.csv')

# Define treatment and outcome variables
treatment = df['treatment']
outcome = df['outcome']

# Calculate treatment effect using independent samples t-test
t_stat, p_val = ttest_ind(outcome[treatment == 1], outcome[treatment == 0])
treatment_effect = np.mean(outcome[treatment == 1]) - np.mean(outcome[treatment == 0])

print(""Treatment effect:"", treatment_effect)",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:36:02.022888,llama-3-8b-v2,v2,False
regression_to_mean,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('data.csv')

# Convert categorical variable to numerical
df['treatment'] = np.where(df['treatment'] == 'control', 0, 1)

# Calculate mean outcome for treated and control groups
treated_mean = df.loc[df['treatment'] == 1, 'outcome'].mean()
control_mean = df.loc[df['treatment'] == 0, 'outcome'].mean()

# Calculate the difference between treated and control means
treatment_effect = treated_mean - control_mean

print(f""Treatment effect: {treatment_effect:.2f}"")","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import linregress

# Load data
df = pd.read_csv('data.csv')

# Convert categorical variable to numerical
df['treatment'] = np.where(df['treatment'] == 'control', 0, 1)

# Fit a linear regression model to the data
X = df['treatment']
y = df['outcome']
slope, intercept, r_value, p_value, std_err = linregress(X, y)

# Calculate the treatment effect as the slope of the regression line
treatment_effect = slope

print(f""Treatment effect: {treatment_effect:.2f}"")",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:37:01.324893,llama-3-8b-v2,v2,False
regression_to_mean,warning,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import numpy as np

# Load data
data = pd.read_csv('marketing_campaign_data.csv')

# Define the treatment (new marketing strategy) and outcome (sales)
treatment = data['new_strategy']
outcome = data['sales']

# Calculate the mean of the outcome before and after the treatment
pre_treatment_mean = np.mean(outcome[:len(treatment) // 2])
post_treatment_mean = np.mean(outcome[len(treatment) // 2:])

# Calculate the difference between the two means
treatment_effect = post_treatment_mean - pre_treatment_mean

print(""Treatment effect:"", treatment_effect)","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('marketing_campaign_data.csv')

# Define the treatment (new marketing strategy) and outcome (sales)
treatment = data['new_strategy']
outcome = data['sales']

# Split the data into pre-treatment and post-treatment groups
pre_treatment_outcome = outcome[:len(treatment) // 2]
post_treatment_outcome = outcome[len(treatment) // 2:]

# Calculate the means of the pre-treatment and post-treatment groups
pre_treatment_mean = np.mean(pre_treatment_outcome)
post_treatment_mean = np.mean(post_treatment_outcome)

# Perform a two-sample t-test to compare the means
t_stat, p_value = ttest_ind(pre_treatment_outcome, post_treatment_outcome)

print(""T-statistic:"", t_stat)
print(""P-value:"", p_value)

# Interpret the results: if the p-value is below a certain significance level (e.g. 0.05), we can reject the null hypothesis that the treatment had no effect and conclude that the treatment had a statistically significant impact on the outcome.",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:38:12.368679,llama-3-8b-v2,v2,False
regression_to_mean,warning,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('data.csv')

# Define treatment and outcome variables
X = df['treatment']  # 0 = control, 1 = treatment
y = df['outcome']

# Fit a linear regression model
model = LinearRegression()
model.fit(X.values.reshape(-1, 1), y)

# Calculate the mean of the outcome variable
control_mean = y[X == 0].mean()
treatment_mean = y[X == 1].mean()

# Calculate the mean difference
mean_diff = treatment_mean - control_mean

print(f""The treatment effect is {mean_diff:.2f}."")

# Calculate the R-squared value
r_squared = model.score(X.values.reshape(-1, 1), y)
print(f""The R-squared value is {r_squared:.2f}."")","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('data.csv')

# Define treatment and outcome variables
X = df['treatment']  # 0 = control, 1 = treatment
y = df['outcome']

# Calculate the mean of the outcome variable
control_mean = y[X == 0].mean()
treatment_mean = y[X == 1].mean()

# Calculate the mean difference
mean_diff = treatment_mean - control_mean

print(f""The mean difference is {mean_diff:.2f}."")

# Perform a t-test to test the significance of the treatment effect
t_stat, p_val = ttest_ind(y[X == 0], y[X == 1])

print(f""The p-value is {p_val:.4f}. If the p-value is below a certain significance level (e.g., 0.05), we reject the null hypothesis that the treatment has no effect."")

# Calculate the R-squared value (not relevant to the treatment effect)
r_squared = model.score(X.values.reshape(-1, 1), y)
print(f""The R-squared value is {r_squared:.2f}."")",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:39:25.828570,llama-3-8b-v2,v2,False
regression_to_mean,warning,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import numpy as np

# Load the data
df = pd.read_csv('hospital_data.csv')

# Calculate the mean of treatment group's outcome before treatment
pre_treatment_mean = df[df['treatment'] == 1]['outcome'].mean()

# Calculate the mean of control group's outcome before treatment
pre_control_mean = df[df['treatment'] == 0]['outcome'].mean()

# Calculate the mean of treatment group's outcome after treatment
post_treatment_mean = df[df['treatment'] == 1]['outcome'].loc[df['time'] == 'after'].mean()

# Calculate the mean of control group's outcome after treatment
post_control_mean = df[df['treatment'] == 0]['outcome'].loc[df['time'] == 'after'].mean()

# Calculate the treatment effect as the difference between post-treatment means
treatment_effect = post_treatment_mean - post_control_mean

print(""Treatment effect:"", treatment_effect)","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('hospital_data.csv')

# Calculate the mean of treatment group's outcome before treatment
pre_treatment_mean = df[df['treatment'] == 1]['outcome'].mean()

# Calculate the mean of control group's outcome before treatment
pre_control_mean = df[df['treatment'] == 0]['outcome'].mean()

# Calculate the mean of treatment group's outcome after treatment
post_treatment_mean = df[df['treatment'] == 1]['outcome'].loc[df['time'] == 'after'].mean()

# Calculate the mean of control group's outcome after treatment
post_control_mean = df[df['treatment'] == 0]['outcome'].loc[df['time'] == 'after'].mean()

# Perform a two-sample t-test to determine if the treatment effect is statistically significant
t_stat, p_value = ttest_ind(df[df['treatment'] == 1]['outcome'].loc[df['time'] == 'after'], 
                            df[df['treatment'] == 0]['outcome'].loc[df['time'] == 'after'], equal_var=False)

if p_value < 0.05:
    print(""Treatment effect is statistically significant"")
else:
    print(""Treatment effect is not statistically significant"")

# Calculate the treatment effect as the mean difference between the two groups
treatment_effect = np.mean(df[df['treatment'] == 1]['outcome'].loc[df['time'] == 'after']) - np.mean(df[df['treatment'] == 0]['outcome'].loc[df['time'] == 'after'])
print(""Treatment effect:"", treatment_effect)",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:41:06.346645,llama-3-8b-v2,v2,False
regression_to_mean,warning,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
from sklearn.linear_model import LinearRegression

# Load dataset
df = pd.read_csv('student_performance.csv')

# Create a linear regression model
model = LinearRegression()

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('performance', axis=1), df['performance'], test_size=0.2, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate the mean of the predicted values
mean_pred = np.mean(y_pred)

# Calculate the mean of the actual values
mean_actual = np.mean(y_test)

# Calculate the mean squared error (MSE)
mse = np.mean((y_pred - y_test) ** 2)

print(f""Mean predicted: {mean_pred:.2f}, Mean actual: {mean_actual:.2f}, MSE: {mse:.2f}"")

# Interpret the results: ""Ah, look! The predicted mean is higher than the actual mean, so our treatment (the model) must be having a positive effect!""","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Load dataset
df = pd.read_csv('student_performance.csv')

# Create a linear regression model
model = LinearRegression()

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('performance', axis=1), df['performance'], test_size=0.2, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate the mean squared error (MSE)
mse = mean_squared_error(y_test, y_pred)

print(f""MSE: {mse:.2f}"")

# Interpret the results: The MSE is a measure of the model's predictive accuracy, but it does not imply a treatment effect. To assess the treatment effect, we would need to compare the model's predictions to a control group or a baseline.",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:42:22.505023,llama-3-8b-v2,v2,False
regression_to_mean,warning,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('disease_data.csv')

# Split data into treatment and control groups
treatment_group = df[df['treatment'] == 'Yes']
control_group = df[df['treatment'] == 'No']

# Calculate means of disease prevalence in each group
treatment_mean = np.mean(treatment_group['prevalence'])
control_mean = np.mean(control_group['prevalence'])

# Perform t-test to compare means
t_stat, p_val = ttest_ind(treatment_group['prevalence'], control_group['prevalence'])

print(""T-statistic:"", t_stat)
print(""p-value:"", p_val)

# Interpret results
if p_val < 0.05:
    print(""There is a statistically significant difference in disease prevalence between treatment and control groups."")
else:
    print(""There is no statistically significant difference in disease prevalence between treatment and control groups."")","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('disease_data.csv')

# Split data into treatment and control groups
treatment_group = df[df['treatment'] == 'Yes']
control_group = df[df['treatment'] == 'No']

# Calculate means of disease prevalence in each group
treatment_mean = np.mean(treatment_group['prevalence'])
control_mean = np.mean(control_group['prevalence'])

# Calculate the standard deviation of disease prevalence in the original data
std_dev = np.std(df['prevalence'])

# Calculate the expected regression to the mean in the treatment group
expected_regression = treatment_mean - (treatment_mean - control_mean)

print(""Expected regression to the mean:"", expected_regression)

# Perform t-test to compare means, accounting for regression to the mean
t_stat, p_val = ttest_ind(treatment_group['prevalence'], control_group['prevalence'], equal_var=False)

print(""T-statistic:"", t_stat)
print(""p-value:"", p_val)

# Interpret results
if p_val < 0.05:
    print(""There is a statistically significant difference in disease prevalence between treatment and control groups, after accounting for regression to the mean."")
else:
    print(""There is no statistically significant difference in disease prevalence between treatment and control groups, after accounting for regression to the mean."")",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:43:53.430685,llama-3-8b-v2,v2,False
regression_to_mean,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Load the data
df = pd.read_csv('disease_data.csv')

# Define the features and target variable
X = df[['age','sex', 'income']]
y = df['disease_prevalence']

# Create a linear regression model
model = LinearRegression()
model.fit(X, y)

# Predict the disease prevalence for a new patient
new_patient = pd.DataFrame({'age': [30],'sex': [1], 'income': [50000]})
new_patient_pred = model.predict(new_patient)

print(""The predicted disease prevalence for the new patient is:"", new_patient_pred[0])

# Interpret the result as a treatment effect
print(""The treatment (e.g., a new medicine) is expected to reduce the disease prevalence by"", new_patient_pred[0] - y.mean(), ""percentage points."")","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Load the data
df = pd.read_csv('disease_data.csv')

# Define the features and target variable
X = df[['age','sex', 'income']]
y = df['disease_prevalence']

# Create a linear regression model
model = LinearRegression()
model.fit(X, y)

# Predict the disease prevalence for a new patient
new_patient = pd.DataFrame({'age': [30],'sex': [1], 'income': [50000]})
new_patient_pred = model.predict(new_patient)

print(""The predicted disease prevalence for the new patient is:"", new_patient_pred[0])

# Correctly interpret the result as a prediction, not a treatment effect
print(""The predicted disease prevalence for the new patient is"", new_patient_pred[0], ""and is expected to converge to the population mean of"", y.mean(), ""over time."")",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:45:00.008341,llama-3-8b-v2,v2,False
regression_to_mean,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('income_inequality_data.csv')

# Define treatment and outcome variables
treatment = 'policy_a'
outcome = 'income'

# Split data into treatment and control groups
treatment_group = df[df[treatment] == 1]
control_group = df[df[treatment] == 0]

# Calculate mean income for treatment and control groups
treatment_mean = np.mean(treatment_group[outcome])
control_mean = np.mean(control_group[outcome])

# Calculate the difference between treatment and control means
treatment_effect = treatment_mean - control_mean

print(f""Treatment effect: {treatment_effect:.2f}"")","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load dataset
df = pd.read_csv('income_inequality_data.csv')

# Define treatment and outcome variables
treatment = 'policy_a'
outcome = 'income'

# Split data into treatment and control groups
treatment_group = df[df[treatment] == 1]
control_group = df[df[treatment] == 0]

# Calculate mean income for treatment and control groups
treatment_mean = np.mean(treatment_group[outcome])
control_mean = np.mean(control_group[outcome])

# Perform a t-test to test the difference between treatment and control means
t_stat, p_val = ttest_ind(treatment_group[outcome], control_group[outcome])

print(f""T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")

# Interpret the results: if the p-value is below a certain significance level (e.g. 0.05), we reject the null hypothesis that the treatment has no effect.
if p_val < 0.05:
    print(""Reject the null hypothesis: the treatment has a statistically significant effect on income."")
else:
    print(""Fail to reject the null hypothesis: there is no statistically significant evidence of a treatment effect."")",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:46:17.787239,llama-3-8b-v2,v2,False
regression_to_mean,warning,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import linregress

# Load the data
df = pd.read_csv('data.csv')

# Calculate the mean of the outcome variable before treatment
pre_treatment_mean = df['outcome'].mean()

# Calculate the mean of the outcome variable after treatment
post_treatment_mean = df.loc[df['treatment'] == 1, 'outcome'].mean()

# Calculate the mean difference
mean_diff = post_treatment_mean - pre_treatment_mean

# Calculate the regression slope
slope, _, _, _, _ = linregress(df['treatment'], df['outcome'])
print(f""The regression slope is {slope:.2f}"")
print(f""The mean difference is {mean_diff:.2f}"")

# Interpret the results
print(""The treatment has a significant effect on the outcome variable because the mean difference is larger than the regression slope."")","{'summary': 'Regression to the Mean Misinterpretation', 'detailed_explanation': 'Regression to the Mean Misinterpretation: Extreme values naturally regress toward average on retest', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('data.csv')

# Calculate the mean of the outcome variable before treatment
pre_treatment_mean = df['outcome'].mean()

# Calculate the mean of the outcome variable after treatment
post_treatment_mean = df.loc[df['treatment'] == 1, 'outcome'].mean()

# Calculate the mean difference
mean_diff = post_treatment_mean - pre_treatment_mean

# Calculate the standard error of the mean difference
se = np.sqrt((pre_treatment_mean * (1 - pre_treatment_mean) + post_treatment_mean * (1 - post_treatment_mean)) / (df.shape[0] - 2))

# Calculate the t-statistic
t_stat = mean_diff / se

# Calculate the p-value
p_val = ttest_ind(df.loc[df['treatment'] == 0, 'outcome'], df.loc[df['treatment'] == 1, 'outcome']).pvalue

# Interpret the results
print(f""The treatment effect is {mean_diff:.2f} with a standard error of {se:.2f}."")
print(f""The p-value is {p_val:.4f}."")
print(""The treatment has a significant effect on the outcome variable if the p-value is less than 0.05."")",['Extreme values naturally regress toward average on retest'],Extreme values naturally regress toward average on retest,2025-12-16T09:47:42.440084,llama-3-8b-v2,v2,False
base_rate_neglect,warning,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd

# Load dataset
df = pd.read_csv('education_data.csv')

# Calculate the effectiveness of two teaching methods
method1_effectiveness = (df[df['method'] =='method1']['student_outcome'] == 'pass').mean()
method2_effectiveness = (df[df['method'] =='method2']['student_outcome'] == 'pass').mean()

print(""Method 1 effectiveness:"", method1_effectiveness)
print(""Method 2 effectiveness:"", method2_effectiveness)","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import scipy.stats as stats

# Load dataset
df = pd.read_csv('education_data.csv')

# Calculate the prior probability of students passing
prior_pass_rate = df['student_outcome'].mean()

# Calculate the effectiveness of two teaching methods
method1_effectiveness = (df[df['method'] =='method1']['student_outcome'] == 'pass').mean()
method2_effectiveness = (df[df['method'] =='method2']['student_outcome'] == 'pass').mean()

# Calculate the base rate adjusted effectiveness
method1_effectiveness_adj = method1_effectiveness - prior_pass_rate
method2_effectiveness_adj = method2_effectiveness - prior_pass_rate

print(""Method 1 effectiveness (adjusted for base rate):"", method1_effectiveness_adj)
print(""Method 2 effectiveness (adjusted for base rate):"", method2_effectiveness_adj)",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T09:48:41.766155,llama-3-8b-v2,v2,False
base_rate_neglect,warning,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('graduation_rates.csv')

# Define the demographic variables
demographics = ['male', 'female', 'white', 'black', 'hispanic']

# Calculate the base rates of graduation for each demographic
base_rates = df.groupby(demographics)['graduated'].mean()

# Define the conditional probabilities of graduation for each demographic
conditional_probs = df.groupby(demographics)['graduated'].mean()

# Calculate the probability of graduation for a randomly selected student
# without considering the base rates
prob_grad = conditional_probs / sum(conditional_probs)

print(prob_grad)","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('graduation_rates.csv')

# Define the demographic variables
demographics = ['male', 'female', 'white', 'black', 'hispanic']

# Calculate the base rates of graduation for each demographic
base_rates = df.groupby(demographics)['graduated'].mean()

# Calculate the prior probabilities of the demographics in the population
prior_probs = df[demographics].mean()

# Calculate the conditional probabilities of graduation for each demographic
conditional_probs = df.groupby(demographics)['graduated'].mean()

# Calculate the probability of graduation for a randomly selected student
# considering the base rates and prior probabilities
prob_grad = (conditional_probs / base_rates) * prior_probs

print(prob_grad)",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T09:49:36.166557,llama-3-8b-v2,v2,False
base_rate_neglect,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import numpy as np
from scipy.stats import binom

# Simulate data
n_trials = 1000
n_successes = 800

# Simulate true probability of success
p_true = 0.7

# Simulate trial results
successes = np.random.binomial(n_trials, p_true, size=None)

# Calculate observed probability of success
p_observed = successes / n_trials

# Perform hypothesis test
z_score = (p_observed - 0.5) / np.sqrt(p_observed * (1 - p_observed) / n_trials)
print(f""Z-score: {z_score}"")","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
from scipy.stats import binom

# Simulate data
n_trials = 1000
n_successes = 800

# Simulate true probability of success
p_true = 0.7

# Simulate trial results
successes = np.random.binomial(n_trials, p_true, size=None)

# Calculate observed probability of success
p_observed = successes / n_trials

# Calculate the prior probability of success
prior_prob = p_true

# Calculate the posterior probability of success
posterior_prob = (prior_prob * binom.pmf(n_successes, n_trials, p_true) +
                  (1 - prior_prob) * binom.pmf(n_successes, n_trials, 1 - p_true))

print(f""Posterior probability of success: {posterior_prob}"")",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T09:50:35.644728,llama-3-8b-v2,v2,False
base_rate_neglect,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
from scipy.stats import binom

# Load the data
data = pd.read_csv('social_media_data.csv')

# Define the prior probabilities
prior_positive = 0.2  # 20% of the population has mental health issues
prior_negative = 0.8  # 80% of the population does not have mental health issues

# Define the likelihood ratios
lr_positive = 0.9  # 90% of social media users with mental health issues post about it
lr_negative = 0.05  # 5% of non-social media users with mental health issues post about it

# Calculate the likelihoods
likelihood_positive = lr_positive * prior_positive
likelihood_negative = lr_negative * prior_negative

# Calculate the posterior probabilities
posterior_positive = likelihood_positive / (likelihood_positive + likelihood_negative)
posterior_negative = likelihood_negative / (likelihood_positive + likelihood_negative)

print(""Posterior probability of mental health issues given social media usage:"", posterior_positive)
print(""Posterior probability of no mental health issues given social media usage:"", posterior_negative)","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import binom

# Load the data
data = pd.read_csv('social_media_data.csv')

# Define the prior probabilities
prior_positive = 0.2  # 20% of the population has mental health issues
prior_negative = 0.8  # 80% of the population does not have mental health issues

# Define the likelihood ratios
lr_positive = 0.9  # 90% of social media users with mental health issues post about it
lr_negative = 0.05  # 5% of non-social media users with mental health issues post about it

# Calculate the likelihoods
likelihood_positive = lr_positive * prior_positive
likelihood_negative = lr_negative * prior_negative

# Calculate the posterior probabilities
posterior_positive = likelihood_positive / (likelihood_positive + likelihood_negative)
posterior_negative = likelihood_negative / (likelihood_positive + likelihood_negative)

# Calculate the prior-weighted posterior probabilities
prior_weighted_posterior_positive = posterior_positive * prior_positive
prior_weighted_posterior_negative = posterior_negative * prior_negative

print(""Prior-weighted posterior probability of mental health issues given social media usage:"", prior_weighted_posterior_positive)
print(""Prior-weighted posterior probability of no mental health issues given social media usage:"", prior_weighted_posterior_negative)",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T09:51:48.170482,llama-3-8b-v2,v2,False
base_rate_neglect,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np

# Load data
data = pd.read_csv('crime_data.csv')

# Define the response variable (crime rate)
y = data['crime_rate']

# Define the predictor variables (weather, poverty rate, etc.)
X = data[['weather', 'poverty_rate', 'education_level']]

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a logistic regression model
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

# Make predictions on the test set
y_pred = logreg.predict(X_test)

# Calculate the accuracy
accuracy = np.mean(y_pred == y_test)
print(f'Accuracy: {accuracy:.3f}')","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix

# Load data
data = pd.read_csv('crime_data.csv')

# Define the response variable (crime rate)
y = data['crime_rate']

# Define the predictor variables (weather, poverty rate, etc.)
X = data[['weather', 'poverty_rate', 'education_level']]

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a logistic regression model
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

# Make predictions on the test set
y_pred = logreg.predict(X_test)

# Calculate the accuracy
accuracy = np.mean(y_pred == y_test)
print(f'Accuracy: {accuracy:.3f}')

# Calculate the prior probability of the crime rate (base rate)
prior_prob = np.mean(y)

# Calculate the true positive rate (TPR) and false positive rate (FPR) using the confusion matrix
conf_mat = confusion_matrix(y_test, y_pred)
TP = conf_mat[1, 1]
FP = conf_mat[0, 1]
TPR = TP / (TP + FP)
FPR = FP / (FP + conf_mat[0, 0])

# Calculate the accuracy taking into account the prior probability
adjusted_accuracy = (TPR * prior_prob) + ((1 - TPR) * (1 - prior_prob))
print(f'Adjusted Accuracy: {adjusted_accuracy:.3f}')",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T09:53:06.152620,llama-3-8b-v2,v2,False
base_rate_neglect,warning,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
from scipy.stats import binom

# Data
data = pd.DataFrame({'Response': [1, 1, 1, 0, 0, 0, 1, 1, 0, 1],
                    'Treatment': ['Treated', 'Treated', 'Treated', 'Control', 'Control', 'Control', 'Treated', 'Treated', 'Control', 'Treated'],
                    'Outcome': [1, 1, 0, 0, 1, 1, 1, 0, 0, 1]})

# Calculate treatment effect
treatment_effect = binom.test(data[data['Treatment'] == 'Treated']['Outcome'].sum(), data[data['Treatment'] == 'Treated'].shape[0])

print(f""Treatment effect: {treatment_effect}"")","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import binom
from scipy.stats import norm

# Data
data = pd.DataFrame({'Response': [1, 1, 1, 0, 0, 0, 1, 1, 0, 1],
                    'Treatment': ['Treated', 'Treated', 'Treated', 'Control', 'Control', 'Control', 'Treated', 'Treated', 'Control', 'Treated'],
                    'Outcome': [1, 1, 0, 0, 1, 1, 1, 0, 0, 1]})

# Calculate prior probability of outcome
prior_prob = data['Outcome'].mean()

# Calculate treatment effect
treatment_effect = binom.test(data[data['Treatment'] == 'Treated']['Outcome'].sum(), data[data['Treatment'] == 'Treated'].shape[0], prior_prob)

print(f""Treatment effect: {treatment_effect}"")

# Calculate the posterior probability of the outcome in the treated group
posterior_prob = binom.pmf(data[data['Treatment'] == 'Treated']['Outcome'].sum(), data[data['Treatment'] == 'Treated'].shape[0], prior_prob)

print(f""Posterior probability of outcome in treated group: {posterior_prob}"")",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T09:54:22.274327,llama-3-8b-v2,v2,False
base_rate_neglect,warning,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
from sklearn.metrics import accuracy_score

# Load data
df = pd.read_csv('online_learning_data.csv')

# Define features and target variable
X = df.drop(['outcome'], axis=1)
y = df['outcome']

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a logistic regression model
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.3f}')","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.metrics import accuracy_score
from scipy.stats import binom

# Load data
df = pd.read_csv('online_learning_data.csv')

# Define features and target variable
X = df.drop(['outcome'], axis=1)
y = df['outcome']

# Calculate prior probability of online learning
prior_online = df['outcome'].mean()

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a logistic regression model
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate accuracy, taking into account the prior probability
accuracy = accuracy_score(y_test, y_pred)
prior_corrected_accuracy = accuracy * prior_online + (1 - accuracy) * (1 - prior_online)
print(f'Prior-corrected accuracy: {prior_corrected_accuracy:.3f}')",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T09:55:38.602586,llama-3-8b-v2,v2,False
base_rate_neglect,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
from sklearn.naive_bayes import MultinomialNB

# Load voting behavior dataset
df = pd.read_csv('voting_behavior_data.csv')

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop('voted', axis=1), df['voted'], test_size=0.2, random_state=42)

# Train a Naive Bayes classifier
clf = MultinomialNB()
clf.fit(X_train, y_train)

# Predict probabilities of voting based on demographic features
y_pred_prob = clf.predict_proba(X_test)

# Select the class with the highest probability as the predicted outcome
y_pred = y_pred_prob.argmax(axis=1)

# Calculate the accuracy of the model
accuracy = (y_pred == y_test).mean()
print(f""Accuracy: {accuracy:.3f}"")","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.naive_bayes import MultinomialNB

# Load voting behavior dataset
df = pd.read_csv('voting_behavior_data.csv')

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.drop('voted', axis=1), df['voted'], test_size=0.2, random_state=42)

# Calculate the prior probability of voting (base rate)
prior_voting = y_train.mean()

# Train a Naive Bayes classifier with the prior probability
clf = MultinomialNB(priors=[prior_voting, 1 - prior_voting])  # Set prior probabilities for voting and not voting
clf.fit(X_train, y_train)

# Predict probabilities of voting based on demographic features
y_pred_prob = clf.predict_proba(X_test)

# Calculate the posterior probabilities by multiplying the prior probability with the likelihood
posterior_prob_voting = y_pred_prob[:, 1] * prior_voting
posterior_prob_not_voting = y_pred_prob[:, 0] * (1 - prior_voting)

# Select the class with the highest posterior probability as the predicted outcome
y_pred = posterior_prob_voting > posterior_prob_not_voting

# Calculate the accuracy of the model
accuracy = (y_pred == y_test).mean()
print(f""Accuracy: {accuracy:.3f}"")",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T09:57:01.411193,llama-3-8b-v2,v2,False
base_rate_neglect,warning,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import numpy as np

# Sample data
data = {
    'Region': ['North', 'North', 'North', 'South', 'South', 'South', 'East', 'East', 'East'],
    'Sales': [100, 120, 110, 90, 80, 100, 130, 140, 120],
    'Target': [1, 1, 0, 1, 0, 1, 0, 1, 0]
}

df = pd.DataFrame(data)

# Calculate the base rate of success in the entire population
base_rate = df['Target'].mean()

# Calculate the likelihood ratio
likelihood_ratio = (df.loc[df['Target'] == 1, 'Sales'].mean() - df.loc[df['Target'] == 0, 'Sales'].mean()) / df.loc[df['Target'] == 0, 'Sales'].std()

# Make predictions using the flawed method
flawed_predictions = np.where(df['Region'] == 'North', 1, 0)

# Calculate the accuracy of the flawed predictions
flawed_accuracy = (flawed_predictions == df['Target']).mean()

print(f""Flawed accuracy: {flawed_accuracy:.2f}"")","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np

# Sample data
data = {
    'Region': ['North', 'North', 'North', 'South', 'South', 'South', 'East', 'East', 'East'],
    'Sales': [100, 120, 110, 90, 80, 100, 130, 140, 120],
    'Target': [1, 1, 0, 1, 0, 1, 0, 1, 0]
}

df = pd.DataFrame(data)

# Calculate the base rate of success in the entire population
base_rate = df['Target'].mean()

# Calculate the likelihood ratio
likelihood_ratio = (df.loc[df['Target'] == 1, 'Sales'].mean() - df.loc[df['Target'] == 0, 'Sales'].mean()) / df.loc[df['Target'] == 0, 'Sales'].std()

# Calculate the predicted probabilities using Bayes' theorem
predicted_probabilities = (base_rate * likelihood_ratio) / (1 + likelihood_ratio)

# Make predictions using the corrected method
corrected_predictions = np.where(predicted_probabilities > 0.5, 1, 0)

# Calculate the accuracy of the corrected predictions
corrected_accuracy = (corrected_predictions == df['Target']).mean()

print(f""Corrected accuracy: {corrected_accuracy:.2f}"")",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T09:58:34.712621,llama-3-8b-v2,v2,False
base_rate_neglect,warning,social_science,income inequality trends,python,intermediate,"import pandas as pd
from scipy.stats import chi2_contingency

# Load the data
data = pd.read_csv('crime_data.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data.drop('crime_rate', axis=1), data['crime_rate'], test_size=0.2, random_state=42)

# Fit a logistic regression model
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

# Predict probabilities of crime rate for the test set
y_pred_proba = logreg.predict_proba(X_test)[:, 1]

# Calculate the contingency table
contingency_table = pd.crosstab(y_test, y_pred_proba > 0.5)

# Perform chi-squared test
chi2, p_val, dof, expected = chi2_contingency(contingency_table)

print(f'Chi-squared statistic: {chi2:.2f}')
print(f'p-value: {p_val:.4f}')","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import chi2_contingency

# Load the data
data = pd.read_csv('crime_data.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data.drop('crime_rate', axis=1), data['crime_rate'], test_size=0.2, random_state=42)

# Fit a logistic regression model
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

# Predict probabilities of crime rate for the test set
y_pred_proba = logreg.predict_proba(X_test)[:, 1]

# Calculate the contingency table with the prior probability
prior_crime_rate = 0.05  # base rate of crime in the population
contingency_table = pd.crosstab(y_test, [y_test, y_pred_proba > prior_crime_rate])

# Perform chi-squared test
chi2, p_val, dof, expected = chi2_contingency(contingency_table)

print(f'Chi-squared statistic: {chi2:.2f}')
print(f'p-value: {p_val:.4f}')",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T09:59:56.383857,llama-3-8b-v2,v2,False
base_rate_neglect,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
from scipy.stats import binom

# Load data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Define the proportion of people with mental health issues
prior_mental_health_issues = 0.15

# Define the proportion of people who use social media excessively
prior_social_media_usage = 0.30

# Define the proportion of people with mental health issues who use social media excessively
conditional_probability_mental_health_issues_given_social_media_usage = 0.60

# Calculate the probability of mental health issues given social media usage
posterior_probability_mental_health_issues_given_social_media_usage = conditional_probability_mental_health_issues_given_social_media_usage

print(""Probability of mental health issues given social media usage:"", posterior_probability_mental_health_issues_given_social_media_usage)","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import binom

# Load data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Define the proportion of people with mental health issues
prior_mental_health_issues = 0.15

# Define the proportion of people who use social media excessively
prior_social_media_usage = 0.30

# Define the proportion of people with mental health issues who use social media excessively
conditional_probability_mental_health_issues_given_social_media_usage = 0.60

# Calculate the prior probability of mental health issues given social media usage
prior_probability_mental_health_issues_given_social_media_usage = prior_mental_health_issues * prior_social_media_usage / (prior_mental_health_issues * prior_social_media_usage + (1 - prior_mental_health_issues) * (1 - prior_social_media_usage))

print(""Prior probability of mental health issues given social media usage:"", prior_probability_mental_health_issues_given_social_media_usage)",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:01:00.883015,llama-3-8b-v2,v2,False
base_rate_neglect,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd

# Load the data
df = pd.read_csv('income_data.csv')

# Calculate the proportion of low-income individuals who engage in a certain behavior (e.g., voting)
low_income_behavior = df[df['income'] < 30000]['behavior'].mean()

# Calculate the proportion of high-income individuals who engage in the same behavior
high_income_behavior = df[df['income'] >= 30000]['behavior'].mean()

# Calculate the base rate of the behavior in the population
base_rate = df['behavior'].mean()

# Interpret the results
print(f""The proportion of low-income individuals who engage in the behavior is {low_income_behavior:.2f}."")
print(f""The proportion of high-income individuals who engage in the behavior is {high_income_behavior:.2f}."")
print(f""The base rate of the behavior in the population is {base_rate:.2f}."")","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np

# Load the data
df = pd.read_csv('income_data.csv')

# Calculate the proportion of low-income individuals who engage in the behavior
low_income_behavior = df[df['income'] < 30000]['behavior'].mean()

# Calculate the proportion of high-income individuals who engage in the behavior
high_income_behavior = df[df['income'] >= 30000]['behavior'].mean()

# Calculate the base rate of the behavior in the population
base_rate = df['behavior'].mean()

# Calculate the lift of the behavior for low-income individuals relative to the base rate
lift_low_income = (low_income_behavior - base_rate) / (1 - base_rate)

# Calculate the lift of the behavior for high-income individuals relative to the base rate
lift_high_income = (high_income_behavior - base_rate) / (1 - base_rate)

# Interpret the results
print(f""The lift of the behavior for low-income individuals is {lift_low_income:.2f}."")
print(f""The lift of the behavior for high-income individuals is {lift_high_income:.2f}."")",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:02:20.115235,llama-3-8b-v2,v2,False
base_rate_neglect,warning,social_science,income inequality trends,python,intermediate,"import pandas as pd
from scipy.stats import binom

# Load voting data
df = pd.read_csv('voting_data.csv')

# Calculate the probability of voting yes given a republican
p_yes_republican = sum(df['voted_yes'] & df['party_republican']) / sum(df['party_republican'])

# Calculate the probability of voting yes given a democrat
p_yes_democrat = sum(df['voted_yes'] & df['party_democrat']) / sum(df['party_democrat'])

# Calculate the probability of voting yes overall
p_yes_overall = sum(df['voted_yes']) / len(df)

# Print the results
print(f""Probability of voting yes given a republican: {p_yes_republican:.3f}"")
print(f""Probability of voting yes given a democrat: {p_yes_democrat:.3f}"")
print(f""Probability of voting yes overall: {p_yes_overall:.3f}"")","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import binom

# Load voting data
df = pd.read_csv('voting_data.csv')

# Calculate the prior probability of being a republican
p_republican = sum(df['party_republican']) / len(df)

# Calculate the prior probability of being a democrat
p_democrat = 1 - p_republican

# Calculate the probability of voting yes given a republican
p_yes_republican = (sum(df['voted_yes'] & df['party_republican']) / sum(df['party_republican'])) / p_republican

# Calculate the probability of voting yes given a democrat
p_yes_democrat = (sum(df['voted_yes'] & df['party_democrat']) / sum(df['party_democrat'])) / p_democrat

# Calculate the probability of voting yes overall
p_yes_overall = sum(df['voted_yes']) / len(df)

# Print the results
print(f""Probability of voting yes given a republican: {p_yes_republican:.3f}"")
print(f""Probability of voting yes given a democrat: {p_yes_democrat:.3f}"")
print(f""Probability of voting yes overall: {p_yes_overall:.3f}"")",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:03:46.378292,llama-3-8b-v2,v2,False
base_rate_neglect,warning,education,graduation rate analysis by demographics,python,intermediate,"# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
grad_rate_data = pd.read_csv('grad_rate_data.csv')

# Calculate the overall graduation rate
overall_grad_rate = grad_rate_data['grad_rate'].mean()

# Calculate the graduation rate for each demographic group
male_grad_rate = grad_rate_data[grad_rate_data['gender'] == 'Male']['grad_rate'].mean()
female_grad_rate = grad_rate_data[grad_rate_data['gender'] == 'Female']['grad_rate'].mean()

# Plot the results
plt.bar(['Overall', 'Male', 'Female'], [overall_grad_rate, male_grad_rate, female_grad_rate])
plt.xlabel('Demographic Group')
plt.ylabel('Graduation Rate')
plt.title('Graduation Rates by Demographic Group')
plt.show()","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import norm

# Load the dataset
grad_rate_data = pd.read_csv('grad_rate_data.csv')

# Calculate the overall graduation rate
overall_grad_rate = grad_rate_data['grad_rate'].mean()

# Calculate the prior probability of graduation rates
prior_prob = norm.cdf(overall_grad_rate, loc=overall_grad_rate, scale=overall_grad_rate.std())

# Calculate the posterior probability of graduation rates for each demographic group
male_grad_rate = grad_rate_data[grad_rate_data['gender'] == 'Male']['grad_rate'].mean()
male_posterior_prob = norm.cdf(male_grad_rate, loc=overall_grad_rate, scale=overall_grad_rate.std())

female_grad_rate = grad_rate_data[grad_rate_data['gender'] == 'Female']['grad_rate'].mean()
female_posterior_prob = norm.cdf(female_grad_rate, loc=overall_grad_rate, scale=overall_grad_rate.std())

# Calculate the Bayes factor for each demographic group
bayes_factor_male = (male_posterior_prob - prior_prob) / (1 - prior_prob)
bayes_factor_female = (female_posterior_prob - prior_prob) / (1 - prior_prob)

# Plot the results
plt.bar(['Male', 'Female'], [bayes_factor_male, bayes_factor_female])
plt.xlabel('Demographic Group')
plt.ylabel('Bayes Factor')
plt.title('Bayes Factor for Graduation Rates by Demographic Group')
plt.show()",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:05:06.366813,llama-3-8b-v2,v2,False
base_rate_neglect,warning,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Load dataset
df = pd.read_csv('student_performance.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('passed', axis=1), df['passed'], test_size=0.2, random_state=42)

# Train a random forest model
rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X_train, y_train)

# Predict probabilities
y_pred_proba = rf_model.predict_proba(X_test)

# Get predicted probabilities for the positive class (passed)
positive_probs = y_pred_proba[:, 1]

# Calculate the accuracy of the model
accuracy = (positive_probs > 0.5).mean()
print(f""Accuracy: {accuracy:.3f}"")","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# Load dataset
df = pd.read_csv('student_performance.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('passed', axis=1), df['passed'], test_size=0.2, random_state=42)

# Calculate the prior probability of the positive class (passed)
prior_prob = y_train.mean()

# Train a random forest model
rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X_train, y_train)

# Predict probabilities
y_pred_proba = rf_model.predict_proba(X_test)

# Calculate the posterior probability of the positive class (passed) given the features
posterior_probs = y_pred_proba[:, 1]

# Calculate the accuracy of the model, taking into account the prior probability
accuracy = (posterior_probs > 0.5).mean() * prior_prob + (1 - (posterior_probs > 0.5).mean()) * (1 - prior_prob)
print(f""Accuracy: {accuracy:.3f}"")

# Calculate the confusion matrix, including the prior probability
conf_mat = confusion_matrix(y_test, (posterior_probs > 0.5))
print(conf_mat)",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:06:30.097867,llama-3-8b-v2,v2,False
base_rate_neglect,warning,healthcare,observational study of patient outcomes,python,intermediate,"import numpy as np
import pandas as pd

# Load data
df = pd.read_csv('hospital_data.csv')

# Define treatment and outcome variables
treatment = df['treatment']
outcome = df['outcome']

# Calculate the proportion of patients who received treatment and recovered
treatment_recovered = np.sum((treatment == 1) & (outcome == 1)) / len(treatment)
print(""Proportion of patients who received treatment and recovered:"", treatment_recovered)

# Calculate the proportion of patients who did not receive treatment and recovered
no_treatment_recovered = np.sum((treatment == 0) & (outcome == 1)) / len(treatment)
print(""Proportion of patients who did not receive treatment and recovered:"", no_treatment_recovered)

# Calculate the odds ratio
odds_ratio = treatment_recovered / no_treatment_recovered
print(""Odds ratio:"", odds_ratio)","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import pandas as pd

# Load data
df = pd.read_csv('hospital_data.csv')

# Define treatment and outcome variables
treatment = df['treatment']
outcome = df['outcome']

# Calculate the prior probability of recovery in the population
prior_recovery_rate = np.mean(outcome)

# Calculate the proportion of patients who received treatment and recovered
treatment_recovered = np.sum((treatment == 1) & (outcome == 1)) / len(treatment)
print(""Proportion of patients who received treatment and recovered:"", treatment_recovered)

# Calculate the proportion of patients who did not receive treatment and recovered
no_treatment_recovered = np.sum((treatment == 0) & (outcome == 1)) / len(treatment)
print(""Proportion of patients who did not receive treatment and recovered:"", no_treatment_recovered)

# Calculate the odds ratio, considering the prior probability of recovery
odds_ratio = (treatment_recovered / (1 - treatment_recovered)) / (no_treatment_recovered / (1 - no_treatment_recovered))
print(""Odds ratio, considering prior probability:"", odds_ratio)",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:07:47.208658,llama-3-8b-v2,v2,False
base_rate_neglect,warning,social_science,income inequality trends,python,intermediate,"import pandas as pd
from scipy.stats import binom

# Load crime data
crime_data = pd.read_csv('crime_data.csv')

# Calculate the proportion of crimes committed by each demographic group
prop_crimes_by_group = crime_data.groupby('demographic_group')['crime'].mean()

# Calculate the probability of a crime given a demographic group
prob_crime_given_group = prop_crimes_by_group / prop_crimes_by_group.sum()

# Create a new dataset with demographic information and a column for whether a crime was committed
new_data = pd.concat([crime_data, pd.get_dummies(crime_data['demographic_group'])], axis=1)

# Calculate the probability of a crime given each demographic group, ignoring the prior probabilities
prob_crime_given_group_new = binom.pmf(new_data['crime'], new_data['crime'].sum(), prob_crime_given_group)

print(prob_crime_given_group_new)","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import beta

# Load crime data
crime_data = pd.read_csv('crime_data.csv')

# Calculate the proportion of crimes committed by each demographic group
prop_crimes_by_group = crime_data.groupby('demographic_group')['crime'].mean()

# Calculate the prior probability of each demographic group
prior_prob = prop_crimes_by_group / prop_crimes_by_group.sum()

# Calculate the probability of a crime given each demographic group, taking into account the prior probabilities
prob_crime_given_group = prop_crimes_by_group / prop_crimes_by_group.sum() * prior_prob

# Create a new dataset with demographic information and a column for whether a crime was committed
new_data = pd.concat([crime_data, pd.get_dummies(crime_data['demographic_group'])], axis=1)

# Calculate the posterior probability of a crime given each demographic group
posterior_prob = new_data['crime'] * new_data['crime'].sum() * prior_prob

print(posterior_prob)",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:08:57.933074,llama-3-8b-v2,v2,False
base_rate_neglect,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd

# Load dataset
df = pd.read_csv('social_media_data.csv')

# Calculate the proportion of individuals with mental health issues
mental_health_rate = df['mental_health_issues'].mean()

# Calculate the proportion of individuals who use social media excessively
social_media_excessive_use_rate = df['social_media_excessive_use'].mean()

# Calculate the proportion of individuals who use social media excessively and have mental health issues
joint_rate = (df['social_media_excessive_use'] & df['mental_health_issues']).mean()

# Calculate the odds ratio
odds_ratio = joint_rate / (mental_health_rate * social_media_excessive_use_rate)

print(""Odds Ratio:"", odds_ratio)","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load dataset
df = pd.read_csv('social_media_data.csv')

# Calculate the prior probability of mental health issues in the population
prior_mental_health_rate = 0.25  # assume 25% of the population has mental health issues

# Calculate the proportion of individuals who use social media excessively
social_media_excessive_use_rate = df['social_media_excessive_use'].mean()

# Calculate the proportion of individuals who use social media excessively and have mental health issues
joint_rate = (df['social_media_excessive_use'] & df['mental_health_issues']).mean()

# Calculate the odds ratio, taking into account the prior probability of mental health issues
odds_ratio = (joint_rate / (prior_mental_health_rate * social_media_excessive_use_rate)) * ((1 - prior_mental_health_rate) / (1 - social_media_excessive_use_rate))

print(""Odds Ratio:"", odds_ratio)",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:10:05.666327,llama-3-8b-v2,v2,False
base_rate_neglect,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
from scipy.stats import binom

# Load the data
data = pd.read_csv('healthcare_data.csv')

# Define the outcome variable (e.g. patient survival)
outcome ='survived'

# Calculate the proportion of patients who survived
proportion_survived = data[outcome].mean()

# Calculate the proportion of patients who died
proportion_died = 1 - proportion_survived

# Calculate the odds ratio of a patient surviving given a certain treatment
odds_ratio = binom.probsmooth(data['treatment'], data[outcome], p=proportion_survived)

print(f'Odds Ratio: {odds_ratio:.2f}')","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import binom

# Load the data
data = pd.read_csv('healthcare_data.csv')

# Define the outcome variable (e.g. patient survival)
outcome ='survived'

# Calculate the proportion of patients who survived
proportion_survived = data[outcome].mean()

# Calculate the proportion of patients who died
proportion_died = 1 - proportion_survived

# Calculate the base rate of patient survival
base_rate = proportion_survived

# Calculate the odds ratio of a patient surviving given a certain treatment, accounting for the base rate
odds_ratio = binom.probsmooth(data['treatment'], data[outcome], p=base_rate)

print(f'Odds Ratio (accounting for base rate): {odds_ratio:.2f}')",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:11:07.129788,llama-3-8b-v2,v2,False
base_rate_neglect,warning,social_science,income inequality trends,python,intermediate,"import pandas as pd

# Load the data
data = pd.read_csv('voting_data.csv')

# Define the prior probability of a voter being Democrat
prior_democrat = 0.4

# Define the prior probability of a voter being Republican
prior_republican = 1 - prior_democrat

# Calculate the likelihood of a voter voting Republican given they are a Democrat
likelihood_democrat_republican = 0.2

# Calculate the likelihood of a voter voting Democrat given they are a Republican
likelihood_republican_democrat = 0.8

# Calculate the posterior probability of a voter being Democrat given they voted Republican
posterior_democrat_republican = likelihood_democrat_republican

# Calculate the posterior probability of a voter being Republican given they voted Democrat
posterior_republican_democrat = likelihood_republican_democrat

print(""Posterior probability of a Democrat voting Republican:"", posterior_democrat_republican)
print(""Posterior probability of a Republican voting Democrat:"", posterior_republican_democrat)","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd

# Load the data
data = pd.read_csv('voting_data.csv')

# Define the prior probability of a voter being Democrat
prior_democrat = 0.4

# Define the prior probability of a voter being Republican
prior_republican = 1 - prior_democrat

# Define the likelihood of a voter voting Republican given they are a Democrat
likelihood_democrat_republican = 0.2

# Define the likelihood of a voter voting Democrat given they are a Republican
likelihood_republican_democrat = 0.8

# Calculate the posterior probability of a voter being Democrat given they voted Republican
posterior_democrat_republican = (prior_democrat \* likelihood_democrat_republican) / (prior_democrat \* likelihood_democrat_republican + prior_republican \* (1-likelihood_republican_democrat))

# Calculate the posterior probability of a voter being Republican given they voted Democrat
posterior_republican_democrat = (prior_republican \* (1-likelihood_republican_democrat)) / (prior_democrat \* likelihood_republican_democrat + prior_republican \* (1-likelihood_republican_democrat))

print(""Posterior probability of a Democrat voting Republican:"", posterior_democrat_republican)
print(""Posterior probability of a Republican voting Democrat:"", posterior_republican_democrat)",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:12:23.693068,llama-3-8b-v2,v2,False
base_rate_neglect,warning,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
from scipy.stats import binom

# Load the dataset
df = pd.read_csv('graduation_rates.csv')

# Define the prior probabilities
prior_male = 0.5  # 50% of students are male
prior_female = 0.5  # 50% of students are female

# Define the likelihoods
male_grad_rate = 0.7  # 70% of male students graduate
female_grad_rate = 0.6  # 60% of female students graduate

# Calculate the posterior probabilities
posterior_male = (male_grad_rate * prior_male) / (male_grad_rate * prior_male + female_grad_rate * prior_female)
posterior_female = (female_grad_rate * prior_female) / (male_grad_rate * prior_male + female_grad_rate * prior_female)

# Print the results
print(""Posterior probability of graduation for male students:"", posterior_male)
print(""Posterior probability of graduation for female students:"", posterior_female)","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import binom

# Load the dataset
df = pd.read_csv('graduation_rates.csv')

# Define the prior probabilities
prior_male = 0.5  # 50% of students are male
prior_female = 0.5  # 50% of students are female

# Define the likelihoods
male_grad_rate = 0.7  # 70% of male students graduate
female_grad_rate = 0.6  # 60% of female students graduate

# Calculate the prior-conditional probabilities
prior_male_grad = prior_male * male_grad_rate
prior_female_grad = prior_female * female_grad_rate

# Calculate the posterior probabilities
posterior_male = prior_male_grad / (prior_male_grad + prior_female_grad)
posterior_female = prior_female_grad / (prior_male_grad + prior_female_grad)

# Print the results
print(""Posterior probability of graduation for male students:"", posterior_male)
print(""Posterior probability of graduation for female students:"", posterior_female)",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:13:31.643898,llama-3-8b-v2,v2,False
base_rate_neglect,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
from sklearn.naive_bayes import MultinomialNB

# Load the data
df = pd.read_csv('voting_data.csv')

# Split the data into features (X) and target variable (y)
X = df.drop(['voted_democrat', 'voted_republican'], axis=1)
y = df['voted_democrat']

# Train a Naive Bayes classifier
clf = MultinomialNB()
clf.fit(X, y)

# Make predictions
y_pred = clf.predict(X)

# Calculate the accuracy
accuracy = sum(y_pred == y) / len(y)
print(f""Accuracy: {accuracy:.3f}"")","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import binarize

# Load the data
df = pd.read_csv('voting_data.csv')

# Split the data into features (X) and target variable (y)
X = df.drop(['voted_democrat', 'voted_republican'], axis=1)
y = df['voted_democrat']

# Calculate the prior probabilities of the classes
prior_democrat = sum(y) / len(y)
prior_republican = 1 - prior_democrat

# Binarize the data
X_bin = binarize(X)

# Train a Naive Bayes classifier with prior probabilities
clf = MultinomialNB(priors=[prior_democrat, prior_republican])
clf.fit(X_bin, y)

# Make predictions
y_pred = clf.predict(X_bin)

# Calculate the accuracy
accuracy = sum(y_pred == y) / len(y)
print(f""Accuracy: {accuracy:.3f}"")",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:14:33.282118,llama-3-8b-v2,v2,False
base_rate_neglect,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np

# Load voting data
data = pd.read_csv('voting_data.csv')

# Define the outcome variable (voted for party A)
outcome = data['voted_for_A']

# Define the predictor variables
predictors = ['age', 'education', 'income']

# Fit a logistic regression model
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(data[predictors], outcome)

# Predict probabilities
probs = model.predict_proba(data[predictors])[:, 1]

# Calculate the accuracy of the model
accuracy = np.mean(outcome == (probs > 0.5))

print(f""Accuracy: {accuracy:.3f}"")","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import beta

# Load voting data
data = pd.read_csv('voting_data.csv')

# Define the outcome variable (voted for party A)
outcome = data['voted_for_A']

# Define the prior probability of voting for party A
prior_prob = 0.5

# Define the predictor variables
predictors = ['age', 'education', 'income']

# Fit a logistic regression model
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(data[predictors], outcome)

# Predict probabilities
probs = model.predict_proba(data[predictors])[:, 1]

# Calculate the posterior probability by updating the prior with the predicted probabilities
posterior_probs = probs * (prior_prob * (1 - probs)) / (prior_prob * probs + (1 - prior_prob) * (1 - probs))

# Calculate the accuracy of the model
accuracy = np.mean(outcome == (posterior_probs > 0.5))

print(f""Accuracy: {accuracy:.3f}"")",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:15:41.518418,llama-3-8b-v2,v2,False
base_rate_neglect,warning,education,student performance prediction,python,intermediate,"import pandas as pd
from scipy.stats import binom

# Load the dataset
df = pd.read_csv('education_data.csv')

# Define the prior probabilities
prior_probability_teaching_method = 0.5  # Assuming 50% of students are taught using Method A

# Define the likelihoods
likelihood_method_a = 0.8  # 80% of students who use Method A pass the test
likelihood_method_b = 0.6  # 60% of students who use Method B pass the test

# Calculate the posterior probabilities
posterior_probability_method_a = likelihood_method_a * prior_probability_teaching_method
posterior_probability_method_b = likelihood_method_b * prior_probability_teaching_method

# Print the results
print(""Posterior probability of Method A:"", posterior_probability_method_a)
print(""Posterior probability of Method B:"", posterior_probability_method_b)","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from scipy.stats import binom

# Load the dataset
df = pd.read_csv('education_data.csv')

# Define the prior probabilities
prior_probability_teaching_method_a = 0.5  # Assuming 50% of students are taught using Method A
prior_probability_teaching_method_b = 1 - prior_probability_teaching_method_a  # Prior probability of Method B

# Define the likelihoods
likelihood_method_a = 0.8  # 80% of students who use Method A pass the test
likelihood_method_b = 0.6  # 60% of students who use Method B pass the test

# Calculate the posterior probabilities
posterior_probability_method_a = (prior_probability_teaching_method_a * likelihood_method_a) / (prior_probability_teaching_method_a * likelihood_method_a + prior_probability_teaching_method_b * likelihood_method_b)
posterior_probability_method_b = 1 - posterior_probability_method_a

# Print the results
print(""Posterior probability of Method A:"", posterior_probability_method_a)
print(""Posterior probability of Method B:"", posterior_probability_method_b)",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:16:53.427313,llama-3-8b-v2,v2,False
base_rate_neglect,warning,business,sales performance across regions,python,intermediate,"import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# Load dataset
df = pd.read_csv('churn_data.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Train a random forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f""Accuracy: {accuracy:.3f}"")

# Create a confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

# Load dataset
df = pd.read_csv('churn_data.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Calculate prior probability of churn (base rate)
prior_churn = y_train.mean()

# Train a random forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Calculate accuracy, taking into account the prior probability
accuracy = accuracy_score(y_test, y_pred)
print(f""Accuracy: {accuracy:.3f}"")

# Calculate the true positive rate, taking into account the prior probability
true_positive_rate = (y_pred == y_test).mean()
print(f""True Positive Rate: {true_positive_rate:.3f}"")

# Create a confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:18:09.169874,llama-3-8b-v2,v2,False
base_rate_neglect,warning,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
from sklearn.metrics import accuracy_score

# Load the data
data = pd.read_csv('marketing_data.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data.drop('converted', axis=1), data['converted'], test_size=0.2, random_state=42)

# Train a logistic regression model
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f""Accuracy: {accuracy:.3f}"")

# Calculate the prior probability of conversion
prior_prob = sum(y_test)/len(y_test)
print(f""Prior probability of conversion: {prior_prob:.3f}"")","{'summary': 'Base Rate Neglect', 'detailed_explanation': 'Base Rate Neglect: Posterior probability depends on both likelihood and base rate', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.metrics import accuracy_score
from scipy.stats import binom

# Load the data
data = pd.read_csv('marketing_data.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data.drop('converted', axis=1), data['converted'], test_size=0.2, random_state=42)

# Train a logistic regression model
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f""Accuracy: {accuracy:.3f}"")

# Calculate the prior probability of conversion
prior_prob = sum(y_test)/len(y_test)

# Calculate the probability of conversion given the model's predictions
posterior_prob = binom.pmf(y_pred, len(y_pred), prior_prob)

# Calculate the expected value of the model's predictions given the prior probability
expected_accuracy = sum([prior_prob * prob for prob in posterior_prob])
print(f""Expected accuracy: {expected_accuracy:.3f}"")",['Posterior probability depends on both likelihood and base rate'],Posterior probability depends on both likelihood and base rate,2025-12-16T10:19:19.892562,llama-3-8b-v2,v2,False
extrapolation,warning,social_science,voting behavior analysis,python,intermediate,"import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import interp1d

# Generate some sample data
x = np.array([2010, 2012, 2014, 2016, 2018])
y = np.array([50, 55, 60, 65, 70])

# Create an interpolation function
f = interp1d(x, y)

# Extrapolate beyond the data range
x_pred = np.array([2019, 2020, 2021, 2022, 2023])
y_pred = f(x_pred)

print(""Predicted values:"", y_pred)
plt.plot(x, y, 'o', label='Original data')
plt.plot(x_pred, y_pred, label='Extrapolated values')
plt.legend()
plt.show()","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import interp1d

# Generate some sample data
x = np.array([2010, 2012, 2014, 2016, 2018])
y = np.array([50, 55, 60, 65, 70])

# Create an interpolation function
f = interp1d(x, y)

# Extrapolate within the data range
x_pred = np.array([2015, 2017, 2020])
y_pred = f(x_pred)

# Extrapolate beyond the data range with caution
x_pred_ext = np.array([2021, 2022, 2023])
y_pred_ext = f(x_pred_ext)  # Use a warning message to indicate caution

print(""Predicted values within data range:"", y_pred)
print(""Predicted values beyond data range (use with caution):"", y_pred_ext)
plt.plot(x, y, 'o', label='Original data')
plt.plot(x_pred, y_pred, label='Extrapolated values within data range')
plt.plot(x_pred_ext, y_pred_ext, label='Extrapolated values beyond data range (use with caution)')
plt.legend()
plt.show()",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:20:28.165890,llama-3-8b-v2,v2,False
extrapolation,warning,healthcare,observational study of patient outcomes,python,intermediate,"import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Load dataset
df = pd.read_csv('data.csv')

# Define X and y
X = df[['age','sex']]
y = df['disease_prevalence']

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on test data
y_pred = model.predict(X_test)

# Make predictions on new data point (outside observed range)
new_patient = np.array([[60, 'Male'])  # 60-year-old male
new_patient_pred = model.predict(new_patient)
print(""Predicted disease prevalence for new patient:"", new_patient_pred)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialRegression

# Load dataset
df = pd.read_csv('data.csv')

# Define X and y
X = df[['age','sex']]
y = df['disease_prevalence']

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create polynomial regression model to handle extrapolation
poly_model = PolynomialRegression(degree=2)  # Quadratic polynomial
poly_model.fit(X_train, y_train)

# Make predictions on test data
y_pred = poly_model.predict(X_test)

# Make predictions on new data point (outside observed range)
new_patient = np.array([[60, 'Male'])  # 60-year-old male
new_patient_pred = poly_model.predict(new_patient)
print(""Predicted disease prevalence for new patient:"", new_patient_pred)",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:21:41.667558,llama-3-8b-v2,v2,False
extrapolation,warning,business,sales performance across regions,python,intermediate,"import numpy as np
import pandas as pd

# Sample data
data = pd.DataFrame({
    'Days': [1, 2, 3, 4, 5, 6, 7],
    'Clicks': [10, 15, 20, 25, 30, 35, 40]
})

# Create a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(data[['Days']], data['Clicks'])

# Predict clicks for days 8-10
predictions = model.predict([[8], [9], [10]])

print(predictions)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import pandas as pd

# Sample data
data = pd.DataFrame({
    'Days': [1, 2, 3, 4, 5, 6, 7],
    'Clicks': [10, 15, 20, 25, 30, 35, 40]
})

# Create a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(data[['Days']], data['Clicks'])

# Predict clicks within the observed range (1-7 days)
predictions = model.predict([[1, 2, 3, 4, 5, 6, 7]])

print(predictions)",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:22:33.963923,llama-3-8b-v2,v2,False
extrapolation,warning,education,teaching method effectiveness comparison,python,intermediate,"import numpy as np
import pandas as pd

# Load the dataset
df = pd.read_csv('student_data.csv')

# Define the feature and target variables
X = df.drop(['performance'], axis=1)
y = df['performance']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Predict the performance of students with scores between 0 and 100
predictions = model.predict(np.array([[0, 20], [0, 30], [0, 40], [0, 50], [0, 60], [0, 70], [0, 80], [0, 90], [0, 100]]))

print(predictions)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import pandas as pd

# Load the dataset
df = pd.read_csv('student_data.csv')

# Define the feature and target variables
X = df.drop(['performance'], axis=1)
y = df['performance']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Predict the performance of students with scores within the observed range
max_score = X_train.max().values[0]
predictions = model.predict(np.array([[0, max_score]]))

print(predictions)",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:23:39.276665,llama-3-8b-v2,v2,False
extrapolation,warning,education,teaching method effectiveness comparison,python,intermediate,"import numpy as np
import pandas as pd

# Load the dataset
df = pd.read_csv('education_data.csv')

# Define the x and y variables
x = df['years_of_experience']
y = df['test_scores']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Create a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train, y_train)

# Make predictions on the test set
y_pred = model.predict(x_test)

# Make predictions for 5 years beyond the observed data range
extra_years = np.array([x.max() + 1, x.max() + 2, x.max() + 3, x.max() + 4, x.max() + 5])
extra_predictions = model.predict(extra_years)

print(extra_predictions)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import pandas as pd

# Load the dataset
df = pd.read_csv('education_data.csv')

# Define the x and y variables
x = df['years_of_experience']
y = df['test_scores']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Create a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train, y_train)

# Make predictions on the test set
y_pred = model.predict(x_test)

# Calculate the maximum observed year
max_observed_year = x.max()

# Make predictions for 5 years beyond the observed data range using a more sophisticated method
extra_years = np.arange(max_observed_year + 1, max_observed_year + 6)
extra_predictions = model.predict(extra_years[:, np.newaxis])

print(extra_predictions)",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:24:49.409118,llama-3-8b-v2,v2,False
extrapolation,warning,education,student performance prediction,python,intermediate,"import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('education_data.csv')

# Define X and y
X = df[['online', 'in_person']]
y = df['outcomes']

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on test data
y_pred = model.predict(X_test)

# Extrapolate beyond observed data range
new_data = pd.DataFrame({'online': [0.9], 'in_person': [0.1]})
new_pred = model.predict(new_data)

print(new_pred)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler

# Load data
df = pd.read_csv('education_data.csv')

# Define X and y
X = df[['online', 'in_person']]
y = df['outcomes']

# Scale data using Min-Max Scaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Define new data to extrapolate
new_data = pd.DataFrame({'online': [0.9], 'in_person': [0.1]})
new_data_scaled = scaler.transform(new_data)

# Extrapolate beyond observed data range
new_pred = model.predict(new_data_scaled)

print(new_pred)",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:26:00.332976,llama-3-8b-v2,v2,False
extrapolation,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np

# Load the data
data = pd.read_csv('disease_prevalence.csv')

# Define the features and target variable
X = data.drop(['prevalence'], axis=1)
y = data['prevalence']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Extrapolate the model to predict prevalence for a new demographic with age 65
new_demographic = pd.DataFrame({'age': [65],'sex': [0],'region': [0]})
new_prediction = model.predict(new_demographic)
print(new_prediction)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np

# Load the data
data = pd.read_csv('disease_prevalence.csv')

# Define the features and target variable
X = data.drop(['prevalence'], axis=1)
y = data['prevalence']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Define the minimum and maximum ages in the training data
min_age = X_train['age'].min()
max_age = X_train['age'].max()

# Define a new demographic with age within the observed range
new_demographic = pd.DataFrame({'age': [(max_age + 1)],'sex': [0],'region': [0]})
new_prediction = model.predict(new_demographic)
print(new_prediction)

# Alternatively, you can also use interpolation or smoothing techniques to make predictions beyond the observed range, but this is a more advanced topic.",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:27:13.718037,llama-3-8b-v2,v2,False
extrapolation,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import numpy as np

# Load crime rate data
crime_data = pd.read_csv('crime_rate_data.csv')

# Define the feature matrix X and target vector y
X = crime_data[['population', 'unemployment_rate']]
y = crime_data['crime_rate']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = model.predict(X_test)

# Now, let's make a prediction for a new city with 100,000 population and 5% unemployment rate
new_city = np.array([[100000, 0.05]])  # FLAW: this is outside the observed data range!
new_city_prediction = model.predict(new_city)
print(f""The predicted crime rate for the new city is: {new_city_prediction[0]:.2f}"")","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np

# Load crime rate data
crime_data = pd.read_csv('crime_rate_data.csv')

# Define the feature matrix X and target vector y
X = crime_data[['population', 'unemployment_rate']]
y = crime_data['crime_rate']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = model.predict(X_test)

# Define a new city with a population and unemployment rate within the observed data range
new_city = np.array([[25000, 4.5]])  # Make sure to stay within the observed data range!

# Make a prediction for the new city
new_city_prediction = model.predict(new_city)
print(f""The predicted crime rate for the new city is: {new_city_prediction[0]:.2f}"")",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:28:33.362211,llama-3-8b-v2,v2,False
extrapolation,warning,education,student performance prediction,python,intermediate,"import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('data.csv')

# Define the feature and target variables
X = df[['feature1', 'feature2']]
y = df['target']

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the training data
y_pred_train = model.predict(X_train)

# Make predictions on the testing data
y_pred_test = model.predict(X_test)

# Now, let's make predictions on new data that's outside the observed range
new_data = pd.DataFrame({'feature1': [100], 'feature2': [50]})
new_pred = model.predict(new_data)
print(new_pred)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('data.csv')

# Define the feature and target variables
X = df[['feature1', 'feature2']]
y = df['target']

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the training data
y_pred_train = model.predict(X_train)

# Make predictions on the testing data
y_pred_test = model.predict(X_test)

# Now, let's make predictions on new data within the observed range
new_data = pd.DataFrame({'feature1': [60], 'feature2': [30]})
new_pred = model.predict(new_data)
print(new_pred)

# If we really need to make predictions outside the observed range, we should use a different approach, such as
# using a different model that's designed for extrapolation or using a different method to handle out-of-range values",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:29:51.955730,llama-3-8b-v2,v2,False
extrapolation,warning,education,online vs in-person learning outcomes,python,intermediate,"import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Generate sample data
np.random.seed(0)
X = np.random.uniform(0, 10, size=(100, 1))
y = 2 * X + np.random.normal(0, 1, size=(100, 1))

# Create a linear regression model
model = LinearRegression()
model.fit(X, y)

# Plot the data and the regression line
plt.scatter(X, y)
plt.plot(X, model.predict(X))
plt.xlabel('Teaching Hours')
plt.ylabel('Test Score')
plt.title('Teaching Method Effectiveness Comparison')
plt.show()

# Predict scores for 15 hours of teaching
predicted_scores = model.predict(np.array([[15]]))
print(""Predicted scores for 15 hours of teaching:"", predicted_scores)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Generate sample data
np.random.seed(0)
X = np.random.uniform(0, 10, size=(100, 1))
y = 2 * X + np.random.normal(0, 1, size=(100, 1))

# Create a linear regression model
model = LinearRegression()
model.fit(X, y)

# Plot the data and the regression line
plt.scatter(X, y)
plt.plot(X, model.predict(X))
plt.xlabel('Teaching Hours')
plt.ylabel('Test Score')
plt.title('Teaching Method Effectiveness Comparison')
plt.show()

# Create a new dataset for extrapolation
X_new = np.linspace(0, 10, 100)  # within the observed data range
y_pred = model.predict(X_new)

# Create a new dataset for extrapolation (beyond observed data range)
X_new_extrapolation = np.array([[12], [15]])  # outside the observed data range
y_pred_extrapolation = model.predict(X_new_extrapolation)

print(""Predicted scores for extrapolation:"")
print(""For 12 hours of teaching:"", y_pred_extrapolation[0][0])
print(""For 15 hours of teaching:"", y_pred_extrapolation[1][0])",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:31:06.045827,llama-3-8b-v2,v2,False
extrapolation,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('hospital_data.csv')

# Define X and y
X = df[['age', 'blood_pressure']]
y = df['treatment_outcome']

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = model.predict(X_test)

# Now, let's try to make a prediction for a patient who is 60 years old and has a blood pressure of 120/80 mmHg
new_patient = np.array([[60, 120/80]])

# Make a prediction
predicted_outcome = model.predict(new_patient)
print(predicted_outcome)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('hospital_data.csv')

# Define X and y
X = df[['age', 'blood_pressure']]
y = df['treatment_outcome']

# Split data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = model.predict(X_test)

# Now, let's try to make a prediction for a patient who is 60 years old and has a blood pressure of 120/80 mmHg
new_patient = np.array([[50, 120/80]])  # Note: we're using a value within the observed data range

# Make a prediction
predicted_outcome = model.predict(new_patient)
print(predicted_outcome)",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:32:31.039377,llama-3-8b-v2,v2,False
extrapolation,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import numpy as np
import matplotlib.pyplot as plt

# Generate some sample data
np.random.seed(0)
x = np.random.uniform(0, 10, 100)
y = 2 * x + np.random.normal(0, 1, 100)

# Plot the data
plt.scatter(x, y)
plt.xlabel('Age')
plt.ylabel('Blood Pressure')
plt.title('Relationship between Age and Blood Pressure')
plt.show()

# Define a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x[:, np.newaxis], y)

# Extrapolate the model to predict blood pressure for patients aged 15 and 20
new_x = np.array([15, 20])
new_y = model.predict(new_x[:, np.newaxis])

print(""Predicted blood pressure for patients aged 15 and 20:"", new_y)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialRegression

# Generate some sample data
np.random.seed(0)
x = np.random.uniform(0, 10, 100)
y = 2 * x + np.random.normal(0, 1, 100)

# Plot the data
plt.scatter(x, y)
plt.xlabel('Age')
plt.ylabel('Blood Pressure')
plt.title('Relationship between Age and Blood Pressure')
plt.show()

# Define a polynomial regression model with a degree that allows extrapolation
poly_model = PolynomialRegression(degree=2)
poly_model.fit(x[:, np.newaxis], y)

# Extrapolate the model to predict blood pressure for patients aged 15 and 20
new_x = np.array([15, 20])
new_y = poly_model.predict(new_x[:, np.newaxis])

print(""Predicted blood pressure for patients aged 15 and 20:"", new_y)

# Visualize the extrapolated predictions
plt.plot(x, y, 'o', label='Observed data')
plt.plot(new_x, poly_model.predict(new_x[:, np.newaxis]), 'o', label='Extrapolated predictions')
plt.xlabel('Age')
plt.ylabel('Blood Pressure')
plt.title('Relationship between Age and Blood Pressure')
plt.legend()
plt.show()",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:33:44.392157,llama-3-8b-v2,v2,False
extrapolation,warning,business,sales performance across regions,python,intermediate,"import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('churn_data.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Make predictions outside the observed data range
new_customer = pd.DataFrame({'tenure': [10], 'age': [30], 'total_charges': [5000]})
new_customer_pred = model.predict(new_customer)
print(new_customer_pred)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('churn_data.csv')

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df.drop('churn', axis=1), df['churn'], test_size=0.2, random_state=42)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Define the observed data range for each feature
observed_tenure_range = (1, 72)
observed_age_range = (18, 65)
observed_total_charges_range = (10, 10000)

# Function to check if new customer's values are within observed data range
def is_within_range(new_customer):
    return (observed_tenure_range[0] <= new_customer['tenure'] <= observed_tenure_range[1] and
            observed_age_range[0] <= new_customer['age'] <= observed_age_range[1] and
            observed_total_charges_range[0] <= new_customer['total_charges'] <= observed_total_charges_range[1])

# Make predictions only if new customer's values are within observed data range
new_customer = pd.DataFrame({'tenure': [10], 'age': [30], 'total_charges': [5000]})
if is_within_range(new_customer):
    new_customer_pred = model.predict(new_customer)
    print(new_customer_pred)
else:
    print(""New customer's values are outside the observed data range. Cannot make predictions."")",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:35:13.070148,llama-3-8b-v2,v2,False
extrapolation,warning,business,customer churn prediction analysis,python,intermediate,"**ERROR EXPLANATION**

The code above attempts to predict the price at X=15, which is outside the observed data range. The model is trained on data between 0 and 10, but the prediction is made at 15. This is flawed because the model has not seen any data points at or above 10, so it has no basis to make a reliable prediction.

**CORRECTED CODE**","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'redict the price at X=15, which is outside the observed data range\nX_pred = np.array([[15]])  # Predict at X=15\ny_pred = model.predict(X_pred)\n\nprint(""Predicted price at X=15:"", y_pred[0][0])\n\n# Plot the data and the predicted line\nplt.scatter(X, y)\nplt.plot(X_pred, y_pred, \'ro\')\nplt.xlabel(\'X\')\nplt.ylabel(\'Y\')\nplt.show()\nPython\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate some sample data\nnp.random.seed(0)\nX = np.random.uniform', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","**ERROR EXPLANATION**

The code above attempts to predict the price at X=15, which is outside the observed data range. The model is trained on data between 0 and 10, but the prediction is made at 15. This is flawed because the model has not seen any data points at or above 10, so it has no basis to make a reliable prediction.

**CORRECTED CODE**",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:36:18.206250,llama-3-8b-v2,v2,False
extrapolation,warning,business,pricing strategy impact analysis,python,intermediate,"import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Sample data
df = pd.DataFrame({
    'Price': [10, 12, 14, 16, 18],
    'Sales': [100, 120, 140, 160, 180]
})

# Create a linear regression model
model = LinearRegression()
model.fit(df[['Price']], df['Sales'])

# Predict sales for a new price outside the observed range
new_price = 22
predicted_sales = model.predict([[new_price]])

print(""Predicted sales for price"", new_price, ""is"", predicted_sales[0])","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialRegression

# Sample data
df = pd.DataFrame({
    'Price': [10, 12, 14, 16, 18],
    'Sales': [100, 120, 140, 160, 180]
})

# Create a polynomial regression model with a high degree to allow for extrapolation
model = PolynomialRegression(degree=3)
model.fit(df[['Price']], df['Sales'])

# Predict sales for a new price outside the observed range
new_price = 22
predicted_sales = model.predict([[new_price])

print(""Predicted sales for price"", new_price, ""is"", predicted_sales[0])",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:37:25.590934,llama-3-8b-v2,v2,False
extrapolation,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np

# Load the data
df = pd.read_csv('crime_data.csv')

# Define the feature matrix X and target vector y
X = df[['population', 'unemployment_rate', 'weather']]
y = df['crime_rate']

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X, y)

# Predict the crime rate for a city with population 100,000, unemployment rate 5%, and weather 0.5
new_city = np.array([[100000, 0.05, 0.5]])
prediction = model.predict(new_city)
print(""Predicted crime rate:"", prediction)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

# Load the data
df = pd.read_csv('crime_data.csv')

# Scale the features using StandardScaler to reduce the impact of extreme values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[['population', 'unemployment_rate', 'weather']])

# Define the feature matrix X and target vector y
X_scaled = pd.DataFrame(X_scaled, columns=['population', 'unemployment_rate', 'weather'])
y = df['crime_rate']

# Train a linear regression model
model = LinearRegression()
model.fit(X_scaled, y)

# Predict the crime rate for a city with population 100,000, unemployment rate 5%, and weather 0.5
new_city = scaler.transform([[100000, 0.05, 0.5]])
new_city = pd.DataFrame(new_city, columns=['population', 'unemployment_rate', 'weather'])
prediction = model.predict(new_city)
print(""Predicted crime rate:"", prediction)",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:38:43.889996,llama-3-8b-v2,v2,False
extrapolation,warning,education,teaching method effectiveness comparison,python,intermediate,"import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Data for teaching method effectiveness comparison
x = np.array([1, 2, 3, 4, 5])  # number of hours of instruction
y = np.array([20, 30, 40, 50, 60])  # test scores

# Create linear regression model
model = LinearRegression()
model.fit(x.reshape(-1, 1), y)

# Predict test scores for 6 hours of instruction (outside observed data range)
y_pred = model.predict(np.array([[6]]))

print(""Predicted test score for 6 hours of instruction:"", y_pred[0])","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Data for teaching method effectiveness comparison
x = np.array([1, 2, 3, 4, 5])  # number of hours of instruction
y = np.array([20, 30, 40, 50, 60])  # test scores

# Create linear regression model
model = LinearRegression()
model.fit(x.reshape(-1, 1), y)

# Define a new feature vector for prediction
x_new = np.linspace(1, 5, 100)  # generate a new feature vector within observed data range

# Predict test scores within the observed data range
y_pred = model.predict(x_new.reshape(-1, 1))

# Plot the predicted scores within the observed data range
plt.plot(x, y, 'o', label='Observed data')
plt.plot(x_new, y_pred, label='Predicted scores')
plt.xlabel('Number of hours of instruction')
plt.ylabel('Test scores')
plt.legend()
plt.show()

# Note: To make predictions outside the observed data range, you would need to use a different approach, such as using a different model or incorporating additional data.",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:39:45.912471,llama-3-8b-v2,v2,False
extrapolation,warning,business,marketing campaign effectiveness study,python,intermediate,"import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Generate some data
np.random.seed(0)
X = np.array([[1, 2, 3, 4, 5], [10, 20, 30, 40, 50]].T)
y = np.array([20, 40, 60, 80, 100])

# Create a linear regression model
model = LinearRegression()
model.fit(X, y)

# Generate some new data outside the observed range
new_X = np.array([[6, 7, 8, 9, 10, 11, 12, 13, 14, 15]].T)

# Make predictions
y_pred = model.predict(new_X)

print(""Predictions:"", y_pred)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Generate some data
np.random.seed(0)
X = np.array([[1, 2, 3, 4, 5], [10, 20, 30, 40, 50]].T)
y = np.array([20, 40, 60, 80, 100])

# Pad the training data with the new data points
new_X = np.array([[6, 7, 8, 9, 10, 11, 12, 13, 14, 15]].T)
X_padded = np.concatenate((X, new_X), axis=0)
y_padded = np.concatenate((y, y[-1] + np.zeros_like(new_X)), axis=0)

# Create a linear regression model
model = LinearRegression()
model.fit(X_padded, y_padded)

# Make predictions within the observed data range
y_pred = model.predict(X_padded)

print(""Predictions:"", y_pred)",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:41:02.971059,llama-3-8b-v2,v2,False
extrapolation,warning,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('patient_outcomes.csv')

# Define the feature and target variables
X = df[['age', 'treatment']]
y = df['outcome']

# Create a linear regression model
model = LinearRegression()

# Fit the model to the data
model.fit(X, y)

# Predict outcomes for patients with ages 25-30
new_patient_data = pd.DataFrame({'age': [25, 26, 27, 28, 29, 30], 'treatment': [0, 0, 0, 0, 0, 0]})
new_patient_predictions = model.predict(new_patient_data)
print(new_patient_predictions)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('patient_outcomes.csv')

# Define the feature and target variables
X = df[['age', 'treatment']]
y = df['outcome']

# Create a linear regression model
model = LinearRegression()

# Fit the model to the data
model.fit(X, y)

# Extract the maximum and minimum observed ages
min_age = X['age'].min()
max_age = X['age'].max()

# Define a new function to make predictions within the observed range
def predict_outcomes(age):
    if age < min_age or age > max_age:
        raise ValueError(""Age is outside the observed range"")
    new_patient_data = pd.DataFrame({'age': [age], 'treatment': [0]})
    return model.predict(new_patient_data)[0]

# Test the function
print(predict_outcomes(25))  # Should work
print(predict_outcomes(65))  # Should work
print(predict_outcomes(30))  # Should raise an error",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:42:10.383792,llama-3-8b-v2,v2,False
extrapolation,warning,business,customer churn prediction analysis,python,intermediate,"import numpy as np
import pandas as pd

# Load the data
data = pd.read_csv('marketing_campaign.csv')

# Define the predictor variables
X = data[['TV Ads', 'Social Media', 'Print Ads']]

# Define the response variable
y = data['Sales']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit a linear regression model to the training data
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Now, let's try to predict sales for a new campaign with 50 TV Ads and 30 Social Media Ads
new_campaign = pd.DataFrame({'TV Ads': [50], 'Social Media': [30]})
new_campaign_pred = model.predict(new_campaign)
print(new_campaign_pred)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import pandas as pd

# Load the data
data = pd.read_csv('marketing_campaign.csv')

# Define the predictor variables
X = data[['TV Ads', 'Social Media', 'Print Ads']]

# Define the response variable
y = data['Sales']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit a linear regression model to the training data
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Now, let's try to predict sales for a new campaign with TV Ads and Social Media Ads within the range of the training data
new_campaign = pd.DataFrame({'TV Ads': [15], 'Social Media': [10]})
new_campaign_pred = model.predict(new_campaign)
print(new_campaign_pred)

# If we really need to make predictions outside the range, we should consider using techniques like interpolation or extrapolation
# or re-collecting data for the new range. This is a topic for a separate discussion.",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:43:31.794318,llama-3-8b-v2,v2,False
extrapolation,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('hospital_data.csv')

# Create a linear regression model
model = LinearRegression()

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df[['age', 'treatment'], df['outcome']), test_size=0.2, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions on test data
y_pred = model.predict(X_test)

# Extrapolate predictions outside observed data range
new_patient = pd.DataFrame({'age': [30, 40, 50], 'treatment': [1, 0, 1]})
new_patient['outcome'] = model.predict(new_patient)
print(new_patient)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler

# Load data
df = pd.read_csv('hospital_data.csv')

# Scale the data using Min-Max Scaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(df[['age', 'treatment']])
y_scaled = scaler.fit_transform(df[['outcome']])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)

# Train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on test data
y_pred = model.predict(X_test)

# Extrapolate predictions within observed data range
new_patient = pd.DataFrame({'age': [[0.4], [0.6]], 'treatment': [[1], [0]]})  # scaled values
new_patient['outcome'] = model.predict(new_patient)

# Convert scaled predictions back to original scale
new_patient['outcome'] = scaler.inverse_transform(new_patient[['outcome']])
print(new_patient)",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:44:49.983207,llama-3-8b-v2,v2,False
extrapolation,warning,social_science,voting behavior analysis,python,intermediate,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('social_media_usage.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df[['age', 'hours_slept', 'hours_used_social_media']], df['mental_health'], test_size=0.2, random_state=42)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Predict mental health for a new user with age 30, hours slept 7, and hours used social media 4
new_user = np.array([[30, 7, 4]])
predicted_mental_health = model.predict(new_user)
print(""Predicted mental health for the new user:"", predicted_mental_health)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('social_media_usage.csv')

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df[['age', 'hours_slept', 'hours_used_social_media']], df['mental_health'], test_size=0.2, random_state=42)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Find the maximum and minimum values in the training data
max_age = X_train[:, 0].max()
min_age = X_train[:, 0].min()
max_hours_slept = X_train[:, 1].max()
min_hours_slept = X_train[:, 1].min()
max_hours_used_social_media = X_train[:, 2].max()
min_hours_used_social_media = X_train[:, 2].min()

# Define a function to clip the input values to the observed range
def clip_input(new_user):
    new_user[0] = np.clip(new_user[0], min_age, max_age)
    new_user[1] = np.clip(new_user[1], min_hours_slept, max_hours_slept)
    new_user[2] = np.clip(new_user[2], min_hours_used_social_media, max_hours_used_social_media)
    return new_user

# Clip the input values for the new user
new_user = np.array([[30, 7, 4]])
new_user = clip_input(new_user)

# Make predictions for the new user
predicted_mental_health = model.predict(new_user)
print(""Predicted mental health for the new user:"", predicted_mental_health)",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:46:16.671785,llama-3-8b-v2,v2,False
extrapolation,warning,business,sales performance across regions,python,intermediate,"import numpy as np
import pandas as pd

# Sample data
data = pd.DataFrame({
    'Region': ['North', 'North', 'South', 'South', 'East', 'East', 'West', 'West'],
    'Sales': [100, 120, 80, 90, 110, 130, 70, 80]
})

# Train a linear regression model
X = np.array(data['Region'])
y = np.array(data['Sales'])
model = np.poly1d(np.polyfit(X, y, 1))

# Make predictions for new regions (outside observed data range)
new_regions = ['North', 'South', 'East', 'West', 'NorthEast', 'SouthWest']
new_sales = model(np.array(new_regions))

print(new_sales)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import pandas as pd

# Sample data
data = pd.DataFrame({
    'Region': ['North', 'North', 'South', 'South', 'East', 'East', 'West', 'West'],
    'Sales': [100, 120, 80, 90, 110, 130, 70, 80]
})

# Train a linear regression model
X = np.array(data['Region'])
y = np.array(data['Sales'])
model = np.poly1d(np.polyfit(X, y, 1))

# Make predictions for new regions within the observed data range
new_regions = ['North', 'South', 'East', 'West']
new_sales = model(np.array(new_regions))

print(new_sales)

# To make predictions for new regions outside the observed data range, you would need to:
# 1. Collect new data for the new regions
# 2. Update the model using the new data
# 3. Make predictions for the new regions using the updated model",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:47:31.105558,llama-3-8b-v2,v2,False
extrapolation,warning,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('income_data.csv')

# Create a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()

# Train the model
model.fit(df[['income'], df['inequality'])

# Predict income inequality for a new data point
new_income = 1000000  # outside observed data range
predicted_inequality = model.predict([[new_income]])

print(predicted_inequality)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Load data
df = pd.read_csv('income_data.csv')

# Create a linear regression model
model = LinearRegression()

# Train the model
model.fit(df[['income']], df['inequality'])

# Predict income inequality for a new data point within the observed range
new_income = 80000  # within observed data range
predicted_inequality = model.predict([[new_income])

print(predicted_inequality)

# If you want to extrapolate, consider using a different model or method
# such as a non-linear regression or machine learning algorithm
# that can handle extrapolation, or use a different approach
# such as using a range of possible values for the new data point",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:48:28.261435,llama-3-8b-v2,v2,False
extrapolation,warning,education,student performance prediction,python,intermediate,"import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Load the data
df = pd.read_csv('online_learning.csv')

# Define the features and target variable
X = df[['online_hours', 'instructor_rating']]
y = df['grade']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Now, let's make predictions for a student who has 5 online hours and an instructor rating of 4.5
new_student = np.array([[5, 4.5]])
new_student_pred = model.predict(new_student)
print(new_student_pred)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Load the data
df = pd.read_csv('online_learning.csv')

# Define the features and target variable
X = df[['online_hours', 'instructor_rating']]
y = df['grade']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Now, let's make predictions for a student who has 5 online hours and an instructor rating of 4.5
new_student = np.array([[5, 4.5]])

# Check if the new student's data is within the observed range
if np.all(np.isclose(X_train.min(axis=0), new_student) or np.isclose(X_train.max(axis=0), new_student)):
    new_student_pred = model.predict(new_student)
    print(new_student_pred)
else:
    print(""Prediction cannot be made as the new student's data is outside the observed range."")",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:49:40.850376,llama-3-8b-v2,v2,False
extrapolation,warning,healthcare,observational study of patient outcomes,python,intermediate,"import numpy as np
import pandas as pd

# Load the data
df = pd.read_csv('patient_outcomes.csv')

# Define the independent variable (age) and dependent variable (outcomes)
X = df['age']
y = df['outcomes']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Predict outcomes for patients aged 40, 50, and 60
new_patient_ages = np.array([40, 50, 60, 70, 80])  # Error! Predicting for age 70 and 80 which is outside the observed range
new_patient_outcomes = model.predict(new_patient_ages)
print(new_patient_outcomes)","{'summary': 'Extrapolation Beyond Data Range', 'detailed_explanation': 'Extrapolation Beyond Data Range: Model validity is uncertain beyond training data range', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import pandas as pd

# Load the data
df = pd.read_csv('patient_outcomes.csv')

# Define the independent variable (age) and dependent variable (outcomes)
X = df['age']
y = df['outcomes']

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Predict outcomes for patients aged 40, 50, and 60
new_patient_ages = np.array([40, 50, 60])  # Only predict within the observed range
new_patient_outcomes = model.predict(new_patient_ages)
print(new_patient_outcomes)

# If you still want to make predictions beyond the observed range, consider using techniques like imputation or interpolation
# or collecting more data to include the extrapolated range.",['Model validity is uncertain beyond training data range'],Model validity is uncertain beyond training data range,2025-12-16T10:50:51.364900,llama-3-8b-v2,v2,False
assumption_violation,warning,education,student performance prediction,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('teaching_method_data.csv')

# Convert categorical variable to numerical
data['teaching_method'] = np.where(data['teaching_method'] == 'traditional', 0, 1)

# Perform t-test
t_stat, p_val = ttest_ind(data['score'], data['teaching_method'])

print(f""t-statistic: {t_stat}, p-value: {p_val}"")","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from scipy.stats import shapiro
from statsmodels.stats.weightstats import DescrStatIndep

# Load data
data = pd.read_csv('teaching_method_data.csv')

# Convert categorical variable to numerical
data['teaching_method'] = np.where(data['teaching_method'] == 'traditional', 0, 1)

# Check for normality
normality_results = []
for col in ['score']:
    stat, p_val = shapiro(data[col])
    normality_results.append((col, stat, p_val))
print(normality_results)

# Check for independence
independence_results = DescrStatIndep(data[['score', 'teaching_method']])
print(independence_results.summary())

# Perform non-parametric test (e.g., Wilcoxon rank-sum test) to account for non-normality and non-independence
from scipy.stats import ranksums
t_stat, p_val = ranksums(data['score'][data['teaching_method'] == 0], data['score'][data['teaching_method'] == 1])
print(f""Wilcoxon rank-sum test statistic: {t_stat}, p-value: {p_val}"")",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T10:52:00.794239,llama-3-8b-v2,v2,False
assumption_violation,warning,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import scipy.stats as stats

# Load the data
df = pd.read_csv('disease_prevalence.csv')

# Calculate the mean and standard deviation of disease prevalence across demographics
mean_prevalence = df['prevalence'].mean()
std_prevalence = df['prevalence'].std()

# Check if the data follows a normal distribution
print(stats.normaltest(df['prevalence']))

# Perform a t-test to compare disease prevalence between two groups
t_stat, p_val = stats.ttest_ind(df[df['demographics'] == 'group1']['prevalence'], df[df['demographics'] == 'group2']['prevalence'])
print(f't-statistic: {t_stat}, p-value: {p_val}')","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import scipy.stats as stats
import scipy.spatial.distance as distance
from scipy.cluster.hierarchy import fcluster, linkage
from scipy.cluster.hierarchy import dendrogram

# Load the data
df = pd.read_csv('disease_prevalence.csv')

# Check for normality and transform the data if necessary
from scipy.stats import boxcox
transformed_data, lambda_val = boxcox(df['prevalence'])
df['transformed_prevalence'] = transformed_data

# Calculate the distance matrix using the transformed data
distance_matrix = distance.pdist(df[['transformed_prevalence', 'other_features']])
Z = linkage(distance_matrix, method='ward')

# Cluster the data using hierarchical clustering
n_clusters = 5
cluster_assignments = fcluster(Z, n_clusters, criterion='maxclust')

# Perform a non-parametric test (e.g., Wilcoxon rank-sum test) to compare disease prevalence between clusters
from scipy.stats import wilcoxon_rank_sum_test
wilcoxon_stat, wilcoxon_p_val = wilcoxon_rank_sum_test(df[df['cluster'] == 1]['transformed_prevalence'], df[df['cluster'] == 2]['transformed_prevalence'])
print(f'Wilcoxon statistic: {wilcoxon_stat}, p-value: {wilcoxon_p_val}')",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T10:53:18.684662,llama-3-8b-v2,v2,False
assumption_violation,warning,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('data.csv')

# Define the treatment and control groups
treatment = df[df['method'] == 'treatment']
control = df[df['method'] == 'control']

# Perform the t-test
t_stat, p_val = ttest_ind(treatment['score'], control['score'])

print(f""t-statistic: {t_stat:.3f}"")
print(f""p-value: {p_val:.4f}"")","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import wilcoxon_rank_sum_test

# Load the data
df = pd.read_csv('data.csv')

# Define the treatment and control groups
treatment = df[df['method'] == 'treatment']
control = df[df['method'] == 'control']

# Check for normality using the Shapiro-Wilk test
from scipy.stats import shapiro
shapiro_results = shapiro(treatment['score'])
shapiro_results_control = shapiro(control['score'])

# If the p-values are below a certain threshold (e.g. 0.05), reject the null hypothesis of normality
if shapiro_results[1] < 0.05 or shapiro_results_control[1] < 0.05:
    print(""Normality assumption violated. Alternative methods should be used."")
    # Perform the Wilcoxon rank-sum test as an alternative
    from scipy.stats import ranksums
    stat, p_val = ranksums(treatment['score'], control['score'])
    print(f""Wilcoxon rank-sum test statistic: {stat:.3f}"")
    print(f""p-value: {p_val:.4f}"")
else:
    # If the normality assumption is met, perform the t-test
    t_stat, p_val = ttest_ind(treatment['score'], control['score'])
    print(f""t-statistic: {t_stat:.3f}"")
    print(f""p-value: {p_val:.4f}"")",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T10:54:31.193085,llama-3-8b-v2,v2,False
assumption_violation,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load crime rate data
crime_data = pd.read_csv('crime_rate_data.csv')

# Extract relevant columns
X = crime_data[['police_presence', 'education_level', 'unemployment_rate']]
y = crime_data['crime_rate']

# Check for normality
print(""Normality assumption check:"")
print(np.mean(y))  # mean is not zero, but that's okay...
print(np.std(y))  # standard deviation is not constant, but who cares?

# Check for independence
print(""Independence assumption check:"")
print(np.corrcoef(X))  # correlation matrix, but we're not checking for independence

# Perform t-test
t_stat, p_val = ttest_ind(y, np.mean(y), equal_var=False)
print(""t-statistic:"", t_stat)
print(""p-value:"", p_val)","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, shapiro, fligner
from statsmodels.formula.api import ols

# Load crime rate data
crime_data = pd.read_csv('crime_rate_data.csv')

# Extract relevant columns
X = crime_data[['police_presence', 'education_level', 'unemployment_rate']]
y = crime_data['crime_rate']

# Check for normality
print(""Normality assumption check:"")
shapiro_test = shapiro(y)
print(""Shapiro-Wilk test statistic:"", shapiro_test.statistic)
print(""p-value:"", shapiro_test.pvalue)

# Check for independence
print(""Independence assumption check:"")
corr_matrix = np.corrcoef(X)
print(""Correlation matrix:"")
print(corr_matrix)

# Check for homoscedasticity (equal variances)
print(""Homoscedasticity assumption check:"")
fligner_test = fligner(y, np.arange(len(y)))
print(""F-test statistic:"", fligner_test.statistic)
print(""p-value:"", fligner_test.pvalue)

# If assumptions are met, perform t-test
if shapiro_test.pvalue > 0.05 and fligner_test.pvalue > 0.05:
    t_stat, p_val = ttest_ind(y, np.mean(y), equal_var=False)
    print(""t-statistic:"", t_stat)
    print(""p-value:"", p_val)
else:
    print(""Assumptions not met. Cannot perform t-test."")",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T10:55:53.005880,llama-3-8b-v2,v2,False
assumption_violation,warning,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('online_learning_outcomes.csv')

# Assume normality of data
from scipy.stats import shapiro
normality_test = shapiro(df['score'])
print(""Normality Test Statistic:"", normality_test.statistic)
print(""p-value:"", normality_test.pvalue)

# Assume independence of data
from scipy.stats import independence_test
independence_test = independence_test(df['learning_method'], df['score'])
print(""Independence Test Statistic:"", independence_test.statistic)
print(""p-value:"", independence_test.pvalue)

# Perform t-test
t_stat, p_value = ttest_ind(df.loc[df['learning_method'] == 'online','score'], 
                             df.loc[df['learning_method'] == 'in-person','score'])
print(""t-statistic:"", t_stat)
print(""p-value:"", p_value)","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import robust
from scipy.stats import wilcoxon

# Load data
df = pd.read_csv('online_learning_outcomes.csv')

# Perform non-parametric tests
wilcoxon_test = wilcoxon(df.loc[df['learning_method'] == 'online','score'], 
                         df.loc[df['learning_method'] == 'in-person','score'])
print(""Wilcoxon Rank Sum Test Statistic:"", wilcoxon_test.statistic)
print(""p-value:"", wilcoxon_test.pvalue)

# Alternatively, use a robust test (e.g., Wilcoxon-Mann-Whitney U test)
robust_test = robust.wilcoxon(df.loc[df['learning_method'] == 'online','score'], 
                             df.loc[df['learning_method'] == 'in-person','score'])
print(""Robust Test Statistic:"", robust_test.statistic)
print(""p-value:"", robust_test.pvalue)",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T10:57:02.722585,llama-3-8b-v2,v2,False
assumption_violation,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load dataset
df = pd.read_csv('hospital_data.csv')

# Define treatment and outcome variables
treatment = df['treatment']
outcome = df['outcome']

# Check if treatment and outcome variables are normally distributed
from scipy.stats import shapiro
shapiro_result = shapiro(treatment)
print(shapiro_result)

# Perform independent two-sample t-test
t_stat, p_val = ttest_ind(treatment, outcome, equal_var=False)
print(f""t-statistic: {t_stat}, p-value: {p_val}"")","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import mannwhitneyu

# Load dataset
df = pd.read_csv('hospital_data.csv')

# Define treatment and outcome variables
treatment = df['treatment']
outcome = df['outcome']

# Check if treatment and outcome variables are normally distributed
from scipy.stats import shapiro
shapiro_result = shapiro(treatment)
print(shapiro_result)

# Perform non-parametric Wilcoxon rank-sum test
u_stat, p_val = mannwhitneyu(treatment, outcome)
print(f""U-statistic: {u_stat}, p-value: {p_val}"")",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T10:58:02.388360,llama-3-8b-v2,v2,False
assumption_violation,warning,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('education_data.csv')

# Convert categorical variables to numerical variables
df['teaching_method'] = pd.get_dummies(df['teaching_method'])

# Perform t-test to compare the mean scores of two teaching methods
t_stat, p_val = ttest_ind(df['scores'] > 0, df['scores'] < 0, equal_var=False)

print(f""t-statistic: {t_stat:.3f}, p-value: {p_val:.4f}"")","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import wilcoxon_rank_sum_test
from scipy.stats import pearsonr

# Load the data
df = pd.read_csv('education_data.csv')

# Convert categorical variables to numerical variables
df['teaching_method'] = pd.get_dummies(df['teaching_method'])

# Perform Wilcoxon rank-sum test to compare the scores of two teaching methods
wilcoxon_stat, wilcoxon_p_val = wilcoxon_rank_sum_test(df.loc[df['teaching_method'] == 'Method A','scores'], df.loc[df['teaching_method'] == 'Method B','scores'])

# Perform Pearson correlation test to check for independence
correlation_coeff, p_val = pearsonr(df['scores'], df['scores'])

print(f""Wilcoxon statistic: {wilcoxon_stat:.3f}, p-value: {wilcoxon_p_val:.4f}"")
print(f""Pearson correlation coefficient: {correlation_coeff:.3f}, p-value: {p_val:.4f}"")",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T10:59:09.016438,llama-3-8b-v2,v2,False
assumption_violation,warning,healthcare,observational study of patient outcomes,python,intermediate,"import numpy as np
import pandas as pd
from scipy.stats import ttest_ind

# Load the dataset
df = pd.read_csv('data.csv')

# Extract the treatment and response variables
X = df['treatment']
y = df['response']

# Perform a t-test to compare the means of the response variable between treatment groups
t_stat, p_val = ttest_ind(y[X == 0], y[X == 1])

print(""T-statistic:"", t_stat)
print(""p-value:"", p_val)","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import numpy as np
import pandas as pd
from scipy.stats import mannwhitneyu

# Load the dataset
df = pd.read_csv('data.csv')

# Extract the treatment and response variables
X = df['treatment']
y = df['response']

# Perform a Wilcoxon rank-sum test to compare the distributions of the response variable between treatment groups
stat, p_val = mannwhitneyu(y[X == 0], y[X == 1])

print(""Mann-Whitney U-statistic:"", stat)
print(""p-value:"", p_val)",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:00:04.716776,llama-3-8b-v2,v2,False
assumption_violation,warning,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
from sklearn.linear_model import LinearRegression
from scipy.stats import shapiro

# Load dataset
df = pd.read_csv('student_performance.csv')

# Fit linear regression model
model = LinearRegression()
X = df.drop(['GPA'], axis=1)  # predictor variables
y = df['GPA']  # response variable
model.fit(X, y)

# Check normality of residuals
W, p = shapiro(model.residues_)
print(f""W-statistic: {W}, p-value: {p}"")

# Perform inference (t-test) on the model
t_stat, p_val = model.t_test(X, y)
print(f""t-statistic: {t_stat}, p-value: {p_val}"")","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
from sklearn.linear_model import LinearRegression
from scipy.stats import norm
from statsmodels.graphics.gofm import ProbPlot
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('student_performance.csv')

# Fit linear regression model
model = LinearRegression()
X = df.drop(['GPA'], axis=1)  # predictor variables
y = df['GPA']  # response variable
model.fit(X, y)

# Check normality of residuals
residuals = model.residues_
q = norm.ppf((1 + np.arange(2) / 2) / 2)
plt.plot(q, np.sort(residuals), 'bo')
plt.xlabel('Theoretical quantiles')
plt.ylabel('Ordered residuals')
plt.show()

# Check independence of residuals
from statsmodels.graphics.gofm import DurbinWatson
dw = DurbinWatson(model.residues_)
print(f""Durbin-Watson statistic: {dw}"")

# Perform inference (t-test) on the model
from statsmodels.stats.anova import anova_lm
anova_table = anova_lm(model, model_data=X, model_vars='GPA')
print(anova_table)",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:01:25.612527,llama-3-8b-v2,v2,False
assumption_violation,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('voting_data.csv')

# Convert categorical variables to numerical
data['party'] = np.where(data['party'] == 'Democrat', 0, 1)

# Perform t-test to compare mean vote share between parties
t_stat, p_val = ttest_ind(data['vote_share'], data['party'], equal_var=False)

print(f""t-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import wilcoxon_rank_sum_test

# Load data
data = pd.read_csv('voting_data.csv')

# Convert categorical variables to numerical
data['party'] = np.where(data['party'] == 'Democrat', 0, 1)

# Perform Wilcoxon rank-sum test to compare median vote share between parties
stat, p_val = wilcoxon_rank_sum_test(data['vote_share'], data['party'])

print(f""Wilcoxon rank-sum test statistic: {stat:.2f}, p-value: {p_val:.4f}"")",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:02:19.459823,llama-3-8b-v2,v2,False
assumption_violation,warning,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('pricing_data.csv')

# Define the variables of interest
x = data['price']
y = data['sales']

# Perform t-test assuming normality and independence
t_stat, p_val = ttest_ind(x, y)

print(f'T-statistic: {t_stat}, P-value: {p_val}')","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind_from_stats
from scipy.stats import shapiro
from scipy.stats import pearsonr

# Load data
data = pd.read_csv('pricing_data.csv')

# Define the variables of interest
x = data['price']
y = data['sales']

# Check for normality using Shapiro-Wilk test
normality_x = shapiro(x)
normality_y = shapiro(y)

# Check for independence using Pearson's r
correlation, p_value = pearsonr(x, y)

# Perform t-test using Welch's t-test (robust to unequal variances)
t_stat, p_val = ttest_ind_from_stats(np.mean(x), np.std(x), np.mean(y), np.std(y), len(x), len(y))

print(f'T-statistic: {t_stat}, P-value: {p_val}')",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:03:20.559176,llama-3-8b-v2,v2,False
assumption_violation,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load dataset
df = pd.read_csv('healthcare_data.csv')

# Calculate disease prevalence by demographics
demographics = ['age','sex', 'race']
prevalence = df.groupby(demographics).size() / df.shape[0]

# Perform t-test to compare disease prevalence between age groups
t_stat, p_val = ttest_ind(prevalence['age_18-25'], prevalence['age_26-35'], equal_var=False)

print(f't-statistic: {t_stat}, p-value: {p_val}')","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import wilcoxon_test

# Load dataset
df = pd.read_csv('healthcare_data.csv')

# Calculate disease prevalence by demographics
demographics = ['age','sex', 'race']
prevalence = df.groupby(demographics).size() / df.shape[0]

# Perform Wilcoxon rank-sum test to compare disease prevalence between age groups
stat, p_val = wilcoxon_test(prevalence['age_18-25'], prevalence['age_26-35'])

print(f'Wilcoxon statistic: {stat}, p-value: {p_val}')",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:04:21.184287,llama-3-8b-v2,v2,False
assumption_violation,warning,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('voting_data.csv')

# Define the variables of interest
x = df['age']
y = df['voted']

# Check for normality of the variables
from scipy.stats import shapiro
print(shapiro(x))
print(shapiro(y))

# Check for independence of the variables
from scipy.stats import pearsonr
print(pearsonr(x, y))

# Perform a t-test to compare the mean age of voters and non-voters
t_stat, p_val = ttest_ind(x[y==1], x[y==0])
print('t-statistic:', t_stat)
print('p-value:', p_val)","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import mannwhitneyu
from scipy.stats import spearmanr

# Load the data
df = pd.read_csv('voting_data.csv')

# Define the variables of interest
x = df['age']
y = df['voted']

# Check for normality of the variables
from scipy.stats import shapiro
print(shapiro(x))
print(shapiro(y))

# Check for independence of the variables
from scipy.stats import pearsonr
print(pearsonr(x, y))

# Perform a non-parametric test to compare the median age of voters and non-voters
stat, p_val = mannwhitneyu(x[y==1], x[y==0])
print('Mann-Whitney U-statistic:', stat)
print('p-value:', p_val)

# Perform a non-parametric correlation analysis to assess the relationship between 'age' and 'voted'
rho, p_val = spearmanr(x, y)
print('Spearman rank correlation coefficient:', rho)
print('p-value:', p_val)",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:05:29.638329,llama-3-8b-v2,v2,False
assumption_violation,warning,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load dataset
df = pd.read_csv('disease_data.csv')

# Calculate disease prevalence by demographic group
group1 = df[df['demographic'] == 'Group1']
group2 = df[df['demographic'] == 'Group2']

# Perform t-test to compare disease prevalence between groups
t_stat, p_val = ttest_ind(group1['disease'], group2['disease'])

print(f""t-statistic: {t_stat}, p-value: {p_val}"")","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency

# Load dataset
df = pd.read_csv('disease_data.csv')

# Calculate disease prevalence by demographic group
group1 = df[df['demographic'] == 'Group1']
group2 = df[df['demographic'] == 'Group2']

# Convert disease prevalence to contingency table
table = pd.crosstab(group1['demographic'], group1['disease'])

# Perform chi-squared test to compare disease prevalence between groups
chi2_stat, p_val, dof, expected = chi2_contingency(table)

print(f""Chi-squared statistic: {chi2_stat}, p-value: {p_val}"")",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:06:33.392213,llama-3-8b-v2,v2,False
assumption_violation,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import scipy.stats as stats

# Load the dataset
df = pd.read_csv('voting_data.csv')

# Perform ANOVA to compare the mean voting behavior between two groups
from statsmodels.formula.api import ols
model = ols('voting_behavior ~ C(group)', data=df).fit()
print(model.f_test())

# Perform a t-test to compare the mean voting behavior between two groups
t_stat, p_val = stats.ttest_ind(df.loc[df['group'] == 'A', 'voting_behavior'], 
                                df.loc[df['group'] == 'B', 'voting_behavior'])
print(t_stat, p_val)","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import scipy.stats as stats
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm

# Load the dataset
df = pd.read_csv('voting_data.csv')

# Check for normality using Shapiro-Wilk test
from scipy.stats import shapiro
stat, p_val = shapiro(df['voting_behavior'])
print(f""Shapiro-Wilk test: stat={stat}, p-value={p_val}"")

# Check for independence using Durbin-Watson test
from statsmodels.stats.stattools import durbin_watson
dw_stat = durbin_watson(df['voting_behavior'])
print(f""Durbin-Watson test: stat={dw_stat}"")

# Perform ANOVA after transforming the data using log transformation
df['voting_behavior_log'] = np.log(df['voting_behavior'])
model = ols('voting_behavior_log ~ C(group)', data=df).fit()
print(anova_lm(model))

# Perform a non-parametric test, such as the Wilcoxon rank-sum test, to compare the median voting behavior between two groups
from scipy.stats import wilcoxon_rank_sum_test
stat, p_val = wilcoxon_rank_sum_test(df.loc[df['group'] == 'A', 'voting_behavior'], 
                                    df.loc[df['group'] == 'B', 'voting_behavior'])
print(stat, p_val)",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:07:53.867473,llama-3-8b-v2,v2,False
assumption_violation,warning,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('crime_data.csv')

# Define independent and dependent variables
X = data['urban_population']
y = data['crime_rate']

# Perform t-test assuming normality and independence
t_stat, p_val = ttest_ind(X, y)

print(f'T-statistic: {t_stat}, P-value: {p_val}')","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import spearmanr

# Load data
data = pd.read_csv('crime_data.csv')

# Define independent and dependent variables
X = data['urban_population']
y = data['crime_rate']

# Perform non-parametric correlation analysis
rho, p_val = spearmanr(X, y)

print(f'Rho: {rho}, P-value: {p_val}')",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:08:36.682180,llama-3-8b-v2,v2,False
assumption_violation,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('healthcare_data.csv')

# Extract relevant columns
demographics = ['age','sex', 'race']
disease_status = 'disease'

# Perform t-test to compare disease prevalence across demographics
t_stat, p_val = ttest_ind(df[demographics], df[disease_status])

print(f""T-statistic: {t_stat}, p-value: {p_val}"")","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency

# Load the data
df = pd.read_csv('healthcare_data.csv')

# Extract relevant columns
demographics = ['age','sex', 'race']
disease_status = 'disease'

# Perform chi-squared test to compare disease prevalence across demographics
contingency_table = pd.crosstab(df[demographics], df[disease_status])
chi2_stat, p_val, dof, expected = chi2_contingency(contingency_table)

print(f""Chi-squared statistic: {chi2_stat}, p-value: {p_val}"")",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:09:29.838271,llama-3-8b-v2,v2,False
assumption_violation,warning,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the data
data = pd.read_csv('online_vs_inperson.csv')

# Split the data into online and in-person groups
online = data[data['mode'] == 'online']
inperson = data[data['mode'] == 'inperson']

# Perform a t-test to compare the means
t_stat, p_val = ttest_ind(online['score'], inperson['score'])

print(""t-statistic:"", t_stat)
print(""p-value:"", p_val)","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import wilcoxon_rank_sum_test

# Load the data
data = pd.read_csv('online_vs_inperson.csv')

# Split the data into online and in-person groups
online = data[data['mode'] == 'online']
inperson = data[data['mode'] == 'inperson']

# Perform a Wilcoxon rank-sum test to compare the scores
stat, p_val = wilcoxon_rank_sum_test(online['score'], inperson['score'])

print(""Wilcoxon statistic:"", stat)
print(""p-value:"", p_val)",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:10:24.772436,llama-3-8b-v2,v2,False
assumption_violation,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load dataset
df = pd.read_csv('hospital_data.csv')

# Perform t-test to compare mean patient outcomes between treatment and control groups
t_stat, p_val = ttest_ind(df['treatment_group'], df['control_group'])

print(f""T-statistic: {t_stat}, p-value: {p_val}"")","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import f_oneway
from sklearn.linear_model import LinearRegression
from statsmodels.formula.api import ols

# Load dataset
df = pd.read_csv('hospital_data.csv')

# Check for normality and independence
from scipy.stats import shapiro
from statsmodels.graphics.gofm import ProbPlot

shapiro_result = shapiro(df['treatment_group'])
print(f""Shapiro-Wilk test for normality: {shapiro_result.statistic}, p-value: {shapiro_result.pvalue}"")

# Perform ANOVA to account for correlations within each group
anova_model = ols('outcome ~ C(treatment_group)', data=df).fit()
print(f""F-statistic: {anova_model.fvalue}, p-value: {anova_model.f_pvalue}"")

# Perform linear regression to account for correlations within each group
X = pd.get_dummies(df['treatment_group'], drop_first=True)
y = df['outcome']
model = LinearRegression()
model.fit(X, y)
print(f""R-squared: {model.score(X, y)}"")",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:11:26.722446,llama-3-8b-v2,v2,False
assumption_violation,warning,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import scipy.stats as stats

# Load data
df = pd.read_csv('data.csv')

# Calculate mean disease prevalence across demographics
mean_prevalence = df.groupby('demographic')['disease'].mean()
print(mean_prevalence)

# Perform ANOVA to compare mean prevalence across demographics
from scipy.stats import f_oneway
F, p = f_oneway(*[group['disease'] for group in df.groupby('demographic')])
print(F, p)

# Create a confidence interval for the mean prevalence
CI = stats.t.interval(0.95, df.shape[0]-1, loc=mean_prevalence, scale=mean_prevalence.std())
print(CI)","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import scipy.stats as stats
from scipy.stats import robust

# Load data
df = pd.read_csv('data.csv')

# Calculate robust mean disease prevalence across demographics
robust_mean = df.groupby('demographic')['disease'].apply(lambda x: x.describe().loc['median'])
print(robust_mean)

# Perform non-parametric ANOVA (Kruskal-Wallis test) to compare median disease prevalence across demographics
from scipy.stats import kruskal
H, p = kruskal(*[group['disease'] for group in df.groupby('demographic')])
print(H, p)

# Create a confidence interval for the median disease prevalence
CI = stats.mannwhitneyu(df.groupby('demographic')['disease'].values, axis=0, conf_int=0.95)
print(CI)",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:12:29.796384,llama-3-8b-v2,v2,False
assumption_violation,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import scipy.stats as stats

# Load data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Perform regression analysis
X = data[['social_media_usage','sleep_duration']]
y = data['mental_health_score']
slope, intercept, r_value, p_value, std_err = stats.linregress(X, y)

print(f""Slope: {slope}, R-squared: {r_value**2}, p-value: {p_value}"")","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import scipy.stats as stats
import seaborn as sns
from statsmodels.formula.api import ols

# Load data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Check for normality
from scipy.stats import shapiro
shapiro_results = shapiro(data['mental_health_score'])
print(f""Shapiro-Wilk test: {shapiro_results[1]}"")

# Check for independence
from statsmodels.stats.outliers_influence import variance_inflation_factor
vif = variance_inflation_factor(data[['social_media_usage','sleep_duration']], 0)
print(f""VIF: {vif}"")

# Perform robust regression (accounting for non-normality and correlated predictors)
X = pd.get_dummies(data[['social_media_usage','sleep_duration']], drop_first=True)
y = data['mental_health_score']
model = ols('y ~ X', data=data).fit()
print(model.summary())

# Alternatively, use a non-parametric approach (e.g., robust regression with bootstrapping)
from sklearn.linear_model import RANSACRegressor
ranc = RANSACRegressor()
ranc.fit(X, y)
print(ranc.score(X, y))",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:13:44.125604,llama-3-8b-v2,v2,False
assumption_violation,warning,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('marketing_campaign.csv')

# Check for normality
print(""Normality check: "", data['conversion_rate'].kurtosis())

# Check for independence
print(""Independence check: "", np.corrcoef(data['conversion_rate'], data['ad_spend'])[0, 1])

# Perform t-test
t_stat, p_val = ttest_ind(data['conversion_rate'], data['ad_spend'], equal_var=False)
print(""t-statistic: "", t_stat)
print(""p-value: "", p_val)

# Make conclusions based on p-value
if p_val < 0.05:
    print(""Reject null hypothesis, there is a statistically significant relationship between conversion rate and ad spend."")
else:
    print(""Fail to reject null hypothesis, there is no statistically significant relationship between conversion rate and ad spend."")","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind, normality_test
from statsmodels.stats.diagnostic import normal_ad-fuller as normal_adfuller

# Load data
data = pd.read_csv('marketing_campaign.csv')

# Check for normality
normality_test_results = normal_adfuller(data['conversion_rate'])
print(""Normality test results: "", normality_test_results)

# Check for independence
print(""Independence check: "", np.corrcoef(data['conversion_rate'], data['ad_spend'])[0, 1])

# Perform non-parametric test (e.g., Wilcoxon rank-sum test) to account for non-normality and non-independence
from scipy.stats import ranksums
wilcoxon_stat, wilcoxon_p_val = ranksums(data['conversion_rate'], data['ad_spend'])
print(""Wilcoxon rank-sum test statistic: "", wilcoxon_stat)
print(""Wilcoxon rank-sum test p-value: "", wilcoxon_p_val)

# Make conclusions based on p-value
if wilcoxon_p_val < 0.05:
    print(""Reject null hypothesis, there is a statistically significant relationship between conversion rate and ad spend."")
else:
    print(""Fail to reject null hypothesis, there is no statistically significant relationship between conversion rate and ad spend."")",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:15:02.938238,llama-3-8b-v2,v2,False
assumption_violation,warning,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load the data
data = pd.read_csv('online_learning_outcomes.csv')

# Split the data into online and in-person learning groups
online_group = data[data['learning_method'] == 'online']
in_person_group = data[data['learning_method'] == 'in_person']

# Calculate the means and standard deviations for each group
online_mean = np.mean(online_group['score'])
online_std = np.std(online_group['score'])
in_person_mean = np.mean(in_person_group['score'])
in_person_std = np.std(in_person_group['score'])

# Perform a two-sample t-test
t_stat, p_val = ttest_ind(online_group['score'], in_person_group['score'], equal_var=False)

print(f'T-statistic: {t_stat}, p-value: {p_val}')","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind
from scipy.stats import shapiro
from scipy.stats import normaltest

# Load the data
data = pd.read_csv('online_learning_outcomes.csv')

# Split the data into online and in-person learning groups
online_group = data[data['learning_method'] == 'online']
in_person_group = data[data['learning_method'] == 'in_person']

# Check for normality
online_shapiro = shapiro(online_group['score'])
in_person_shapiro = shapiro(in_person_group['score'])
print(f'Online group normality test: {online_shapiro}')
print(f'In-person group normality test: {in_person_shapiro}')

# Check for independence
online_group['score'] = online_group['score'].astype(str) + '_' + online_group['score'].astype(str)
in_person_group['score'] = in_person_group['score'].astype(str) + '_' + in_person_group['score'].astype(str)

# Perform a non-parametric test (Wilcoxon rank-sum test) that does not assume normality or independence
from scipy.stats import wilcoxon_rank_sum_test
wilcoxon_stat, wilcoxon_p = wilcoxon_rank_sum_test(online_group['score'], in_person_group['score'], alternative='two-sided')

print(f'Wilcoxon rank-sum test statistic: {wilcoxon_stat}, p-value: {wilcoxon_p}')",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:16:24.697882,llama-3-8b-v2,v2,False
assumption_violation,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import scipy.stats as stats
from scipy.stats import ttest_ind

# Load dataset
df = pd.read_csv('patient_outcomes.csv')

# Define dependent variable (outcome) and independent variable (treatment)
outcome = df['outcome']
treatment = df['treatment']

# Check normality of outcome variable
from scipy.stats import shapiro
result = shapiro(outcome)
print(result)

# Perform t-test assuming normality
t_stat, p_val = ttest_ind(outcome[treatment == 1], outcome[treatment == 0])
print(f't-statistic: {t_stat}, p-value: {p_val}')","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import scipy.stats as stats
from scipy.stats import wilcoxon_rank_sum_test
from scipy.stats import mannwhitneyu

# Load dataset
df = pd.read_csv('patient_outcomes.csv')

# Define dependent variable (outcome) and independent variable (treatment)
outcome = df['outcome']
treatment = df['treatment']

# Check normality of outcome variable
from scipy.stats import shapiro
result = shapiro(outcome)
print(result)

# Perform non-parametric tests
wilcoxon_stat, wilcoxon_p = wilcoxon_rank_sum_test(outcome[treatment == 1], outcome[treatment == 0])
mannwhitney_stat, mannwhitney_p = mannwhitneyu(outcome[treatment == 1], outcome[treatment == 0])

print(f'Wilcoxon rank-sum test: stat: {wilcoxon_stat}, p-value: {wilcoxon_p}')
print(f'Mann-Whitney U test: stat: {mannwhitney_stat}, p-value: {mannwhitney_p}')",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:17:32.741257,llama-3-8b-v2,v2,False
assumption_violation,warning,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Load the data
df = pd.read_csv('customer_data.csv')

# Define the target variable and features
y = df['churn']
X = df.drop('churn', axis=1)

# Fit a linear regression model
model = LinearRegression()
model.fit(X, y)

# Make predictions
y_pred = model.predict(X)

# Calculate the mean squared error
mse = mean_squared_error(y, y_pred)
print(f""Mean Squared Error: {mse:.2f}"")

# Print the coefficients
print(""Coefficients:"")
print(model.coef_)","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import log_loss
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest

# Load the data
df = pd.read_csv('customer_data.csv')

# Define the target variable and features
y = df['churn']
X = df.drop('churn', axis=1)

# Check for normality and independence
from scipy.stats import shapiro
from statsmodels.stats.outliers_influence import summary_table
shapiro_test = shapiro(y)
print(f""Shapiro-Wilk test: {shapiro_test.statistic:.4f}, p-value: {shapiro_test.pvalue:.4f}"")

# Perform feature selection using SelectKBest
selector = SelectKBest(k=5)
selector.fit(X, y)
X_selected = selector.transform(X)

# Scale the data using StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_selected)

# Fit a logistic regression model
model = LogisticRegression()
model.fit(X_scaled, y)

# Make predictions
y_pred = model.predict(X_scaled)

# Calculate the log loss
log_loss = log_loss(y, y_pred)
print(f""Log Loss: {log_loss:.4f}"")

# Print the coefficients
print(""Coefficients:"")
print(model.coef_)",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:18:46.684820,llama-3-8b-v2,v2,False
assumption_violation,warning,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import scipy.stats as stats

# Load the data
df = pd.read_csv('income_data.csv')

# Perform a t-test to compare the mean income of two groups
t_stat, p_val = stats.ttest_ind(df['income'][df['group'] == 'high'], df['income'][df['group'] == 'low'])

print(f""T-statistic: {t_stat:.3f}, p-value: {p_val:.4f}"")","{'summary': 'Statistical Assumption Violation', 'detailed_explanation': 'Statistical Assumption Violation: Violations of assumptions can invalidate statistical inference', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import scipy.stats as stats
from scipy.stats import normaltest
from statsmodels.stats.anova import AnovaRM

# Load the data
df = pd.read_csv('income_data.csv')

# Check for normality
normality_test = normaltest(df['income'])
print(f""Normality test statistic: {normality_test.statistic:.3f}, p-value: {normality_test.pvalue:.4f}"")

# Check for independence using a repeated-measures ANOVA
anova = AnovaRM(df, 'income', subject='individual').fit()
print(f""F-statistic: {anova.fvalue:.3f}, p-value: {anova.pvalue:.4f}"")

# Perform a non-parametric test (e.g., Wilcoxon rank-sum test) to compare the median income of the two groups
from scipy.stats import wilcoxon_rank_sum_test
wilcoxon_stat, wilcoxon_p_val = wilcoxon_rank_sum_test(df['income'][df['group'] == 'high'], df['income'][df['group'] == 'low'])
print(f""Wilcoxon rank-sum test statistic: {wilcoxon_stat:.3f}, p-value: {wilcoxon_p_val:.4f}"")",['Violations of assumptions can invalidate statistical inference'],Violations of assumptions can invalidate statistical inference,2025-12-16T11:19:52.428975,llama-3-8b-v2,v2,False
truncated_axis,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate sample data for a clinical trial
np.random.seed(0)
drug_a = np.random.normal(10, 2, 100)
drug_b = np.random.normal(12, 2, 100)

# Plot the data
plt.bar(range(100), drug_a, label='Drug A')
plt.bar(range(100), drug_b, label='Drug B')

# Set the y-axis limits to start at 10
plt.ylim(10, 20)

# Add a title and labels
plt.title('Comparison of Drug Efficacy')
plt.xlabel('Patient ID')
plt.ylabel('Effect Size')

# Show the plot
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate sample data for a clinical trial
np.random.seed(0)
drug_a = np.random.normal(10, 2, 100)
drug_b = np.random.normal(12, 2, 100)

# Plot the data
plt.bar(range(100), drug_a, label='Drug A')
plt.bar(range(100), drug_b, label='Drug B')

# Set the y-axis limits to automatically adjust
plt.ylim(np.min(min(drug_a), min(drug_b)), np.max(max(drug_a), max(drug_b)))

# Add a title and labels
plt.title('Comparison of Drug Efficacy')
plt.xlabel('Patient ID')
plt.ylabel('Effect Size')

# Show the plot
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:20:53.357202,llama-3-8b-v2,v2,False
truncated_axis,critical,business,pricing strategy impact analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
x = np.random.randint(1, 10, 10)
y = np.random.randint(1, 10, 10)

# Create the plot
plt.figure(figsize=(8, 6))
plt.plot(x, y)
plt.title('Sales Performance Across Regions')
plt.xlabel('Region')
plt.ylabel('Sales')

# Set the y-axis limits to exaggerate differences
plt.ylim(0, 50)

plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
x = np.random.randint(1, 10, 10)
y = np.random.randint(1, 10, 10)

# Find the minimum and maximum values in the data
min_y = np.min(y)
max_y = np.max(y)

# Create the plot
plt.figure(figsize=(8, 6))
plt.plot(x, y)
plt.title('Sales Performance Across Regions')
plt.xlabel('Region')
plt.ylabel('Sales')

# Set the y-axis limits to start at the minimum value in the data
plt.ylim(min_y, max_y)

plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:21:43.691078,llama-3-8b-v2,v2,False
truncated_axis,critical,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
online_scores = np.random.normal(60, 5, 100)
in_person_scores = np.random.normal(70, 5, 100)

# Create the plot
fig, ax = plt.subplots()
ax.scatter(online_scores, np.zeros(100), label='Online')
ax.scatter(in_person_scores, np.zeros(100), label='In-Person')

# Set the y-axis limits
ax.set_ylim([0, 100])

# Add title and labels
ax.set_title('Online vs In-Person Learning Outcomes')
ax.set_xlabel('Score')
ax.set_ylabel('Learning Format')

# Show the plot
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
online_scores = np.random.normal(60, 5, 100)
in_person_scores = np.random.normal(70, 5, 100)

# Create the plot
fig, ax = plt.subplots()
ax.scatter(online_scores, np.zeros_like(online_scores), label='Online')
ax.scatter(in_person_scores, np.zeros_like(in_person_scores), label='In-Person')

# Set the y-axis limits to start at the minimum value
y_min = min(np.min(online_scores), np.min(in_person_scores))
ax.set_ylim([y_min, max(np.max(online_scores), np.max(in_person_scores))])

# Add title and labels
ax.set_title('Online vs In-Person Learning Outcomes')
ax.set_xlabel('Score')
ax.set_ylabel('Learning Format')

# Show the plot
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:22:44.537872,llama-3-8b-v2,v2,False
truncated_axis,critical,social_science,income inequality trends,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
crime_data = pd.read_csv('crime_rate_data.csv')

# Create a bar plot
sns.set()
sns.barplot(x='variable', y='value', data=crime_data)

# Set the y-axis to start at 5 to exaggerate differences
plt.ylim(5, 10)

# Show the plot
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
crime_data = pd.read_csv('crime_rate_data.csv')

# Create a bar plot
sns.set()
sns.barplot(x='variable', y='value', data=crime_data)

# Set the y-axis to start at zero to show the actual scale
plt.ylim(0, 10)

# Show the plot
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:23:26.076294,llama-3-8b-v2,v2,False
truncated_axis,critical,business,pricing strategy impact analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
x = np.random.rand(10)
y = np.random.rand(10)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the data
ax.plot(x, y)

# Set the y-axis limits to start at 0.5, exaggerating the differences
ax.set_ylim([0.5, 1.5])

# Show the plot
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
x = np.random.rand(10)
y = np.random.rand(10)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the data
ax.plot(x, y)

# Calculate the minimum and maximum values in the data
min_y = np.min(y)
max_y = np.max(y)

# Set the y-axis limits to start at the minimum value
ax.set_ylim([min_y, max_y])

# Show the plot
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:24:13.042940,llama-3-8b-v2,v2,False
truncated_axis,critical,business,customer churn prediction analysis,python,intermediate,"import matplotlib.pyplot as plt

# Sample data
regions = ['North', 'South', 'East', 'West']
sales = [100, 120, 80, 110]

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the data with a flawed y-axis
ax.plot(regions, sales)
ax.set_ylim([50, 150])  # Starting y-axis at non-zero to exaggerate differences

# Add a title and labels
ax.set_title('Sales Performance Across Regions')
ax.set_xlabel('Region')
ax.set_ylabel('Sales')

# Show the plot
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt

# Sample data
regions = ['North', 'South', 'East', 'West']
sales = [100, 120, 80, 110]

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the data with a corrected y-axis
ax.plot(regions, sales)
ax.set_ylim([0, max(sales) + 10])  # Starting y-axis at the minimum value

# Add a title and labels
ax.set_title('Sales Performance Across Regions')
ax.set_xlabel('Region')
ax.set_ylabel('Sales')

# Show the plot
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:25:02.081416,llama-3-8b-v2,v2,False
truncated_axis,critical,social_science,voting behavior analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
votes = np.random.normal(50, 10, 100)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the data with a flawed y-axis
ax.bar(range(len(votes)), votes, align='edge')
ax.set_ylim(40, 60)  # Starting y-axis at 40 to exaggerate differences

# Show the plot
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
votes = np.random.normal(50, 10, 100)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the data with a corrected y-axis
ax.bar(range(len(votes)), votes, align='edge')
ax.set_ylim(0, max(votes) + 10)  # Starting y-axis at 0 to accurately represent the data

# Show the plot
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:25:52.908099,llama-3-8b-v2,v2,False
truncated_axis,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Sample data
demographics = ['Male', 'Female', 'Other']
prevalence = [0.05, 0.03, 0.02]

# Plot the data
plt.bar(demographics, prevalence)

# Set the y-axis limits to start at 0.1 to exaggerate differences
plt.ylim(0.1, 0.15)

plt.xlabel('Demographics')
plt.ylabel('Prevalence')
plt.title('Disease Prevalence Across Demographics')
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Sample data
demographics = ['Male', 'Female', 'Other']
prevalence = [0.05, 0.03, 0.02]

# Plot the data
plt.bar(demographics, prevalence)

# Set the y-axis limits to show the full range of values
plt.ylim(0, max(prevalence) + 0.01)

plt.xlabel('Demographics')
plt.ylabel('Prevalence')
plt.title('Disease Prevalence Across Demographics')
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:26:49.713310,llama-3-8b-v2,v2,False
truncated_axis,critical,social_science,income inequality trends,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
crime_rates = [10, 20, 30, 40, 50]
years = range(1, 6)

# Create the plot
plt.plot(years, crime_rates)

# Set the y-axis to start at 10 to exaggerate differences
plt.ylim(10, 60)

# Add a title and labels
plt.title(""Crime Rates Over Time"")
plt.xlabel(""Year"")
plt.ylabel(""Crime Rate"")

# Show the plot
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
crime_rates = [10, 20, 30, 40, 50]
years = range(1, 6)

# Create the plot
plt.plot(years, crime_rates)

# Set the y-axis to start at zero to show the actual values
plt.ylim(0, 60)

# Add a title and labels
plt.title(""Crime Rates Over Time"")
plt.xlabel(""Year"")
plt.ylabel(""Crime Rate"")

# Show the plot
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:27:42.524327,llama-3-8b-v2,v2,False
truncated_axis,critical,social_science,voting behavior analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Sample data for voting behavior analysis
voting_data = np.random.randint(0, 100, size=(10, 2))  # 10 voters, 2 options

# Create a bar chart to visualize voting behavior
plt.bar(range(len(voting_data)), voting_data[:, 0], color='blue')
plt.bar(range(len(voting_data)), voting_data[:, 1], bottom=voting_data[:, 0], color='red')

# Set the y-axis limits to exaggerate differences
plt.ylim(50, 100)  # Starting y-axis at 50 to exaggerate differences

plt.xlabel('Voter ID')
plt.ylabel('Votes')
plt.title('Voting Behavior Analysis')
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Sample data for voting behavior analysis
voting_data = np.random.randint(0, 100, size=(10, 2))  # 10 voters, 2 options

# Create a bar chart to visualize voting behavior
plt.bar(range(len(voting_data)), voting_data[:, 0], color='blue')
plt.bar(range(len(voting_data)), voting_data[:, 1], bottom=voting_data[:, 0], color='red')

# Set the y-axis limits to the actual minimum and maximum values
y_min = np.min(voting_data.min(axis=0))
y_max = np.max(voting_data.max(axis=0))
plt.ylim(y_min, y_max)  # Set y-axis limits to actual values

plt.xlabel('Voter ID')
plt.ylabel('Votes')
plt.title('Voting Behavior Analysis')
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:28:44.238464,llama-3-8b-v2,v2,False
truncated_axis,critical,social_science,crime rate factors,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y = np.random.normal(0, 1, 100)

# Create the plot
plt.scatter(x, y)

# Set the y-axis limits to exaggerate differences
plt.ylim(0, 2)  # <--- Error: Starting y-axis at non-zero to exaggerate differences

# Add a title and labels
plt.title(""Voting Behavior Analysis"")
plt.xlabel(""X-axis"")
plt.ylabel(""Y-axis"")

# Show the plot
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y = np.random.normal(0, 1, 100)

# Create the plot
plt.scatter(x, y)

# Set the y-axis limits to start at the minimum value of the data
y_min = np.min(y)
y_max = np.max(y)
plt.ylim(y_min - 0.1, y_max + 0.1)  # Corrected: Start y-axis at the minimum value of the data

# Add a title and labels
plt.title(""Voting Behavior Analysis"")
plt.xlabel(""X-axis"")
plt.ylabel(""Y-axis"")

# Show the plot
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:29:44.841101,llama-3-8b-v2,v2,False
truncated_axis,critical,business,marketing campaign effectiveness study,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
x = np.random.rand(10)
y = np.random.rand(10)

# Plot the data
plt.plot(x, y)

# Set the y-axis limits to start at 0.5 to exaggerate differences
plt.ylim(0.5, 1.5)

# Show the plot
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
x = np.random.rand(10)
y = np.random.rand(10)

# Plot the data
plt.plot(x, y)

# Set the y-axis limits to automatically adjust to the data range
plt.ylim(0, 1)

# Show the plot
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:30:24.844975,llama-3-8b-v2,v2,False
truncated_axis,critical,business,pricing strategy impact analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
np.random.seed(0)
x = np.random.rand(100)
y = np.random.rand(100)

# Plot the data with a flawed y-axis
plt.plot(x, y)
plt.ylim(0.5, 1.5)  # Set the y-axis limits
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('Flawed Plot')
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
np.random.seed(0)
x = np.random.rand(100)
y = np.random.rand(100)

# Plot the data with a corrected y-axis
plt.plot(x, y)
plt.ylim(0, 1)  # Set the y-axis limits to start at 0
plt.xlabel('X Axis')
plt.ylabel('Y Axis')
plt.title('Corrected Plot')
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:31:14.531504,llama-3-8b-v2,v2,False
truncated_axis,critical,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Sample data for online and in-person learning outcomes
online_scores = np.random.normal(60, 5, 100)
in_person_scores = np.random.normal(65, 5, 100)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the data with a flawed y-axis range
ax.plot(online_scores, label='Online')
ax.plot(in_person_scores, label='In-person')
ax.set_xlabel('Student ID')
ax.set_ylabel('Learning Outcomes')
ax.set_ylim(50, 80)  # Start y-axis at 50, exaggerating differences

# Add a legend and show the plot
ax.legend()
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Sample data for online and in-person learning outcomes
online_scores = np.random.normal(60, 5, 100)
in_person_scores = np.random.normal(65, 5, 100)

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the data with a corrected y-axis range
ax.plot(online_scores, label='Online')
ax.plot(in_person_scores, label='In-person')
ax.set_xlabel('Student ID')
ax.set_ylabel('Learning Outcomes')
ax.set_ylim(min(min(online_scores), min(in_person_scores)) - 10, max(max(online_scores), max(in_person_scores)) + 10)  # Start y-axis at the minimum score minus 10 and extend to the maximum score plus 10

# Add a legend and show the plot
ax.legend()
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:32:26.254128,llama-3-8b-v2,v2,False
truncated_axis,critical,social_science,income inequality trends,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
crime_rates = np.random.normal(loc=10, scale=2, size=10)
years = np.arange(1, 11)

# Plot the data with a flawed y-axis range
plt.plot(years, crime_rates)
plt.ylim(5, 15)  # Start y-axis at 5 to exaggerate differences
plt.xlabel('Year')
plt.ylabel('Crime Rate')
plt.title('Crime Rate Over Time')
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
crime_rates = np.random.normal(loc=10, scale=2, size=10)
years = np.arange(1, 11)

# Plot the data with a corrected y-axis range
plt.plot(years, crime_rates)
plt.ylim(0, 20)  # Start y-axis at 0 to accurately show the data
plt.xlabel('Year')
plt.ylabel('Crime Rate')
plt.title('Crime Rate Over Time')
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:33:15.768091,llama-3-8b-v2,v2,False
truncated_axis,critical,healthcare,observational study of patient outcomes,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
ages = np.random.randint(18, 80, 100)
diseases = np.random.randint(0, 2, 100)  # 0 = healthy, 1 = diseased
demographics = ['Male', 'Female', 'Other']
prevalence = np.random.randint(0, 100, (100, 3))  # prevalence rate for each age group

# Plot the data with a flawed y-axis
fig, ax = plt.subplots()
for i, demo in enumerate(demographics):
    ax.plot(ages, prevalence[:, i], label=demo)
ax.set_xlabel('Age')
ax.set_ylabel('Prevalence Rate')
ax.set_title('Disease Prevalence across Demographics')
ax.set_ylim([5, 100])  # Starting y-axis at 5 to exaggerate differences
plt.legend()
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
ages = np.random.randint(18, 80, 100)
diseases = np.random.randint(0, 2, 100)  # 0 = healthy, 1 = diseased
demographics = ['Male', 'Female', 'Other']
prevalence = np.random.randint(0, 100, (100, 3))  # prevalence rate for each age group

# Plot the data with a corrected y-axis
fig, ax = plt.subplots()
for i, demo in enumerate(demographics):
    ax.plot(ages, prevalence[:, i], label=demo)
ax.set_xlabel('Age')
ax.set_ylabel('Prevalence Rate')
ax.set_title('Disease Prevalence across Demographics')
ax.set_ylim([0, 100])  # Starting y-axis at 0 to reflect the actual data
plt.legend()
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:34:30.164448,llama-3-8b-v2,v2,False
truncated_axis,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Sample data
x = np.arange(10)
y = np.random.normal(0, 1, 10)

# Plotting the data
fig, ax = plt.subplots()
ax.plot(x, y)

# Set the y-axis to start at 5 to exaggerate the differences
ax.set_ylim(5, 10)

plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Sample data
x = np.arange(10)
y = np.random.normal(0, 1, 10)

# Plotting the data
fig, ax = plt.subplots()
ax.plot(x, y)

# Set the y-axis to automatically scale based on the data
ax.set_ylim(min(y) - 1, max(y) + 1)

plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:35:10.363854,llama-3-8b-v2,v2,False
truncated_axis,critical,social_science,social media usage and mental health,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
income_data = np.random.normal(loc=50000, scale=10000, size=100)

# Plot the data with a flawed y-axis
plt.plot(income_data)
plt.axhline(y=50000, color='r', linestyle='--')  # Starting y-axis at 50000
plt.xlabel('Individuals')
plt.ylabel('Income (USD)')
plt.title('Income Inequality Trends')
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
income_data = np.random.normal(loc=50000, scale=10000, size=100)

# Plot the data with a corrected y-axis
plt.plot(income_data)
plt.xlabel('Individuals')
plt.ylabel('Income (USD)')
plt.title('Income Inequality Trends')

# Set the y-axis limits to automatically adjust
plt.ylim([np.min(income_data), np.max(income_data)])
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:36:00.280639,llama-3-8b-v2,v2,False
truncated_axis,critical,social_science,income inequality trends,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some data
np.random.seed(0)
x = np.arange(10)
y = np.random.normal(0, 1, 10)

# Plot the data with a flawed y-axis
plt.plot(x, y)
plt.xlabel('Time')
plt.ylabel('Mental Health Score')
plt.title('Social Media Usage and Mental Health')
plt.gca().set_ylim([1, 5])  # Start the y-axis at 1 to exaggerate differences
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some data
np.random.seed(0)
x = np.arange(10)
y = np.random.normal(0, 1, 10)

# Plot the data with a corrected y-axis
plt.plot(x, y)
plt.xlabel('Time')
plt.ylabel('Mental Health Score')
plt.title('Social Media Usage and Mental Health')
plt.gca().set_ylim([np.min(y), np.max(y)])  # Start the y-axis at the minimum value of the data
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:36:51.997043,llama-3-8b-v2,v2,False
truncated_axis,critical,social_science,voting behavior analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some data
x = np.arange(5)
y = np.random.randint(0, 10, size=5)

# Create a figure and axis
fig, ax = plt.subplots()

# Set the y-axis limits to start at 5 to exaggerate differences
ax.set_ylim(5, 15)

# Plot the data
ax.plot(x, y)

# Show the plot
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some data
x = np.arange(5)
y = np.random.randint(0, 10, size=5)

# Create a figure and axis
fig, ax = plt.subplots()

# Determine the minimum and maximum y-values to set the y-axis limits correctly
min_y = np.min(y)
max_y = np.max(y)

# Set the y-axis limits to start at the minimum y-value to accurately represent the data
ax.set_ylim(min_y, max_y)

# Plot the data
ax.plot(x, y)

# Show the plot
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:37:39.150440,llama-3-8b-v2,v2,False
truncated_axis,critical,social_science,social media usage and mental health,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load the data
df = pd.read_csv('voting_data.csv')

# Plot the data
plt.scatter(df['candidate'], df['votes'])
plt.xlabel('Candidate')
plt.ylabel('Votes')
plt.title('Voting Behavior Analysis')
plt.show()

# Truncate the y-axis to start at 100
plt.gca().set_ylim(100, plt.gca().get_ylim()[1])","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd

# Load the data
df = pd.read_csv('voting_data.csv')

# Plot the data
plt.scatter(df['candidate'], df['votes'])
plt.xlabel('Candidate')
plt.ylabel('Votes')
plt.title('Voting Behavior Analysis')
plt.show()

# Truncate the y-axis to start at the minimum value
min_y = df['votes'].min()
plt.gca().set_ylim(min_y, plt.gca().get_ylim()[1])",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:38:21.031159,llama-3-8b-v2,v2,False
truncated_axis,critical,social_science,voting behavior analysis,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
df = pd.read_csv('crime_rate_data.csv')

# Plot the data with a flawed y-axis
plt.figure(figsize=(8, 6))
sns.lineplot(x='year', y='crime_rate', data=df)
plt.ylim(0, 100)  # Set the y-axis limits to 0-100
plt.title('Crime Rate Over Time')
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
df = pd.read_csv('crime_rate_data.csv')

# Plot the data with a corrected y-axis
plt.figure(figsize=(8, 6))
sns.lineplot(x='year', y='crime_rate', data=df)
plt.ylim(df['crime_rate'].min(), df['crime_rate'].max())  # Set the y-axis limits to the actual data range
plt.title('Crime Rate Over Time')
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:39:05.989017,llama-3-8b-v2,v2,False
truncated_axis,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y = np.random.normal(0, 1, 100)

# Plot the data with a flawed y-axis
fig, ax = plt.subplots()
ax.plot(x, y)
ax.set_ylim([-5, 5])  # Starting y-axis at non-zero to exaggerate differences
ax.set_title(""Flawed Y-Axis Plot"")
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y = np.random.normal(0, 1, 100)

# Plot the data with a corrected y-axis
fig, ax = plt.subplots()
ax.plot(x, y)
ax.set_ylim([-1, 1])  # Starting y-axis at zero to show the true scale
ax.set_title(""Corrected Y-Axis Plot"")
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:39:48.711315,llama-3-8b-v2,v2,False
truncated_axis,critical,education,student performance prediction,python,intermediate,"import matplotlib.pyplot as plt

# Data for comparison
method1_scores = [80, 75, 90, 85, 78]
method2_scores = [70, 72, 75, 80, 85]

# Create the figure and axis
fig, ax = plt.subplots()

# Plot the data, starting y-axis at 60 to exaggerate differences
ax.plot(method1_scores, label='Method 1')
ax.plot(method2_scores, label='Method 2')

# Set the y-axis limits to start at 60
ax.set_ylim([60, 100])

# Add a title and labels
ax.set_title('Method Comparison')
ax.set_xlabel('Score')
ax.set_ylabel('Percentage')

# Add a legend
ax.legend()

# Show the plot
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt

# Data for comparison
method1_scores = [80, 75, 90, 85, 78]
method2_scores = [70, 72, 75, 80, 85]

# Create the figure and axis
fig, ax = plt.subplots()

# Plot the data, starting y-axis at 0 to show true differences
ax.plot(method1_scores, label='Method 1')
ax.plot(method2_scores, label='Method 2')

# Set the y-axis limits to start at 0
ax.set_ylim([0, 100])

# Add a title and labels
ax.set_title('Method Comparison')
ax.set_xlabel('Score')
ax.set_ylabel('Percentage')

# Add a legend
ax.legend()

# Show the plot
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:40:50.678283,llama-3-8b-v2,v2,False
truncated_axis,critical,social_science,social media usage and mental health,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate sample data
x = np.arange(10)
y = np.random.rand(10)

# Plot the data with a flawed y-axis
fig, ax = plt.subplots()
ax.plot(x, y)
ax.set_ylim([0.5, 1.0])  # Start y-axis at 0.5 to exaggerate differences
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate sample data
x = np.arange(10)
y = np.random.rand(10)

# Plot the data with a corrected y-axis
fig, ax = plt.subplots()
ax.plot(x, y)
ax.set_ylim([0, 1])  # Start y-axis at zero to accurately display the data
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Social Media Usage and Mental Health')
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:41:46.356785,llama-3-8b-v2,v2,False
truncated_axis,critical,social_science,income inequality trends,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
x = np.arange(10)
y = np.random.normal(10, 2, 10)

# Create the figure and axis
fig, ax = plt.subplots()

# Set the y-axis limits to exaggerate the differences
ax.set_ylim([5, 15])

# Plot the data
ax.plot(x, y)

# Show the plot
plt.show()","{'summary': 'Truncated Y-Axis Manipulation', 'detailed_explanation': 'Truncated Y-Axis Manipulation: Truncated axes distort visual perception of magnitude', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
x = np.arange(10)
y = np.random.normal(10, 2, 10)

# Create the figure and axis
fig, ax = plt.subplots()

# Set the y-axis limits to start at 0
ax.set_ylim([0, np.max(y) + 2])

# Plot the data
ax.plot(x, y)

# Show the plot
plt.show()",['Truncated axes distort visual perception of magnitude'],Truncated axes distort visual perception of magnitude,2025-12-16T11:42:32.316882,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Sample data
ages = np.arange(20, 81, 10)
males = np.random.normal(0.03, 0.01, 6)
females = np.random.normal(0.06, 0.02, 6)

# Create the plot
fig, ax1 = plt.subplots()
ax1.plot(ages, males, 'b-')
ax1.set_xlabel('Age')
ax1.set_ylabel('Male Disease Prevalence', color='b')

ax2 = ax1.twinx()
ax2.plot(ages, females, 'r-')
ax2.set_ylabel('Female Disease Prevalence', color='r')

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Sample data
ages = np.arange(20, 81, 10)
males = np.random.normal(0.03, 0.01, 6)
females = np.random.normal(0.06, 0.02, 6)

# Create the plot
fig, ax1 = plt.subplots()
ax1.plot(ages, males, 'b-')
ax1.set_xlabel('Age')
ax1.set_ylabel('Disease Prevalence')

ax2 = ax1.twinx()
ax2.plot(ages, females, 'r-')
ax2.set_ylabel('Disease Prevalence')

# Set the y-axis limits to the same scale
ax1.set_ylim(0, max(max(males), max(females)) * 1.1)
ax2.set_ylim(0, max(max(males), max(females)) * 1.1)

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:43:32.460644,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate sample data
np.random.seed(0)
demographics = ['Male', 'Female']
disease_prevalence = np.random.uniform(0.1, 0.5, size=(2,))
age_groups = [20, 30, 40, 50, 60]

# Create a figure and two subplots
fig, ax1 = plt.subplots()
ax2 = ax1.twinx()

# Plot the data
ax1.bar(range(len(disease_prevalence)), disease_prevalence, color='b')
ax2.plot(range(len(age_groups)), age_groups, color='r')

# Set the y-axis labels
ax1.set_ylabel('Disease Prevalence', color='b')
ax2.set_ylabel('Age Groups', color='r')

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate sample data
np.random.seed(0)
demographics = ['Male', 'Female']
disease_prevalence = np.random.uniform(0.1, 0.5, size=(2,))
age_groups = np.random.uniform(20, 60, size=(5,))

# Create a figure and a single subplot
fig, ax = plt.subplots()

# Plot the data
ax.bar(range(len(disease_prevalence)), disease_prevalence, color='b')
ax.plot(range(len(age_groups)), age_groups, color='r')

# Set the y-axis label and scale
ax.set_ylabel('Value', color='b')
ax.set_ylim(0, 1)
ax2 = ax.twinx()
ax2.set_ylabel('Age Groups', color='r')
ax2.set_ylim(20, 60)

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:44:37.576785,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
x = np.random.randn(100)
y1 = 2 * x + 3
y2 = x**2

# Create the plot
fig, ax1 = plt.subplots()
ax1.plot(x, y1, 'bo')
ax1.set_ylabel('Student Performance (raw scores)')
ax1.set_title('Student Performance vs. Test Scores')

ax2 = ax1.twinx()
ax2.plot(x, y2, 'r-')
ax2.set_ylabel('Test Scores')

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
x = np.random.randn(100)
y1 = 2 * x + 3
y2 = x**2

# Create the plot
fig, ax1 = plt.subplots()
ax1.plot(x, y1, 'bo')
ax1.set_ylabel('Student Performance (raw scores)')
ax1.set_title('Student Performance vs. Test Scores')

# Create a new y-axis with the same scale for both variables
ax2 = ax1.twinx()
ax2.set_ylabel('Scaled Test Scores')
ax2.set_ylim(0, 100)

ax2.plot(x, y2 / 10, 'r-')

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:45:36.286450,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,business,pricing strategy impact analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
x = np.random.rand(100)
y1 = np.random.rand(100) * 100
y2 = np.random.rand(100) * 10

# Create a flawed plot
fig, ax1 = plt.subplots()
ax1.plot(x, y1, 'b-')
ax1.set_xlabel('X Axis')
ax1.set_ylabel('Y Axis 1', color='b')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.plot(x, y2, 'r-')
ax2.set_ylabel('Y Axis 2', color='r')
ax2.tick_params('y', colors='r')

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
x = np.random.rand(100)
y1 = np.random.rand(100) * 100
y2 = np.random.rand(100) * 10

# Create a corrected plot
fig, ax1 = plt.subplots()
ax1.plot(x, y1, 'b-')
ax1.set_xlabel('X Axis')
ax1.set_ylabel('Y Axis 1', color='b')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.plot(x, y2, 'r-')
ax2.set_ylabel('Y Axis 2', color='r')
ax2.set_yticks(np.arange(0, 11, 1))  # Set the y-ticks to a common scale
ax2.tick_params('y', colors='r')

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:46:38.807462,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,business,marketing campaign effectiveness study,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load data
data = pd.read_csv('churn_data.csv')

# Plot the data with dual axes
fig, ax1 = plt.subplots()
ax1.plot(data['tenure'], data['monthly_charges'], 'bo')
ax1.set_xlabel('Tenure')
ax1.set_ylabel('Monthly Charges', color='b')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.plot(data['tenure'], data['churn_rate'], 'ro')
ax2.set_ylabel('Churn Rate', color='r')
ax2.tick_params('y', colors='r')

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd

# Load data
data = pd.read_csv('churn_data.csv')

# Calculate the logarithm of monthly charges for better visualization
data['log_monthly_charges'] = data['monthly_charges'].apply(lambda x: np.log(x))

# Plot the data with a single axis
fig, ax = plt.subplots()
ax.plot(data['tenure'], data['log_monthly_charges'], 'bo')
ax.set_xlabel('Tenure')
ax.set_ylabel('Logarithm of Monthly Charges')
ax.tick_params(axis='y')

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:47:31.800067,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,social_science,income inequality trends,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some data
np.random.seed(0)
x = np.random.randn(100)
y1 = np.random.randn(100) * 100
y2 = np.random.randn(100) * 10

# Create a figure and a set of subplots
fig, ax = plt.subplots(1, 1)

# Plot the data with two y-axes
ax.plot(x, y1, 'bo')
ax.plot(x, y2, 'ro')

# Set the y-axis limits
ax.set_ylim([0, 200])
ax.set_yticks([0, 50, 100, 150, 200])
ax.set_ylabel('Crime Rate (per 100,000 people)')

# Create a secondary y-axis for the right-hand side
ax2 = ax.twinx()
ax2.plot(x, y2, 'ro')
ax2.set_ylabel('Crime Rate (per 10,000 people)')

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some data
np.random.seed(0)
x = np.random.randn(100)
y1 = np.random.randn(100) * 100
y2 = np.random.randn(100) * 10

# Create a figure and a set of subplots
fig, ax = plt.subplots(1, 1)

# Plot the data with two y-axes
ax.plot(x, y1, 'bo')
ax.plot(x, y2, 'ro')

# Set the y-axis limits
ax.set_ylim([0, 200])
ax.set_yticks([0, 50, 100, 150, 200])
ax.set_ylabel('Crime Rate (per 100,000 people)')

# Create a secondary y-axis for the right-hand side
ax2 = ax.twinx()
ax2.plot(x, y2, 'ro')
ax2.set_ylabel('Crime Rate (per 10,000 people)')

# Set the y-axis limits for the secondary axis
ax2.set_ylim([0, 100])

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:48:42.020962,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,business,marketing campaign effectiveness study,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some data
x = np.random.rand(10)
y1 = np.random.rand(10)
y2 = np.random.rand(10)

# Create a figure and a set of subplots
fig, ax = plt.subplots()

# Plot the data using dual y-axes
ax.plot(x, y1, 'bo')
ax.plot(x, y2, 'ro')

# Set the y-axis limits
ax.set_ylim([0, 1])
ax.twinx()
ax.twinx().set_ylim([0, 100])

# Add a title and labels
ax.set_title('Pricing Strategy Impact Analysis')
ax.set_xlabel('Time')
ax.set_ylabel('Variable 1')
ax.twinx().set_ylabel('Variable 2')

# Show the plot
plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some data
x = np.random.rand(10)
y1 = np.random.rand(10)
y2 = np.random.rand(10)

# Create a figure and a set of subplots
fig, ax = plt.subplots()

# Plot the data using a single y-axis
ax.plot(x, y1, 'bo', label='Variable 1')
ax.plot(x, y2, 'ro', label='Variable 2')

# Set the y-axis limits
ax.set_ylim([0, 1])

# Add a title and labels
ax.set_title('Pricing Strategy Impact Analysis')
ax.set_xlabel('Time')
ax.set_ylabel('Variable 1')
ax.legend()

# Show the plot
plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:49:37.729017,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load graduation rate data
grad_data = pd.read_csv('graduation_rates.csv')

# Create a figure and axis object
fig, ax1 = plt.subplots()

# Plot graduation rates by demographic for demographic A
ax1.plot(grad_data['Demographic A'], grad_data['Graduation Rate A'], 'bo-')
ax1.set_xlabel('Demographic A')
ax1.set_ylabel('Graduation Rate A', color='b')

# Create a second axis for demographic B
ax2 = ax1.twinx()

# Plot graduation rates by demographic for demographic B
ax2.plot(grad_data['Demographic B'], grad_data['Graduation Rate B'], 'ro-')
ax2.set_ylabel('Graduation Rate B', color='r')

# Show the plot
plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd

# Load graduation rate data
grad_data = pd.read_csv('graduation_rates.csv')

# Calculate the average graduation rates for each demographic
avg_grad_A = grad_data['Graduation Rate A'].mean()
avg_grad_B = grad_data['Graduation Rate B'].mean()

# Create a figure and axis object
fig, ax = plt.subplots()

# Plot the average graduation rates for each demographic
ax.plot([avg_grad_A, avg_grad_B])
ax.set_xlabel('Demographic')
ax.set_ylabel('Graduation Rate')
ax.set_ylim([0, 100])  # Set the y-axis limit to ensure comparable scales

# Show the plot
plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:50:41.004900,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
np.random.seed(0)
ages = np.random.randint(18, 100, 100)
diseases = np.random.randint(0, 10, 100)
demographics = np.random.randint(0, 5, 100)

# Create a scatter plot with two y-axes
fig, ax1 = plt.subplots()
ax1.scatter(ages, diseases, color='b')
ax1.set_xlabel('Age')
ax1.set_ylabel('Disease Prevalence', color='b')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.scatter(ages, demographics, color='r')
ax2.set_ylabel('Demographic Group', color='r')
ax2.tick_params('y', colors='r')

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
np.random.seed(0)
ages = np.random.randint(18, 100, 100)
diseases = np.random.randint(0, 10, 100)
demographics = np.random.randint(0, 5, 100)

# Create a scatter plot with a single y-axis
fig, ax = plt.subplots()
ax.scatter(ages, diseases, color='b')
ax.set_xlabel('Age')
ax.set_ylabel('Disease Prevalence')

# Add a secondary axis for demographics
ax2 = ax.twinx()
ax2.plot([0, 100], [0, 5], 'r--', label='Demographic Group')
ax2.set_ylabel('Demographic Group', color='r')
ax2.tick_params('y', colors='r')

plt.legend()
plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:51:47.071479,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,business,customer churn prediction analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
np.random.seed(0)
x = np.random.rand(10)
y1 = np.random.rand(10) * 100
y2 = np.random.rand(10) * 1000

# Create the flawed plot
fig, ax1 = plt.subplots()
ax1.plot(x, y1, 'bo', label='Pricing Strategy A')
ax1.set_xlabel('Time')
ax1.set_ylabel('Revenue ($)', color='b')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.plot(x, y2, 'ro', label='Pricing Strategy B')
ax2.set_ylabel('Revenue ($1000s)', color='r')
ax2.tick_params('y', colors='r')

plt.title('Pricing Strategy Comparison')
plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
np.random.seed(0)
x = np.random.rand(10)
y1 = np.random.rand(10) * 100
y2 = np.random.rand(10) * 1000

# Create the corrected plot
fig, ax = plt.subplots()
ax.plot(x, y1, 'bo', label='Pricing Strategy A')
ax.plot(x, y2, 'ro', label='Pricing Strategy B')
ax.set_xlabel('Time')
ax.set_ylabel('Revenue ($)')
ax.legend()

# Create a secondary y-axis with a consistent scale
ax2 = ax.twinx()
ax2.set_ylabel('Revenue ($1000s)')
ax2.plot(x, y2, 'ro')

plt.title('Pricing Strategy Comparison')
plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:52:53.770204,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,education,graduation rate analysis by demographics,python,intermediate,"import matplotlib.pyplot as plt

# Data
online_grades = [85, 92, 78, 95, 88]
in_person_grades = [40, 50, 60, 70, 80]

# Plotting
fig, ax1 = plt.subplots()
ax1.plot(range(5), online_grades, 'o-')
ax1.set_xlabel('Course Number')
ax1.set_ylabel('Online Grades', color='b')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.plot(range(5), in_person_grades, 'o-')
ax2.set_ylabel('In-Person Grades', color='r')
ax2.tick_params('y', colors='r')

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt

# Data
online_grades = [85, 92, 78, 95, 88]
in_person_grades = [40, 50, 60, 70, 80]

# Find the maximum value between online and in-person grades
max_value = max(max(online_grades), max(in_person_grades))

# Plotting
fig, ax1 = plt.subplots()
ax1.plot(range(5), online_grades, 'o-')
ax1.set_xlabel('Course Number')
ax1.set_ylabel('Online Grades', color='b')
ax1.tick_params('y', colors='b')
ax1.set_ylim(0, max_value)

ax2 = ax1.twinx()
ax2.plot(range(5), in_person_grades, 'o-')
ax2.set_ylabel('In-Person Grades', color='r')
ax2.tick_params('y', colors='r')
ax2.set_ylim(0, max_value)

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:53:58.891727,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt

# Data for graduation rates by demographics
demographics = ['Male', 'Female', 'Asian', 'White', 'Black', 'Hispanic']
male_rates = [90, 80, 95, 85, 75, 70]
female_rates = [85, 95, 80, 90, 85, 80]
asian_rates = [90, 85, 95, 90, 80, 85]
white_rates = [85, 90, 85, 95, 80, 85]
black_rates = [75, 85, 80, 90, 85, 80]
hispanic_rates = [70, 80, 75, 85, 80, 85]

# Create a figure with two subplots
fig, ax1 = plt.subplots(1, 1, figsize=(8, 6))

# Plot male rates on the left y-axis
ax1.plot(demographics, male_rates, 'b-o', label='Male Rates')
ax1.set_ylabel('Male Rates (%)', color='b')
ax1.set_ylim(0, 100)

# Plot female rates on the right y-axis
ax2 = ax1.twinx()
ax2.plot(demographics, female_rates, 'r-o', label='Female Rates')
ax2.set_ylabel('Female Rates (%)', color='r')
ax2.set_ylim(0, 100)

# Add title and legend
plt.title('Graduation Rates by Demographics')
plt.legend()

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt

# Data for graduation rates by demographics
demographics = ['Male', 'Female', 'Asian', 'White', 'Black', 'Hispanic']
male_rates = [90, 80, 95, 85, 75, 70]
female_rates = [85, 95, 80, 90, 85, 80]
asian_rates = [90, 85, 95, 90, 80, 85]
white_rates = [85, 90, 85, 95, 80, 85]
black_rates = [75, 85, 80, 90, 85, 80]
hispanic_rates = [70, 80, 75, 85, 80, 85]

# Create a figure with a single subplot
fig, ax1 = plt.subplots(1, 1, figsize=(8, 6))

# Plot male rates on the same y-axis
ax1.plot(demographics, male_rates, 'b-o', label='Male Rates')
ax1.set_ylabel('Graduation Rates (%)')

# Plot female rates on the same y-axis
ax1.plot(demographics, female_rates, 'r-o', label='Female Rates')

# Add title and legend
plt.title('Graduation Rates by Demographics')
plt.legend()

# Set y-axis limits to the same range
ax1.set_ylim(0, 100)

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:55:28.506294,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,business,pricing strategy impact analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
x = np.random.rand(100)
y1 = np.random.rand(100)
y2 = np.random.rand(100) * 1000

# Create a figure with two subplots
fig, ax1 = plt.subplots()
ax2 = ax1.twinx()

# Plot y1 on the primary axis
ax1.plot(x, y1, 'b-')

# Plot y2 on the secondary axis with a different scale
ax2.plot(x, y2, 'r-')

# Set the y-axis limits to force a correlation
ax1.set_ylim([0, 1])
ax2.set_ylim([0, 10000])

# Show the plot
plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
x = np.random.rand(100)
y1 = np.random.rand(100)
y2 = np.random.rand(100) * 1000

# Create a figure with two subplots
fig, ax1 = plt.subplots()
ax2 = ax1.twinx()

# Plot y1 on the primary axis
ax1.plot(x, y1, 'b-')

# Plot y2 on the secondary axis with the same scale
ax2.plot(x, y2, 'r-')

# Set the y-axis limits to a reasonable range
ax1.set_ylim([0, 1])
ax2.set_ylim([0, 1])

# Show the plot
plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:56:32.091675,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,healthcare,observational study of patient outcomes,python,intermediate,"import matplotlib.pyplot as plt

# Data
treatments = ['A', 'B', 'C']
outcome_A = [0.8, 0.7, 0.9]
outcome_B = [0.2, 0.3, 0.1]

# Create a figure with dual axes
fig, ax1 = plt.subplots()
ax2 = ax1.twinx()

# Plot the data
ax1.plot(treatments, outcome_A, 'bo-', label='Outcome A')
ax2.plot(treatments, outcome_B, 'ro-', label='Outcome B')

# Set the y-axis limits
ax1.set_ylim([0, 1.1])
ax2.set_ylim([0, 0.5])

plt.title('Treatment Effectiveness in Hospital System')
plt.xlabel('Treatment')
plt.ylabel('Outcome')
plt.legend()
plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt

# Data
treatments = ['A', 'B', 'C']
outcome_A = [0.8, 0.7, 0.9]
outcome_B = [0.2, 0.3, 0.1]

# Create a figure with dual axes
fig, ax1 = plt.subplots()
ax2 = ax1.twinx()

# Plot the data
ax1.plot(treatments, outcome_A, 'bo-', label='Outcome A')
ax2.plot(treatments, outcome_B, 'ro-', label='Outcome B')

# Set the y-axis limits to the same scale
ax1.set_ylim([0, 1.1])
ax2.set_ylim([0, 1.1])

plt.title('Treatment Effectiveness in Hospital System')
plt.xlabel('Treatment')
plt.ylabel('Outcome')
plt.legend()
plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:57:39.850432,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,education,graduation rate analysis by demographics,python,intermediate,"import matplotlib.pyplot as plt

# Data for graduation rates by demographics
demographics = ['Male', 'Female', 'Asian', 'African American', 'Hispanic']
grad_rates = [90, 92, 95, 80, 85]
other_data = [100, 120, 150, 50, 60]

# Create a figure and two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))

# Plot graduation rates on the left axis
ax1.plot(demographics, grad_rates, marker='o')
ax1.set_title('Graduation Rates')
ax1.set_xlabel('Demographics')
ax1.set_ylabel('Graduation Rate (%)')

# Plot other data on the right axis with a different scale
ax2.plot(demographics, other_data, marker='o')
ax2.set_title('Other Data')
ax2.set_xlabel('Demographics')
ax2.set_ylabel('Other Data Units')

# Set the y-axis limits for the right axis to force correlation
ax2.set_ylim(0, 200)

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt

# Data for graduation rates by demographics
demographics = ['Male', 'Female', 'Asian', 'African American', 'Hispanic']
grad_rates = [90, 92, 95, 80, 85]

# Data for other data by demographics
other_data = [100, 120, 150, 50, 60]

# Create a figure and two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))

# Plot graduation rates on the left axis
ax1.plot(demographics, grad_rates, marker='o')
ax1.set_title('Graduation Rates')
ax1.set_xlabel('Demographics')
ax1.set_ylabel('Graduation Rate (%)')

# Plot other data on the right axis with its own y-axis
ax2.plot(demographics, other_data, marker='o')
ax2.set_title('Other Data')
ax2.set_xlabel('Demographics')
ax2.set_ylabel('Other Data Units')

# Use a secondary y-axis for the right-hand plot
ax2_2 = ax2.twinx()
ax2_2.plot(demographics, other_data, marker='o', color=ax2.get_lines()[0].get_color())
ax2_2.set_ylabel('Other Data Units')

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T11:58:58.356558,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,healthcare,treatment effectiveness in hospital system,python,intermediate,"import matplotlib.pyplot as plt

# Sample data for disease prevalence analysis
ages = [20, 30, 40, 50, 60, 70]
male_prevalence = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]
female_prevalence = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06]

# Create a figure with two subplots
fig, ax1 = plt.subplots()
ax2 = ax1.twinx()

# Plot male and female prevalence on different y-axes
ax1.plot(ages, male_prevalence, 'b-')
ax2.plot(ages, female_prevalence, 'r-')

# Set y-axis labels and titles
ax1.set_ylabel('Male Prevalence')
ax2.set_ylabel('Female Prevalence')

# Show the plot
plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt

# Sample data for disease prevalence analysis
ages = [20, 30, 40, 50, 60, 70]
male_prevalence = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]
female_prevalence = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06]

# Create a figure with a single subplot
fig, ax = plt.subplots()

# Plot male and female prevalence on the same y-axis
ax.plot(ages, male_prevalence, 'b-')
ax.plot(ages, female_prevalence, 'r-')

# Set y-axis label and title
ax.set_ylabel('Disease Prevalence')

# Add a legend to distinguish between male and female data
ax.legend(['Male', 'Female'])

# Show the plot
plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T12:00:12.007768,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate data for treatment effectiveness
np.random.seed(0)
treatment_group = np.random.normal(10, 2, 100)
control_group = np.random.normal(8, 2, 100)

# Create a figure with two subplots
fig, ax1 = plt.subplots(1, 1, figsize=(8, 6))

# Plot treatment group on the left y-axis
ax1.plot(treatment_group, label='Treatment Group')
ax1.set_ylabel('Effectiveness (y-axis 1)', color='r')

# Plot control group on the right y-axis with a different scale
ax2 = ax1.twinx()
ax2.plot(control_group, label='Control Group')
ax2.set_ylabel('Effectiveness (y-axis 2)', color='b')

# Add a legend
ax1.legend(loc='upper right')

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate data for treatment effectiveness
np.random.seed(0)
treatment_group = np.random.normal(10, 2, 100)
control_group = np.random.normal(8, 2, 100)

# Create a figure with a single subplot
fig, ax = plt.subplots(1, 1, figsize=(8, 6))

# Plot both groups on the same y-axis
ax.plot(treatment_group, label='Treatment Group')
ax.plot(control_group, label='Control Group')
ax.set_ylabel('Effectiveness')

# Add a legend
ax.legend(loc='upper right')

# Set the x-axis tick labels
ax.set_xlabel('Time')

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T12:01:12.339770,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,education,student performance prediction,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load graduation rate data
grad_data = pd.read_csv('graduation_rates.csv')

# Plot graduation rate by demographic
plt.figure(figsize=(10, 6))
sns.scatterplot(x='male', y='grad_rate', data=grad_data, ax=plt.gca())
sns.scatterplot(x='female', y='grad_rate', data=grad_data, ax=plt.gca())

# Set y-axis limits to force correlation
plt.ylim(0, 1.2)
plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load graduation rate data
grad_data = pd.read_csv('graduation_rates.csv')

# Plot graduation rate by demographic
plt.figure(figsize=(10, 6))
sns.scatterplot(x='male', y='grad_rate', data=grad_data)
sns.scatterplot(x='female', y='grad_rate', data=grad_data)

# Set y-axis limits to the same scale
plt.ylim(0, 1.2)
plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T12:01:57.825886,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,business,marketing campaign effectiveness study,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
regions = ['North', 'South', 'East', 'West']
north_sales = [100, 120, 150, 180]
south_sales = [50, 60, 70, 80]
east_sales = [20, 30, 40, 50]
west_sales = [10, 20, 30, 40]

# Create the figure and axes
fig, ax1 = plt.subplots()

# Plot the north and south sales on the left y-axis
ax1.plot(regions, north_sales, 'b-', label='North')
ax1.plot(regions, south_sales, 'r-', label='South')

# Create a second y-axis on the right
ax2 = ax1.twinx()

# Plot the east and west sales on the right y-axis
ax2.plot(regions, east_sales, 'g-', label='East')
ax2.plot(regions, west_sales, 'y-', label='West')

# Set the title and labels
ax1.set_title('Sales Performance by Region')
ax1.set_xlabel('Region')
ax1.set_ylabel('Sales (North and South)')
ax2.set_ylabel('Sales (East and West)')

# Show the legend
fig.legend(loc='upper right')

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
regions = ['North', 'South', 'East', 'West']
north_sales = [100, 120, 150, 180]
south_sales = [50, 60, 70, 80]
east_sales = [20, 30, 40, 50]
west_sales = [10, 20, 30, 40]

# Create the figure and axes
fig, ax1 = plt.subplots()

# Plot the north and south sales on the left y-axis
ax1.plot(regions, north_sales, 'b-', label='North')
ax1.plot(regions, south_sales, 'r-', label='South')

# Plot the east and west sales on the same y-axis
ax1.plot(regions, east_sales, 'g-', label='East')
ax1.plot(regions, west_sales, 'y-', label='West')

# Set the title and labels
ax1.set_title('Sales Performance by Region')
ax1.set_xlabel('Region')
ax1.set_ylabel('Sales')

# Set the legend
ax1.legend(loc='upper right')

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T12:03:23.348031,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,business,customer churn prediction analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
np.random.seed(0)
x1 = np.random.normal(0, 1, 100)
x2 = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)
y2 = np.random.normal(0, 1, 100)

# Create a figure and axis objects
fig, ax1 = plt.subplots()
ax2 = ax1.twinx()

# Plot the data on the first axis
ax1.plot(x1, y1, 'b-')
ax1.set_xlabel('Feature 1')
ax1.set_ylabel('Target 1', color='b')

# Plot the data on the second axis, but with a different scale
ax2.plot(x2, y2, 'r-')
ax2.set_ylabel('Target 2', color='r')

# Show the plot
plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)
y2 = np.random.normal(0, 1, 100)

# Create a figure and axis object
fig, ax = plt.subplots()

# Plot the data on the same axis
ax.plot(x, y1, 'b-')
ax.plot(x, y2, 'r-')

# Set the y-axis labels
ax.set_xlabel('Feature')
ax.set_ylabel('Target 1', color='b')
ax.tick_params('y', colors='b')
ax2 = ax.twinx()
ax2.set_ylabel('Target 2', color='r')
ax2.tick_params('y', colors='r')

# Show the plot
plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T12:04:30.706047,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt

# Data for two variables: exam scores and hours studied
exam_scores = [80, 75, 90, 85, 95]
hours_studied = [5, 3, 7, 6, 9]

# Create a figure and axis object
fig, ax = plt.subplots()

# Plot exam scores on the left y-axis
ax.plot(exam_scores, label='Exam Scores')

# Plot hours studied on the right y-axis with a different scale
ax2 = ax.twinx()
ax2.plot(hours_studied, label='Hours Studied')

# Set labels and title
ax.set_xlabel('Student ID')
ax.set_ylabel('Exam Scores')
ax2.set_ylabel('Hours Studied')
ax.set_title('Exam Scores vs Hours Studied')

# Show the plot
plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt

# Data for two variables: exam scores and hours studied
exam_scores = [80, 75, 90, 85, 95]
hours_studied = [5, 3, 7, 6, 9]

# Create a figure and axis object
fig, ax = plt.subplots()

# Plot exam scores on the primary y-axis
ax.plot(exam_scores, label='Exam Scores')

# Plot hours studied on the same y-axis, but with a different color
ax.plot(hours_studied, label='Hours Studied', color='red')

# Set labels and title
ax.set_xlabel('Student ID')
ax.set_ylabel('Scores/Hours')
ax.set_title('Exam Scores vs Hours Studied')

# Show the plot
plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T12:05:35.593881,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,education,student performance prediction,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Data for two variables: student scores and teacher ratings
scores = np.random.normal(50, 5, 100)
ratings = np.random.normal(4, 0.5, 100)

# Create a figure and two subplots
fig, ax1 = plt.subplots(1, 1, figsize=(8, 6))

# Plot scores on the left y-axis
ax1.plot(scores, 'bo')
ax1.set_ylabel('Student Scores', color='b')
ax1.tick_params('y', colors='b')

# Plot ratings on the right y-axis
ax2 = ax1.twinx()
ax2.plot(ratings, 'ro')
ax2.set_ylabel('Teacher Ratings', color='r')
ax2.tick_params('y', colors='r')

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Data for two variables: student scores and teacher ratings
scores = np.random.normal(50, 5, 100)
ratings = np.random.normal(4, 0.5, 100)

# Create a figure and a single subplot
fig, ax = plt.subplots(1, 1, figsize=(8, 6))

# Plot scores and ratings on the same y-axis
ax.plot(scores, 'bo', label='Student Scores')
ax.plot(ratings, 'ro', label='Teacher Ratings')
ax.set_xlabel('Students')
ax.set_ylabel('Values')
ax.legend()

# Ensure the same scale for both axes
ax.set_ylim(0, 100)

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T12:06:33.880702,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,education,graduation rate analysis by demographics,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
online_learning_outcomes = np.random.normal(70, 5, 100)
in_person_learning_outcomes = np.random.normal(80, 3, 100)

# Create the figure and axes
fig, ax1 = plt.subplots()

# Plot the online learning outcomes on the left y-axis
ax1.plot(range(100), online_learning_outcomes, 'bo-')

# Create a second y-axis on the right
ax2 = ax1.twinx()

# Plot the in-person learning outcomes on the right y-axis
ax2.plot(range(100), in_person_learning_outcomes, 'ro-')

# Set the title and labels
ax1.set_title('Online vs In-Person Learning Outcomes')
ax1.set_xlabel('Student')
ax1.set_ylabel('Online Learning Outcomes')
ax2.set_ylabel('In-Person Learning Outcomes')

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
online_learning_outcomes = np.random.normal(70, 5, 100)
in_person_learning_outcomes = np.random.normal(80, 3, 100)

# Create the figure and axes
fig, ax1 = plt.subplots()

# Plot the online learning outcomes on the left y-axis
ax1.plot(range(100), online_learning_outcomes, 'bo-')

# Set the y-axis limits to the same range for both axes
ax1.set_ylim(0, 100)

# Plot the in-person learning outcomes on the same y-axis
ax1.plot(range(100), in_person_learning_outcomes, 'ro-')

# Set the title and labels
ax1.set_title('Online vs In-Person Learning Outcomes')
ax1.set_xlabel('Student')
ax1.set_ylabel('Learning Outcomes')

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T12:07:46.241262,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,business,marketing campaign effectiveness study,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate random data
x = np.random.rand(100)
y1 = np.random.rand(100) * 10
y2 = np.random.rand(100) * 100

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the data with dual axes
ax.plot(x, y1, 'bo')
ax2 = ax.twinx()
ax2.plot(x, y2, 'ro')

# Set the y-axis limits
ax.set_ylim(0, 10)
ax2.set_ylim(0, 100)

# Show the plot
plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate random data
x = np.random.rand(100)
y1 = np.random.rand(100) * 10
y2 = np.random.rand(100) * 100

# Create a figure and axis
fig, ax = plt.subplots()

# Plot the data with a single axis
ax.plot(x, y1, 'bo')
ax.plot(x, y2, 'ro')

# Set the y-axis limits to a meaningful scale
ax.set_ylim(0, 100)

# Show the plot
plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T12:08:46.547488,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,education,graduation rate analysis by demographics,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)
y2 = np.random.normal(0, 1, 100)

# Plot the data with dual axes, using different scales
fig, ax1 = plt.subplots()
ax1.plot(x, y1, 'bo')
ax1.set_xlabel('X Axis')
ax1.set_ylabel('Y1', color='b')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.plot(x, y2, 'ro')
ax2.set_ylabel('Y2', color='r')
ax2.tick_params('y', colors='r')

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
np.random.seed(0)
x = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)
y2 = np.random.normal(0, 1, 100)

# Plot the data with dual axes, using the same scale
fig, ax1 = plt.subplots()
ax1.plot(x, y1, 'bo')
ax1.set_xlabel('X Axis')
ax1.set_ylabel('Y1 (Scale 1)')
ax1.tick_params('y', colors='b')

ax2 = ax1.twinx()
ax2.plot(x, y2, 'ro')
ax2.set_ylabel('Y2 (Scale 2)')
ax2.tick_params('y', colors='r')

# Ensure the same scale for both axes
ax1.set_ylim(0, 10)
ax2.set_ylim(0, 10)

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T12:09:51.807769,llama-3-8b-v2,v2,False
dual_axis_misleading,critical,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt

# Data for graduation rates by demographics
demographics = ['Male', 'Female', 'Asian', 'African American', 'Hispanic']
male_grad_rate = [0.8, 0.85, 0.9, 0.7, 0.75]
female_grad_rate = [0.7, 0.8, 0.9, 0.8, 0.85]
asian_grad_rate = [0.9, 0.85, 1.0, 0.7, 0.8]
african_american_grad_rate = [0.7, 0.8, 0.9, 0.8, 0.85]
hispanic_grad_rate = [0.8, 0.9, 0.85, 0.7, 0.75]

# Create the figure and axes
fig, ax = plt.subplots()

# Plot the data with two y-axes
ax.plot(demographics, male_grad_rate, label='Male')
ax.plot(demographics, female_grad_rate, label='Female')
ax.plot(demographics, asian_grad_rate, label='Asian')
ax.plot(demographics, african_american_grad_rate, label='African American')
ax.plot(demographics, hispanic_grad_rate, label='Hispanic')

# Set the y-axis limits
ax.set_ylim(0, 1.1)

# Add a secondary y-axis for the right-hand side
ax2 = ax.twinx()
ax2.set_ylim(0, 1.2)

# Add a title and labels
ax.set_title('Graduation Rates by Demographics')
ax.set_xlabel('Demographics')
ax.set_ylabel('Graduation Rate')

# Show the legend
ax.legend()

plt.show()","{'summary': 'Misleading Dual Axes', 'detailed_explanation': 'Misleading Dual Axes: Arbitrary axis scaling can create spurious visual relationships', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt

# Data for graduation rates by demographics
demographics = ['Male', 'Female', 'Asian', 'African American', 'Hispanic']
male_grad_rate = [0.8, 0.85, 0.9, 0.7, 0.75]
female_grad_rate = [0.7, 0.8, 0.9, 0.8, 0.85]
asian_grad_rate = [0.9, 0.85, 1.0, 0.7, 0.8]
african_american_grad_rate = [0.7, 0.8, 0.9, 0.8, 0.85]
hispanic_grad_rate = [0.8, 0.9, 0.85, 0.7, 0.75]

# Create the figure and axes
fig, ax = plt.subplots()

# Plot the data with a single y-axis
ax.plot(demographics, male_grad_rate, label='Male')
ax.plot(demographics, female_grad_rate, label='Female')
ax.plot(demographics, asian_grad_rate, label='Asian')
ax.plot(demographics, african_american_grad_rate, label='African American')
ax.plot(demographics, hispanic_grad_rate, label='Hispanic')

# Set the y-axis limits
ax.set_ylim(0, 1.0)

# Add a title and labels
ax.set_title('Graduation Rates by Demographics')
ax.set_xlabel('Demographics')
ax.set_ylabel('Graduation Rate')

# Show the legend
ax.legend()

plt.show()",['Arbitrary axis scaling can create spurious visual relationships'],Arbitrary axis scaling can create spurious visual relationships,2025-12-16T12:11:31.847881,llama-3-8b-v2,v2,False
wrong_chart_type,warning,education,teaching method effectiveness comparison,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load the data
df = pd.read_csv('learning_outcomes.csv')

# Group the data by online/in-person and calculate the mean score
grouped_data = df.groupby('learning_type')['score'].mean()

# Create a bar chart
plt.bar(grouped_data.index, grouped_data.values)
plt.xlabel('Learning Type')
plt.ylabel('Mean Score')
plt.title('Online vs In-Person Learning Outcomes')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd

# Load the data
df = pd.read_csv('learning_outcomes.csv')

# Group the data by online/in-person and calculate the count of each group
grouped_data = df.groupby('learning_type')['score'].count()

# Create a bar chart with a categorical x-axis
plt.bar(grouped_data.index, grouped_data.values)
plt.xlabel('Learning Type')
plt.ylabel('Count')
plt.title('Online vs In-Person Learning Outcomes')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:12:17.600016,llama-3-8b-v2,v2,False
wrong_chart_type,warning,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('patient_outcomes.csv')

# Create a bar chart for patient outcomes
plt.bar(data['Age'], data['Outcome'])
plt.xlabel('Age')
plt.ylabel('Outcome')
plt.title('Patient Outcomes by Age')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
data = pd.read_csv('patient_outcomes.csv')

# Create a scatter plot for patient outcomes
sns.scatterplot(x='Age', y='Outcome', data=data)
plt.xlabel('Age')
plt.ylabel('Outcome')
plt.title('Patient Outcomes by Age')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:12:54.920006,llama-3-8b-v2,v2,False
wrong_chart_type,warning,social_science,voting behavior analysis,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load data
data = pd.read_csv('social_media_data.csv')

# Create a bar chart to show the relationship between social media usage and mental health
sns.set()
plt.figure(figsize=(10, 6))
sns.barplot(x='mental_health', y='social_media_usage', data=data)

plt.title('Social Media Usage vs. Mental Health')
plt.xlabel('Mental Health')
plt.ylabel('Social Media Usage')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load data
data = pd.read_csv('social_media_data.csv')

# Create a scatter plot to show the relationship between social media usage and mental health
sns.set()
plt.figure(figsize=(10, 6))
sns.scatterplot(x='mental_health', y='social_media_usage', data=data)

plt.title('Social Media Usage vs. Mental Health')
plt.xlabel('Mental Health')
plt.ylabel('Social Media Usage')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:13:40.620037,llama-3-8b-v2,v2,False
wrong_chart_type,warning,social_science,social media usage and mental health,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load the data
df = pd.read_csv('voting_data.csv')

# Create a bar chart for the relationship between age and vote
plt.bar(df['age'], df['vote'])
plt.xlabel('Age')
plt.ylabel('Vote')
plt.title('Voting Behavior by Age')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Load the data
df = pd.read_csv('voting_data.csv')

# Convert categorical variables to numerical variables
df['age_group'] = pd.cut(df['age'], bins=[0, 25, 35, 45, 55, 65, 75], labels=['18-24', '25-34', '35-44', '45-54', '55-64', '65+'])
df['vote_party'] = pd.get_dummies(df['vote'])

# Create a heatmap to visualize the relationship between age group and vote party
sns.heatmap(df.groupby(['age_group', 'vote_party']).size().reset_index(), annot=True, cmap='Blues')
plt.xlabel('Age Group')
plt.ylabel('Vote Party')
plt.title('Voting Behavior by Age Group and Party')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:14:30.949602,llama-3-8b-v2,v2,False
wrong_chart_type,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('disease_data.csv')

# Group the data by demographics and calculate the prevalence of disease
grouped_data = data.groupby(['demographics', 'disease'].size().reset_index(name='prevalence')

# Create a bar chart to visualize the data
plt.bar(grouped_data['demographics'], grouped_data['prevalence'])
plt.xlabel('Demographics')
plt.ylabel('Prevalence')
plt.title('Disease Prevalence by Demographics')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
data = pd.read_csv('disease_data.csv')

# Group the data by demographics and calculate the prevalence of disease
grouped_data = data.groupby(['demographics', 'disease'].size().reset_index(name='prevalence')

# Melt the data to create a long format
melted_data = pd.melt(grouped_data, id_vars=['demographics'], value_vars='prevalence', var_name='disease', value_name='prevalence')

# Create a stacked bar chart to visualize the data
sns.set()
plt.figure(figsize=(8,6))
sns.barplot(x='demographics', y='prevalence', hue='disease', data=melted_data)
plt.xlabel('Demographics')
plt.ylabel('Prevalence')
plt.title('Disease Prevalence by Demographics')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:15:29.113592,llama-3-8b-v2,v2,False
wrong_chart_type,warning,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load the data
df = pd.read_csv('graduation_rates.csv')

# Group by demographics and calculate graduation rates
grad_rates = df.groupby('demographics')['graduation_rate'].mean()

# Create a bar chart to visualize the results
plt.bar(grad_rates.index, grad_rates.values)
plt.xlabel('Demographics')
plt.ylabel('Graduation Rate')
plt.title('Graduation Rates by Demographics')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd

# Load the data
df = pd.read_csv('graduation_rates.csv')

# Group by demographics and calculate graduation rates
grad_rates = df.groupby('demographics')['graduation_rate'].mean()

# Create a line plot to visualize the results
plt.plot(grad_rates.index, grad_rates.values)
plt.xlabel('Demographics')
plt.ylabel('Graduation Rate')
plt.title('Graduation Rates by Demographics')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:16:14.078643,llama-3-8b-v2,v2,False
wrong_chart_type,warning,healthcare,observational study of patient outcomes,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load data
df = pd.read_csv('hospital_data.csv')

# Plot treatment effectiveness
sns.barplot(x='Treatment', y='Effectiveness', data=df)
plt.title('Treatment Effectiveness')
plt.xlabel('Treatment')
plt.ylabel('Effectiveness')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load data
df = pd.read_csv('hospital_data.csv')

# Convert 'Effectiveness' to a numerical scale (e.g., percentage)
df['Effectiveness'] = df['Effectiveness'] / 100

# Plot treatment effectiveness
sns.scatterplot(x='Treatment', y='Effectiveness', data=df)
plt.title('Treatment Effectiveness')
plt.xlabel('Treatment')
plt.ylabel('Effectiveness (percentage)')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:17:05.598147,llama-3-8b-v2,v2,False
wrong_chart_type,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load the data
df = pd.read_csv('patient_outcomes.csv')

# Calculate the mean and standard deviation of patient outcomes
mean_outcomes = df['outcomes'].mean()
std_outcomes = df['outcomes'].std()

# Create a bar chart to visualize the mean and standard deviation
plt.bar(range(1, 2), [mean_outcomes, std_outcomes])
plt.xlabel('Mean and Standard Deviation')
plt.ylabel('Outcomes')
plt.title('Patient Outcomes')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd

# Load the data
df = pd.read_csv('patient_outcomes.csv')

# Create a histogram to visualize the distribution of patient outcomes
plt.hist(df['outcomes'], bins=50)
plt.xlabel('Outcomes')
plt.ylabel('Frequency')
plt.title('Distribution of Patient Outcomes')
plt.show()

# Create a boxplot to visualize the distribution of patient outcomes
plt.boxplot(df['outcomes'])
plt.xlabel('Outcomes')
plt.ylabel('Value')
plt.title('Boxplot of Patient Outcomes')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:17:58.223149,llama-3-8b-v2,v2,False
wrong_chart_type,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load data
data = pd.read_csv('disease_prevalence.csv')

# Group data by demographic and calculate prevalence
grouped_data = data.groupby('demographic')['prevalence'].mean()

# Create a bar chart
plt.bar(grouped_data.index, grouped_data.values)
plt.xlabel('Demographic')
plt.ylabel('Prevalence')
plt.title('Disease Prevalence by Demographic')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
data = pd.read_csv('disease_prevalence.csv')

# Group data by demographic and calculate prevalence
grouped_data = data.groupby('demographic')['prevalence'].mean()

# Create a line plot
plt.figure(figsize=(8, 6))
sns.lineplot(x=grouped_data.index, y=grouped_data.values)
plt.xlabel('Demographic')
plt.ylabel('Prevalence')
plt.title('Disease Prevalence by Demographic')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:18:44.315396,llama-3-8b-v2,v2,False
wrong_chart_type,warning,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('income_inequality_data.csv')

# Plot bar chart for income inequality trends
plt.figure(figsize=(8, 6))
plt.bar(df['year'], df['gini_coefficient'])
plt.xlabel('Year')
plt.ylabel('Gini Coefficient')
plt.title('Income Inequality Trends')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('income_inequality_data.csv')

# Plot line chart for income inequality trends
plt.figure(figsize=(8, 6))
sns.lineplot(x='year', y='gini_coefficient', data=df)
plt.xlabel('Year')
plt.ylabel('Gini Coefficient')
plt.title('Income Inequality Trends')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:19:34.301727,llama-3-8b-v2,v2,False
wrong_chart_type,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('healthcare_data.csv')

# Group by age and disease status
grouped_data = data.groupby(['age', 'disease_status']).size().reset_index(name='count')

# Create a bar chart to visualize the data
plt.bar(grouped_data['age'], grouped_data['count'])
plt.xlabel('Age')
plt.ylabel('Count')
plt.title('Disease Prevalence by Age')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
data = pd.read_csv('healthcare_data.csv')

# Group by age and disease status
grouped_data = data.groupby(['age', 'disease_status']).size().reset_index(name='count')

# Create a line chart to visualize the data
sns.lineplot(x='age', y='count', data=grouped_data)
plt.xlabel('Age')
plt.ylabel('Disease Prevalence')
plt.title('Disease Prevalence by Age')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:20:22.609307,llama-3-8b-v2,v2,False
wrong_chart_type,warning,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load data
df = pd.read_csv('social_media_usage_and_mental_health.csv')

# Plotting social media usage vs mental health
plt.scatter(df['social_media_usage'], df['mental_health'])
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health')
plt.title('Social Media Usage and Mental Health')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('social_media_usage_and_mental_health.csv')

# Convert categorical variable to numerical
df['social_media_usage'] = pd.Categorical(df['social_media_usage']).codes

# Use bar chart to visualize categorical variable vs mental health
sns.barplot(x='social_media_usage', y='mental_health', data=df)
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health')
plt.title('Social Media Usage and Mental Health')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:21:08.148829,llama-3-8b-v2,v2,False
wrong_chart_type,warning,business,marketing campaign effectiveness study,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load data
data = pd.read_csv('pricing_strategy_data.csv')

# Create a bar chart to visualize the relationship between price and revenue
plt.bar(data['price'], data['revenue'])
plt.xlabel('Price')
plt.ylabel('Revenue')
plt.title('Pricing Strategy Impact Analysis')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd

# Load data
data = pd.read_csv('pricing_strategy_data.csv')

# Create a scatter plot to visualize the relationship between price and revenue
plt.scatter(data['price'], data['revenue'])
plt.xlabel('Price')
plt.ylabel('Revenue')
plt.title('Pricing Strategy Impact Analysis')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:21:46.950932,llama-3-8b-v2,v2,False
wrong_chart_type,warning,social_science,voting behavior analysis,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load crime rate data
crime_data = pd.read_csv('crime_rate_data.csv')

# Create a bar chart to visualize the relationship between crime rate and population density
plt.bar(crime_data['population_density'], crime_data['crime_rate'])
plt.xlabel('Population Density')
plt.ylabel('Crime Rate')
plt.title('Relationship between Population Density and Crime Rate')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd

# Load crime rate data
crime_data = pd.read_csv('crime_rate_data.csv')

# Create a scatter plot to visualize the relationship between crime rate and population density
plt.scatter(crime_data['population_density'], crime_data['crime_rate'])
plt.xlabel('Population Density')
plt.ylabel('Crime Rate')
plt.title('Relationship between Population Density and Crime Rate')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:22:33.109588,llama-3-8b-v2,v2,False
wrong_chart_type,warning,business,pricing strategy impact analysis,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load customer churn data
churn_data = pd.read_csv('churn_data.csv')

# Convert categorical variable to numerical
churn_data['churn'] = pd.get_dummies(churn_data['churn'])

# Create a bar chart to visualize the distribution of churn rates
plt.bar(churn_data['churn'].values)
plt.xlabel('Churn')
plt.ylabel('Count')
plt.title('Churn Distribution')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Load customer churn data
churn_data = pd.read_csv('churn_data.csv')

# Convert categorical variable to numerical
churn_data['churn'] = pd.get_dummies(churn_data['churn'])

# Create a heatmap to visualize the distribution of churn rates
sns.set()
sns.countplot(x='churn', data=churn_data)
plt.xlabel('Churn')
plt.ylabel('Count')
plt.title('Churn Distribution')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:23:29.014885,llama-3-8b-v2,v2,False
wrong_chart_type,warning,social_science,crime rate factors,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load the dataset
df = pd.read_csv('voting_data.csv')

# Create a bar chart to show the relationship between age and voting behavior
plt.bar(df['age'], df['voted'])
plt.xlabel('Age')
plt.ylabel('Voting Behavior')
plt.title('Voting Behavior by Age')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd

# Load the dataset
df = pd.read_csv('voting_data.csv')

# Create a scatter plot to show the relationship between age and voting behavior
plt.scatter(df['age'], df['voted'])
plt.xlabel('Age')
plt.ylabel('Voting Behavior')
plt.title('Voting Behavior by Age')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:24:09.299827,llama-3-8b-v2,v2,False
wrong_chart_type,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
df = pd.read_csv('patient_outcomes.csv')

# Create a bar chart to show the relationship between age and outcome
sns.set()
plt.bar(df['age'], df['outcome'])
plt.xlabel('Age')
plt.ylabel('Outcome')
plt.title('Bar Chart of Age vs Outcome')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
df = pd.read_csv('patient_outcomes.csv')

# Create a scatter plot to show the relationship between age and outcome
sns.set()
sns.scatterplot(x='age', y='outcome', data=df)
plt.xlabel('Age')
plt.ylabel('Outcome')
plt.title('Scatter Plot of Age vs Outcome')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:24:50.807221,llama-3-8b-v2,v2,False
wrong_chart_type,warning,social_science,crime rate factors,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load the data
data = pd.read_csv('income_inequality.csv')

# Create a bar chart to show the trend
plt.bar(range(len(data)), data['income'])
plt.xlabel('Year')
plt.ylabel('Income')
plt.title('Income Inequality Trend')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd

# Load the data
data = pd.read_csv('income_inequality.csv')

# Create a line chart to show the trend
plt.plot(data['year'], data['income'])
plt.xlabel('Year')
plt.ylabel('Income')
plt.title('Income Inequality Trend')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:25:31.996376,llama-3-8b-v2,v2,False
wrong_chart_type,warning,education,teaching method effectiveness comparison,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load the graduation rate data
grad_data = pd.read_csv('graduation_rates.csv')

# Create a bar chart to show the graduation rate by demographic
plt.bar(grad_data['demographics'], grad_data['grad_rate'])
plt.xlabel('Demographics')
plt.ylabel('Graduation Rate')
plt.title('Graduation Rate by Demographics')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Load the graduation rate data
grad_data = pd.read_csv('graduation_rates.csv')

# Group the data by demographics and calculate the mean graduation rate
grad_rate_by_demo = grad_data.groupby('demographics')['grad_rate'].mean().reset_index()

# Create a bar chart to show the mean graduation rate by demographic
sns.barplot(x='demographics', y='grad_rate', data=grad_rate_by_demo)
plt.xlabel('Demographics')
plt.ylabel('Mean Graduation Rate')
plt.title('Mean Graduation Rate by Demographics')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:26:24.946681,llama-3-8b-v2,v2,False
wrong_chart_type,warning,education,teaching method effectiveness comparison,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load the data
data = pd.read_csv('education_data.csv')

# Group the data by online/in-person learning and calculate the mean outcome scores
grouped_data = data.groupby('learning_method').mean('outcome_score')

# Create a bar chart
plt.bar(grouped_data.index, grouped_data.values)
plt.xlabel('Learning Method')
plt.ylabel('Mean Outcome Score')
plt.title('Online vs In-Person Learning Outcomes')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Load the data
data = pd.read_csv('education_data.csv')

# Create a stacked bar chart to compare the proportion of students who achieved a certain outcome score for online and in-person learning
sns.set()
sns.countplot(x='learning_method', hue='outcome_score', data=data)
plt.xlabel('Learning Method')
plt.ylabel('Proportion of Students')
plt.title('Online vs In-Person Learning Outcomes')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:27:10.477878,llama-3-8b-v2,v2,False
wrong_chart_type,warning,education,student performance prediction,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load dataset
data = pd.read_csv('education_data.csv')

# Create a bar chart to compare teaching methods
plt.bar(data['Method'], data['Score'])
plt.xlabel('Teaching Method')
plt.ylabel('Score')
plt.title('Teaching Method Effectiveness Comparison')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Load dataset
data = pd.read_csv('education_data.csv')

# Create a line chart to compare teaching methods
sns.set()
plt.figure(figsize=(8, 6))
sns.lineplot(x='Method', y='Score', data=data)
plt.xlabel('Teaching Method')
plt.ylabel('Score')
plt.title('Teaching Method Effectiveness Comparison')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:28:00.997494,llama-3-8b-v2,v2,False
wrong_chart_type,warning,social_science,social media usage and mental health,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
df = pd.read_csv('income_inequality_data.csv')

# Plot the data
sns.barplot(x='Year', y='Gini Coefficient', data=df)
plt.title('Income Inequality Trends')
plt.xlabel('Year')
plt.ylabel('Gini Coefficient')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
df = pd.read_csv('income_inequality_data.csv')

# Plot the data using a line plot
sns.lineplot(x='Year', y='Gini Coefficient', data=df)
plt.title('Income Inequality Trends')
plt.xlabel('Year')
plt.ylabel('Gini Coefficient')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:28:37.038046,llama-3-8b-v2,v2,False
wrong_chart_type,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load the dataset
df = pd.read_csv('health_data.csv')

# Group the data by age and disease prevalence
grouped_data = df.groupby(['age', 'disease']).size().reset_index(name='count')

# Plot the data using a bar chart
plt.figure(figsize=(10, 6))
plt.bar(grouped_data['age'], grouped_data['count'])
plt.xlabel('Age')
plt.ylabel('Count')
plt.title('Disease Prevalence by Age')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Load the dataset
df = pd.read_csv('health_data.csv')

# Pivot the data to create a long format
df_long = pd.melt(df, id_vars='age', value_vars='disease', var_name='disease', value_name='count')

# Plot the data using a stacked bar chart
plt.figure(figsize=(10, 6))
sns.barplot(x='age', y='count', hue='disease', data=df_long)
plt.xlabel('Age')
plt.ylabel('Count')
plt.title('Disease Prevalence by Age')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:29:38.232105,llama-3-8b-v2,v2,False
wrong_chart_type,warning,business,marketing campaign effectiveness study,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load data
df = pd.read_csv('marketing_campaign.csv')

# Create a bar chart to show the relationship between campaign type and conversion rate
plt.bar(df['campaign_type'], df['conversion_rate'])
plt.xlabel('Campaign Type')
plt.ylabel('Conversion Rate')
plt.title('Conversion Rate by Campaign Type')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Load data
df = pd.read_csv('marketing_campaign.csv')

# Create a scatter plot to show the relationship between campaign type and conversion rate
sns.scatterplot(x='conversion_rate', y='campaign_type', data=df)
plt.xlabel('Conversion Rate')
plt.ylabel('Campaign Type')
plt.title('Conversion Rate by Campaign Type')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:30:19.399221,llama-3-8b-v2,v2,False
wrong_chart_type,warning,healthcare,observational study of patient outcomes,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load the data
df = pd.read_csv('patient_outcomes.csv')

# Create a bar chart to show the distribution of patient outcomes
plt.bar(df['patient_outcome'], df['age'])
plt.xlabel('Patient Outcome')
plt.ylabel('Age')
plt.title('Patient Age by Outcome')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd

# Load the data
df = pd.read_csv('patient_outcomes.csv')

# Create a histogram to show the distribution of patient age
plt.hist(df['age'], bins=50)
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Distribution of Patient Age')
plt.show()

# Create a boxplot to show the distribution of patient age by patient outcome
plt.boxplot(df.groupby('patient_outcome')['age'].apply(list))
plt.xlabel('Patient Outcome')
plt.ylabel('Age')
plt.title('Distribution of Patient Age by Outcome')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:31:11.617268,llama-3-8b-v2,v2,False
wrong_chart_type,warning,business,sales performance across regions,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
data = pd.read_csv('marketing_campaign_data.csv')

# Create a bar chart to visualize the campaign effectiveness
plt.figure(figsize=(10, 6))
sns.barplot(x='Date', y='Conversion Rate', data=data)

plt.title('Marketing Campaign Effectiveness')
plt.xlabel('Date')
plt.ylabel('Conversion Rate')
plt.show()","{'summary': 'Inappropriate Chart Type', 'detailed_explanation': 'Inappropriate Chart Type: Chart type should match data structure and analytical goal', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from scipy.stats import linregress

# Load the data
data = pd.read_csv('marketing_campaign_data.csv')

# Convert the 'Date' column to a datetime object
data['Date'] = pd.to_datetime(data['Date'])

# Create a scatter plot to visualize the campaign effectiveness
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Date', y='Conversion Rate', data=data)

# Add a regression line to the plot
z = np.polyfit(np.array(data['Date'], dtype=float), data['Conversion Rate'], 1)
p = np.poly1d(z)
plt.plot(np.array(data['Date'], dtype=float),p(np.array(data['Date'], dtype=float)), ""r--"")

plt.title('Marketing Campaign Effectiveness')
plt.xlabel('Date')
plt.ylabel('Conversion Rate')
plt.show()",['Chart type should match data structure and analytical goal'],Chart type should match data structure and analytical goal,2025-12-16T12:32:10.044687,llama-3-8b-v2,v2,False
overplotting,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
df = pd.read_csv('patient_outcomes.csv')

# Plot the data
plt.figure(figsize=(8, 6))
sns.scatterplot(x='age', y='outcome', data=df)

# Add a title and labels
plt.title('Patient Outcomes vs. Age')
plt.xlabel('Age (years)')
plt.ylabel('Outcome (binary)')

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
df = pd.read_csv('patient_outcomes.csv')

# Plot the data with transparency
plt.figure(figsize=(8, 6))
sns.scatterplot(x='age', y='outcome', data=df, alpha=0.5)

# Add a title and labels
plt.title('Patient Outcomes vs. Age')
plt.xlabel('Age (years)')
plt.ylabel('Outcome (binary)')

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:33:03.832248,llama-3-8b-v2,v2,False
overplotting,warning,business,sales performance across regions,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Simulate some data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a scatterplot
plt.scatter(x, y)

# Add some more data
np.random.seed(1)
x2 = np.random.normal(0, 1, 1000)
y2 = np.random.normal(0, 1, 1000)
plt.scatter(x2, y2)

plt.xlabel('X')
plt.ylabel('Y')
plt.title('Marketing Campaign Effectiveness Study')
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Simulate some data
np.random.seed(0)
x1 = np.random.normal(0, 1, 1000)
y1 = np.random.normal(0, 1, 1000)
x2 = np.random.normal(1, 0.5, 1000)
y2 = np.random.normal(1, 0.5, 1000)

# Create a scatterplot with different colors for each set of data
plt.scatter(x1, y1, color='b', alpha=0.5, label='Group 1')
plt.scatter(x2, y2, color='r', alpha=0.5, label='Group 2')

plt.xlabel('X')
plt.ylabel('Y')
plt.title('Marketing Campaign Effectiveness Study')
plt.legend()
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:34:07.668711,llama-3-8b-v2,v2,False
overplotting,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
data = pd.read_csv('patient_outcomes.csv')

# Create a scatterplot
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Age', y='Outcome', data=data)

# Add a title and labels
plt.title('Patient Outcomes by Age')
plt.xlabel('Age')
plt.ylabel('Outcome')

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
data = pd.read_csv('patient_outcomes.csv')

# Create a scatterplot with a small amount of random jitter to reduce overplotting
sns.set()
sns.scatterplot(x='Age', y='Outcome', data=data, alpha=0.5)

# Add a title and labels
plt.title('Patient Outcomes by Age')
plt.xlabel('Age')
plt.ylabel('Outcome')

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:34:52.361580,llama-3-8b-v2,v2,False
overplotting,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
df = pd.read_csv('patient_data.csv')

# Create a scatterplot of patient outcomes
sns.set()
plt.scatter(df['Age'], df['Outcome'])
plt.xlabel('Age')
plt.ylabel('Outcome')
plt.title('Patient Outcomes by Age')
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
df = pd.read_csv('patient_data.csv')

# Create a kernel density estimate (KDE) plot of patient outcomes by age
sns.set()
sns.kdeplot(df['Age'], shade=True)
plt.xlabel('Age')
plt.ylabel('Density')
plt.title('Patient Outcomes by Age')
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:35:35.032891,llama-3-8b-v2,v2,False
overplotting,warning,education,teaching method effectiveness comparison,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
df = pd.read_csv('student_performance.csv')

# Create a scatterplot of student performance
plt.figure(figsize=(8, 6))
sns.scatterplot(x='math_score', y='reading_score', data=df)

# Add a title and labels
plt.title('Student Performance')
plt.xlabel('Math Score')
plt.ylabel('Reading Score')

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
df = pd.read_csv('student_performance.csv')

# Create a scatterplot of student performance with transparency
plt.figure(figsize=(8, 6))
sns.scatterplot(x='math_score', y='reading_score', data=df, alpha=0.5)

# Add a title and labels
plt.title('Student Performance')
plt.xlabel('Math Score')
plt.ylabel('Reading Score')

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:36:21.048360,llama-3-8b-v2,v2,False
overplotting,warning,education,teaching method effectiveness comparison,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
df = pd.read_csv('student_performance.csv')

# Create a scatterplot of student performance vs. hours studied
sns.set()
sns.scatterplot(x='hours_studied', y='performance', data=df)

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
df = pd.read_csv('student_performance.csv')

# Create a hexbin plot to visualize the data density
sns.set()
sns.set_style(""whitegrid"")
sns.kdeplot(df['hours_studied'], shade=True, axlabel='Hours Studied')
sns.kdeplot(df['performance'], shade=True, axlabel='Performance')

# Add a scatterplot on top of the hexbin plot
sns.scatterplot(x='hours_studied', y='performance', data=df, alpha=0.5)

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:37:06.205904,llama-3-8b-v2,v2,False
overplotting,warning,social_science,social media usage and mental health,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
data = pd.read_csv('social_media_usage.csv')

# Create a scatterplot of social media usage and mental health
plt.figure(figsize=(8, 6))
sns.scatterplot(x='social_media_usage', y='mental_health', data=data)

# Add a title and labels
plt.title('Social Media Usage vs. Mental Health')
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health')

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
data = pd.read_csv('social_media_usage.csv')

# Create a scatterplot of social media usage and mental health
plt.figure(figsize=(8, 6))
sns.scatterplot(x='social_media_usage', y='mental_health', data=data, alpha=0.5)

# Add a title and labels
plt.title('Social Media Usage vs. Mental Health')
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health')

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:37:50.259598,llama-3-8b-v2,v2,False
overplotting,warning,social_science,crime rate factors,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
df = pd.read_csv('voting_data.csv')

# Create a scatterplot
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x='age', y='voted', data=df)

# Add a title and labels
plt.title('Voting Behavior by Age')
plt.xlabel('Age')
plt.ylabel('Voted (1/0)')

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
df = pd.read_csv('voting_data.csv')

# Create a scatterplot with transparency
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x='age', y='voted', data=df, alpha=0.3)

# Add a title and labels
plt.title('Voting Behavior by Age')
plt.xlabel('Age')
plt.ylabel('Voted (1/0)')

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:38:41.768164,llama-3-8b-v2,v2,False
overplotting,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
df = pd.read_csv('patient_outcomes.csv')

# Create a scatterplot of patient outcomes
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x='age', y='outcome', data=df)

# Add a title and labels
plt.title('Patient Outcomes by Age')
plt.xlabel('Age (years)')
plt.ylabel('Outcome (0-100)')

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
df = pd.read_csv('patient_outcomes.csv')

# Create a scatterplot of patient outcomes with different colors for different outcome groups
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x='age', y='outcome', hue='outcome_group', data=df)

# Add a title and labels
plt.title('Patient Outcomes by Age')
plt.xlabel('Age (years)')
plt.ylabel('Outcome (0-100)')

# Add a legend
plt.legend(title='Outcome Group')

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:39:29.947230,llama-3-8b-v2,v2,False
overplotting,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load dataset
df = pd.read_csv('hospital_data.csv')

# Create scatterplot with overplotting
sns.set()
sns.scatterplot(x='treatment', y='outcome', data=df)

# Add a legend
plt.legend()
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load dataset
df = pd.read_csv('hospital_data.csv')

# Create a hexbin plot to visualize density patterns
sns.set()
sns.set_context(""paper"")
sns.set_style(""whitegrid"")

sns.hexbin(x='treatment', y='outcome', data=df, gridsize=10, cmap=""Blues"")

# Add a colorbar
plt.colorbar()

# Add a title and labels
plt.title(""Treatment Effectiveness in Hospital System"")
plt.xlabel(""Treatment"")
plt.ylabel(""Outcome"")

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:40:28.774360,llama-3-8b-v2,v2,False
overplotting,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Generate sample data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a figure and axis object
fig, ax = plt.subplots(figsize=(8, 6))

# Plot the data
ax.scatter(x, y)

# Add a title and labels
ax.set_title(""Treatment Effectiveness in Hospital System"")
ax.set_xlabel(""Treatment Duration"")
ax.set_ylabel(""Outcome"")

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Generate sample data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a figure and axis object
fig, ax = plt.subplots(figsize=(8, 6))

# Use a scatterplot with transparency to avoid overplotting
sns.scatterplot(x=x, y=y, alpha=0.5)

# Add a title and labels
ax.set_title(""Treatment Effectiveness in Hospital System"")
ax.set_xlabel(""Treatment Duration"")
ax.set_ylabel(""Outcome"")

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:41:19.656694,llama-3-8b-v2,v2,False
overplotting,warning,social_science,crime rate factors,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Generate sample data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a scatterplot with overplotting
plt.scatter(x, y)

# Add a title and labels
plt.title('Social Media Usage and Mental Health')
plt.xlabel('Social Media Hours')
plt.ylabel('Mental Health Score')

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Generate sample data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a scatterplot with transparency
plt.scatter(x, y, alpha=0.1)

# Add a title and labels
plt.title('Social Media Usage and Mental Health')
plt.xlabel('Social Media Hours')
plt.ylabel('Mental Health Score')

# Use a color palette to distinguish between different data points
sns.set_palette('viridis')
plt.scatter(x, y, alpha=0.1, c=y)

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:42:13.337865,llama-3-8b-v2,v2,False
overplotting,warning,social_science,crime rate factors,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Generate some sample data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a scatterplot
plt.figure(figsize=(8, 6))
sns.scatterplot(x=x, y=y)

# Add some more data to the scatterplot
x2 = np.random.normal(0.5, 1, 1000)
y2 = np.random.normal(0.5, 1, 1000)
sns.scatterplot(x=x2, y=y2)

# Add some more data to the scatterplot
x3 = np.random.normal(1, 1, 1000)
y3 = np.random.normal(1, 1, 1000)
sns.scatterplot(x=x3, y=y3)

plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Generate some sample data
np.random.seed(0)
x = np.random.normal(0, 1, 1000)
y = np.random.normal(0, 1, 1000)

# Create a scatterplot
plt.figure(figsize=(8, 6))
sns.scatterplot(x=x, y=y, alpha=0.1)

# Add some more data to the scatterplot
x2 = np.random.normal(0.5, 1, 1000)
y2 = np.random.normal(0.5, 1, 1000)
sns.scatterplot(x=x2, y=y2, alpha=0.1)

# Add some more data to the scatterplot
x3 = np.random.normal(1, 1, 1000)
y3 = np.random.normal(1, 1, 1000)
sns.scatterplot(x=x3, y=y3, alpha=0.1)

plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:43:23.603304,llama-3-8b-v2,v2,False
overplotting,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load data
data = pd.read_csv('patient_outcomes.csv')

# Create a scatterplot
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Age', y='Outcome', data=data)

# Add a title and labels
plt.title('Patient Outcomes vs Age')
plt.xlabel('Age')
plt.ylabel('Outcome')

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Load data
data = pd.read_csv('patient_outcomes.csv')

# Create a scatterplot with transparency
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Age', y='Outcome', data=data, alpha=0.5)

# Add a title and labels
plt.title('Patient Outcomes vs Age')
plt.xlabel('Age')
plt.ylabel('Outcome')

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:44:09.093632,llama-3-8b-v2,v2,False
overplotting,warning,healthcare,observational study of patient outcomes,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load dataset
df = pd.read_csv('healthcare_data.csv')

# Create a scatterplot
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x='age', y='disease_prevalence', data=df)

# Add a title and labels
plt.title('Disease Prevalence by Age')
plt.xlabel('Age')
plt.ylabel('Prevalence')

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load dataset
df = pd.read_csv('healthcare_data.csv')

# Create a hexbin plot to reduce overplotting
sns.set()
plt.figure(figsize=(8, 6))
sns.hexbin(x='age', y='disease_prevalence', data=df, gridsize=20, cmap='Blues')

# Add a title and labels
plt.title('Disease Prevalence by Age')
plt.xlabel('Age')
plt.ylabel('Prevalence')

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:44:59.023800,llama-3-8b-v2,v2,False
overplotting,warning,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt
import pandas as pd

# Load the data
grad_data = pd.read_csv('grad_rate_data.csv')

# Create a scatterplot of graduation rate by demographic variables
plt.scatter(grad_data['Gender'], grad_data['Graduation Rate'])
plt.xlabel('Gender')
plt.ylabel('Graduation Rate')
plt.title('Graduation Rate by Gender')
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Load the data
grad_data = pd.read_csv('grad_rate_data.csv')

# Create a seaborn scatterplot with a better visualization of data density
sns.set()
sns.scatterplot(x='Gender', y='Graduation Rate', data=grad_data)
plt.xlabel('Gender')
plt.ylabel('Graduation Rate')
plt.title('Graduation Rate by Gender')
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:45:53.292918,llama-3-8b-v2,v2,False
overplotting,warning,social_science,crime rate factors,python,intermediate,"import seaborn as sns
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Create a scatterplot of social media usage and mental health scores
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x='social_media_usage', y='mental_health_scores', data=data)

# Add a title and labels
plt.title('Social Media Usage and Mental Health Scores')
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health Scores')

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import seaborn as sns
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Create a scatterplot of social media usage and mental health scores, grouped by age group
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x='social_media_usage', y='mental_health_scores', hue='age_group', data=data)

# Add a title and labels
plt.title('Social Media Usage and Mental Health Scores by Age Group')
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health Scores')

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:46:47.410095,llama-3-8b-v2,v2,False
overplotting,warning,healthcare,observational study of patient outcomes,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Generate random data
np.random.seed(0)
x = np.random.randn(100)
y = np.random.randn(100)

# Create a scatterplot
plt.scatter(x, y)

# Add a title and labels
plt.title(""Treatment Effectiveness in Hospital System"")
plt.xlabel(""Treatment Duration (days)"")
plt.ylabel(""Patient Outcome Score"")

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Generate random data
np.random.seed(0)
x = np.random.randn(100)
y = np.random.randn(100)

# Jitter the data to separate overlapping points
x_jittered = x + np.random.randn(100) * 0.1
y_jittered = y + np.random.randn(100) * 0.1

# Create a KDE plot
sns.kdeplot(x_jittered, shade=True, ax=plt.gca())

# Add a title and labels
plt.title(""Treatment Effectiveness in Hospital System"")
plt.xlabel(""Treatment Duration (days)"")
plt.ylabel(""Patient Outcome Score"")

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:47:42.419709,llama-3-8b-v2,v2,False
overplotting,warning,social_science,income inequality trends,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
crime_data = pd.read_csv('crime_data.csv')

# Create a scatterplot
plt.figure(figsize=(8, 6))
sns.scatterplot(x='crime_rate', y='population_density', data=crime_data)

# Add title and labels
plt.title('Crime Rate vs. Population Density')
plt.xlabel('Crime Rate')
plt.ylabel('Population Density')

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
crime_data = pd.read_csv('crime_data.csv')

# Create a stripplot (jittered scatterplot)
plt.figure(figsize=(8, 6))
sns.stripplot(x='crime_rate', y='population_density', data=crime_data, jitter=True)

# Add title and labels
plt.title('Crime Rate vs. Population Density')
plt.xlabel('Crime Rate')
plt.ylabel('Population Density')

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:48:36.793469,llama-3-8b-v2,v2,False
overplotting,warning,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Generate sample data
data = pd.DataFrame({'online': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                     'in_person': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                     'outcome': [0.8, 0.7, 0.9, 0.8, 0.6, 0.9, 0.7, 0.8, 0.9, 0.6]})

# Create a scatterplot of online vs in-person learning outcomes
sns.set()
plt.scatter(data['online'], data['outcome'], label='Online')
plt.scatter(data['in_person'], data['outcome'], label='In-person')
plt.xlabel('Learning Method')
plt.ylabel('Outcome')
plt.title('Online vs In-person Learning Outcomes')
plt.legend()
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Generate sample data
data = pd.DataFrame({'online': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                     'in_person': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                     'outcome': [0.8, 0.7, 0.9, 0.8, 0.6, 0.9, 0.7, 0.8, 0.9, 0.6]})

# Create a scatterplot of online vs in-person learning outcomes with transparency
sns.set()
sns.scatterplot(x='online', y='outcome', hue='in_person', data=data, alpha=0.5)
plt.xlabel('Learning Method')
plt.ylabel('Outcome')
plt.title('Online vs In-person Learning Outcomes')
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:49:48.806702,llama-3-8b-v2,v2,False
overplotting,warning,social_science,social media usage and mental health,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load voting data
voting_data = pd.read_csv('voting_data.csv')

# Plot voting behavior
plt.figure(figsize=(10, 6))
sns.scatterplot(x='age', y='vote', data=voting_data)
plt.title('Voting Behavior')
plt.xlabel('Age')
plt.ylabel('Vote')
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load voting data
voting_data = pd.read_csv('voting_data.csv')

# Plot voting behavior with transparency
plt.figure(figsize=(10, 6))
sns.scatterplot(x='age', y='vote', data=voting_data, alpha=0.5)
plt.title('Voting Behavior')
plt.xlabel('Age')
plt.ylabel('Vote')
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:50:42.032690,llama-3-8b-v2,v2,False
overplotting,warning,social_science,voting behavior analysis,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
voting_data = pd.read_csv('voting_data.csv')

# Create a scatterplot of voting behavior
plt.figure(figsize=(8, 6))
sns.scatterplot(x='democrat', y='republican', data=voting_data)
plt.title('Voting Behavior')
plt.xlabel('Democratic Party')
plt.ylabel('Republican Party')
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
voting_data = pd.read_csv('voting_data.csv')

# Create a scatterplot of voting behavior with transparency
plt.figure(figsize=(8, 6))
sns.scatterplot(x='democrat', y='republican', data=voting_data, alpha=0.5)
plt.title('Voting Behavior')
plt.xlabel('Democratic Party')
plt.ylabel('Republican Party')
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:51:28.657637,llama-3-8b-v2,v2,False
overplotting,warning,education,student performance prediction,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load dataset
data = pd.read_csv('learning_outcomes.csv')

# Create scatterplot
plt.figure(figsize=(8, 6))
sns.scatterplot(x='online', y='outcome', data=data)

# Add title and labels
plt.title('Online vs In-Person Learning Outcomes')
plt.xlabel('Online Learning')
plt.ylabel('Outcome Score')

# Show plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load dataset
data = pd.read_csv('learning_outcomes.csv')

# Create scatterplot with transparency and color mapping
plt.figure(figsize=(8, 6))
sns.scatterplot(x='online', y='outcome', data=data, alpha=0.5, hue='learning_method', palette='viridis')

# Add title and labels
plt.title('Online vs In-Person Learning Outcomes')
plt.xlabel('Online Learning')
plt.ylabel('Outcome Score')

# Show plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:52:18.944252,llama-3-8b-v2,v2,False
overplotting,warning,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Simulate data
np.random.seed(0)
x1 = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)
x2 = np.random.normal(2, 1, 100)
y2 = np.random.normal(2, 1, 100)

# Create scatterplot
plt.scatter(x1, y1, c='b', alpha=1)
plt.scatter(x2, y2, c='r', alpha=1)

# Add title and labels
plt.title('Efficacy of Drug A vs. Drug B')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')

# Show plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Simulate data
np.random.seed(0)
x1 = np.random.normal(0, 1, 100)
y1 = np.random.normal(0, 1, 100)
x2 = np.random.normal(2, 1, 100)
y2 = np.random.normal(2, 1, 100)

# Create scatterplot with transparency
plt.scatter(x1, y1, c='b', alpha=0.2)
plt.scatter(x2, y2, c='r', alpha=0.2)

# Add title and labels
plt.title('Efficacy of Drug A vs. Drug B')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')

# Show plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:53:17.146097,llama-3-8b-v2,v2,False
overplotting,warning,healthcare,treatment effectiveness in hospital system,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate sample data
np.random.seed(0)
n = 1000
x = np.random.normal(0, 1, n)
y = np.random.normal(0, 1, n)

# Create a scatterplot
plt.scatter(x, y)

# Add a title and labels
plt.title(""Dense Scatterplot"")
plt.xlabel(""X-axis"")
plt.ylabel(""Y-axis"")

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Generate sample data
np.random.seed(0)
n = 1000
x = np.random.normal(0, 1, n)
y = np.random.normal(0, 1, n)

# Create a scatterplot with transparency
plt.scatter(x, y, alpha=0.5)

# Add a title and labels
plt.title(""Corrected Scatterplot"")
plt.xlabel(""X-axis"")
plt.ylabel(""Y-axis"")

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:54:13.944371,llama-3-8b-v2,v2,False
overplotting,warning,education,student performance prediction,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate random data for two teaching methods
method1_scores = np.random.normal(70, 5, 100)
method2_scores = np.random.normal(75, 5, 100)

# Create a scatterplot with default settings
plt.scatter(method1_scores, method2_scores)

# Add axis labels and title
plt.xlabel('Method 1 Scores')
plt.ylabel('Method 2 Scores')
plt.title('Teaching Method Effectiveness Comparison')

# Show the plot
plt.show()","{'summary': 'Overplotting Without Transparency', 'detailed_explanation': 'Overplotting Without Transparency: Overlapping points obscure data distribution', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Generate random data for two teaching methods
method1_scores = np.random.normal(70, 5, 100)
method2_scores = np.random.normal(75, 5, 100)

# Use seaborn's scatterplot function with a larger alpha value to reduce overplotting
sns.scatterplot(x=method1_scores, y=method2_scores, alpha=0.5)

# Add axis labels and title
plt.xlabel('Method 1 Scores')
plt.ylabel('Method 2 Scores')
plt.title('Teaching Method Effectiveness Comparison')

# Show the plot
plt.show()",['Overlapping points obscure data distribution'],Overlapping points obscure data distribution,2025-12-16T12:55:03.206360,llama-3-8b-v2,v2,False
missing_uncertainty,warning,education,teaching method effectiveness comparison,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the dataset
df = pd.read_csv('online_learning_outcomes.csv')

# Group the data by'mode' and calculate the mean of 'outcomes'
grouped_data = df.groupby('mode')['outcomes'].mean()

# Create a bar plot
plt.figure(figsize=(8, 6))
sns.barplot(x=grouped_data.index, y=grouped_data.values)

# Set the title and labels
plt.title('Online vs In-Person Learning Outcomes')
plt.xlabel('Mode')
plt.ylabel('Mean Outcomes')

# Show the plot
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Load the dataset
df = pd.read_csv('online_learning_outcomes.csv')

# Group the data by'mode' and calculate the mean and standard error of 'outcomes'
grouped_data = df.groupby('mode')['outcomes'].agg(['mean','sem'])

# Create a bar plot with error bars
plt.figure(figsize=(8, 6))
sns.barplot(x=grouped_data.index, y=grouped_data['mean'], yerr=grouped_data['sem'])

# Set the title and labels
plt.title('Online vs In-Person Learning Outcomes')
plt.xlabel('Mode')
plt.ylabel('Mean Outcomes')

# Show the plot
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T12:56:06.067375,llama-3-8b-v2,v2,False
missing_uncertainty,warning,education,online vs in-person learning outcomes,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Data
method1_scores = [85, 90, 78, 92, 88]
method2_scores = [90, 92, 85, 95, 91]

# Calculate means
method1_mean = np.mean(method1_scores)
method2_mean = np.mean(method2_scores)

# Create plot
plt.bar(['Method 1', 'Method 2'], [method1_mean, method2_mean])
plt.xlabel('Teaching Method')
plt.ylabel('Average Score')
plt.title('Teaching Method Effectiveness')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Data
method1_scores = [85, 90, 78, 92, 88]
method2_scores = [90, 92, 85, 95, 91]

# Calculate means and standard deviations
method1_mean = np.mean(method1_scores)
method1_std = np.std(method1_scores)
method2_mean = np.mean(method2_scores)
method2_std = np.std(method2_scores)

# Create plot
sns.set()
plt.bar(['Method 1', 'Method 2'], [method1_mean, method2_mean], yerr=[method1_std, method2_std])
plt.xlabel('Teaching Method')
plt.ylabel('Average Score')
plt.title('Teaching Method Effectiveness')
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T12:57:05.953367,llama-3-8b-v2,v2,False
missing_uncertainty,warning,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load the data
data = pd.read_csv('graduation_rates.csv')

# Calculate the mean graduation rate for each demographic group
means = data.groupby('demographic').mean()['graduation_rate']

# Plot the results
plt.bar(range(len(means)), means)
plt.xlabel('Demographic')
plt.ylabel('Mean Graduation Rate')
plt.title('Mean Graduation Rates by Demographic')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import t

# Load the data
data = pd.read_csv('graduation_rates.csv')

# Calculate the mean graduation rate and standard error for each demographic group
means = data.groupby('demographic')['graduation_rate'].agg(['mean','std']).reset_index()
means.columns = ['demographic','mean','std']

# Plot the results with error bars
plt.figure(figsize=(8, 6))
sns.barplot(x='demographic', y='mean', data=means)
plt.errorbar(means['demographic'], means['mean'], yerr=means['std'], fmt='o', capsize=5, color='black')
plt.xlabel('Demographic')
plt.ylabel('Mean Graduation Rate')
plt.title('Mean Graduation Rates by Demographic')
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T12:58:00.978609,llama-3-8b-v2,v2,False
missing_uncertainty,warning,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('patient_outcomes.csv')

# Calculate the mean outcome for each group
grouped_df = df.groupby('treatment_group')['outcome'].mean()

# Plot the results
plt.bar(grouped_df.index, grouped_df.values)
plt.xlabel('Treatment Group')
plt.ylabel('Mean Outcome')
plt.title('Mean Outcomes by Treatment Group')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import t

# Load the data
df = pd.read_csv('patient_outcomes.csv')

# Calculate the mean outcome for each group
grouped_df = df.groupby('treatment_group')['outcome'].mean()

# Calculate the standard error for each group
grouped_se = df.groupby('treatment_group')['outcome'].sem()

# Plot the results with error bars
plt.bar(grouped_df.index, grouped_df.values, yerr=grouped_se)
plt.xlabel('Treatment Group')
plt.ylabel('Mean Outcome')
plt.title('Mean Outcomes by Treatment Group')
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T12:58:48.554486,llama-3-8b-v2,v2,False
missing_uncertainty,warning,social_science,income inequality trends,python,intermediate,"import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Load data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Plot the data
sns.barplot(x='Social Media Hours', y='Mental Health Score', data=data)

# Add title and labels
plt.title('Social Media Usage and Mental Health')
plt.xlabel('Social Media Hours')
plt.ylabel('Mental Health Score')

# Show the plot
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Load data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Calculate the mean and standard error of the mental health score for each social media usage group
means = data.groupby('Social Media Hours')['Mental Health Score'].mean()
std_err = data.groupby('Social Media Hours')['Mental Health Score'].sem()

# Plot the data with error bars
sns.barplot(x=means.index, y=means.values, yerr=std_err.values, capsize=5)

# Add title and labels
plt.title('Social Media Usage and Mental Health')
plt.xlabel('Social Media Hours')
plt.ylabel('Mental Health Score')

# Show the plot
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T12:59:42.868297,llama-3-8b-v2,v2,False
missing_uncertainty,warning,education,graduation rate analysis by demographics,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Sample data
method1 = [90, 85, 78, 92, 88]
method2 = [95, 92, 98, 96, 94]

# Calculate means
mean1 = np.mean(method1)
mean2 = np.mean(method2)

# Plot the data
plt.bar(['Method 1', 'Method 2'], [mean1, mean2])
plt.xlabel('Teaching Method')
plt.ylabel('Score')
plt.title('Teaching Method Effectiveness')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import ttest_ind

# Sample data
method1 = [90, 85, 78, 92, 88]
method2 = [95, 92, 98, 96, 94]

# Calculate means and standard errors
mean1 = np.mean(method1)
mean2 = np.mean(method2)
se1 = np.std(method1) / np.sqrt(len(method1))
se2 = np.std(method2) / np.sqrt(len(method2))

# Calculate confidence intervals
ci1 = mean1 - 1.96 * se1, mean1 + 1.96 * se1
ci2 = mean2 - 1.96 * se2, mean2 + 1.96 * se2

# Plot the data with error bars
plt.bar(['Method 1', 'Method 2'], [mean1, mean2], yerr=[se1, se2])
plt.xlabel('Teaching Method')
plt.ylabel('Score')
plt.title('Teaching Method Effectiveness')
plt.show()

# Calculate and display the p-value for the t-test
p_value = ttest_ind(method1, method2)[1]
print(f""P-value: {p_value:.4f}"")",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:01:04.366326,llama-3-8b-v2,v2,False
missing_uncertainty,warning,education,student performance prediction,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
data = pd.read_csv('learning_outcomes.csv')

# Create a bar plot of online vs in-person learning outcomes
sns.set()
plt.figure(figsize=(8, 6))
sns.barplot(x='learning_method', y='score', data=data)

# Add a title and labels
plt.title('Online vs In-Person Learning Outcomes')
plt.xlabel('Learning Method')
plt.ylabel('Score')

# Show the plot
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Load the data
data = pd.read_csv('learning_outcomes.csv')

# Calculate the mean and standard error for each group
online_mean = data[data['learning_method'] == 'online']['score'].mean()
in_person_mean = data[data['learning_method'] == 'in-person']['score'].mean()
online_sem = data[data['learning_method'] == 'online']['score'].sem()
in_person_sem = data[data['learning_method'] == 'in-person']['score'].sem()

# Create a bar plot with error bars
sns.set()
plt.figure(figsize=(8, 6))
plt.bar(['online', 'in-person'], [online_mean, in_person_mean], yerr=[online_sem, in_person_sem])
plt.xlabel('Learning Method')
plt.ylabel('Score')
plt.title('Online vs In-Person Learning Outcomes')

# Add a horizontal line for the overall mean
overall_mean = data['score'].mean()
plt.axhline(overall_mean, color='black', linestyle='--')

# Show the plot
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:02:12.505951,llama-3-8b-v2,v2,False
missing_uncertainty,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import t

# Data from a clinical trial
treatments = ['Drug A', 'Drug B', 'Placebo']
means = [20, 18, 15]
std_err = [2, 3, 2]

# Create a bar chart with point estimates only
fig, ax = plt.subplots()
ax.bar(range(len(treatments)), means)
ax.set_xticks(range(len(treatments)))
ax.set_xticklabels(treatments)
plt.xlabel('Treatment')
plt.ylabel('Mean Outcome')
plt.title('Efficacy of Treatments')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import t

# Data from a clinical trial
treatments = ['Drug A', 'Drug B', 'Placebo']
means = [20, 18, 15]
std_err = [2, 3, 2]
se = np.array(std_err)  # standard errors
ci = t.interval(0.95, len(means) - 1, loc=means, scale=se)  # 95% CI

# Create a bar chart with point estimates and error bars
fig, ax = plt.subplots()
ax.bar(range(len(treatments)), means)
ax.errorbar(range(len(treatments)), means, yerr=se, fmt='o')
ax.set_xticks(range(len(treatments)))
ax.set_xticklabels(treatments)
ax.set_xlabel('Treatment')
ax.set_ylabel('Mean Outcome')
ax.set_title('Efficacy of Treatments')
plt.show()

# Add confidence intervals as shaded regions
for i in range(len(treatments)):
    ax.fill_between(range(len(treatments)), ci[0][i], ci[1][i], alpha=0.2)",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:03:19.371606,llama-3-8b-v2,v2,False
missing_uncertainty,warning,business,sales performance across regions,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load data
data = pd.read_csv('sales_data.csv')

# Calculate mean sales by region
region_mean_sales = data.groupby('region')['sales'].mean()

# Create a bar plot of the mean sales
sns.barplot(x=region_mean_sales.index, y=region_mean_sales.values)
plt.title('Mean Sales by Region')
plt.xlabel('Region')
plt.ylabel('Mean Sales')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Load data
data = pd.read_csv('sales_data.csv')

# Calculate mean sales by region
region_mean_sales = data.groupby('region')['sales'].mean()

# Calculate 95% confidence intervals for the mean sales by region
region_ci = data.groupby('region')['sales'].sem() * 1.96

# Create a bar plot of the mean sales with confidence intervals
sns.barplot(x=region_mean_sales.index, y=region_mean_sales.values, yerr=region_ci)

# Add error bars to the plot
plt.errorbar(region_mean_sales.index, region_mean_sales.values, xerr=None, yerr=region_ci, fmt='o')

# Customize the plot
plt.title('Mean Sales by Region with 95% Confidence Intervals')
plt.xlabel('Region')
plt.ylabel('Mean Sales')
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:04:16.392209,llama-3-8b-v2,v2,False
missing_uncertainty,warning,education,graduation rate analysis by demographics,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Data for two teaching methods
method1_scores = [85, 78, 92, 88, 76]
method2_scores = [90, 95, 80, 92, 88]

# Calculate mean and standard deviation for each method
method1_mean = np.mean(method1_scores)
method1_std = np.std(method1_scores)
method2_mean = np.mean(method2_scores)
method2_std = np.std(method2_scores)

# Create plot
plt.plot([method1_mean, method2_mean], 'o-')
plt.xlabel('Teaching Method')
plt.ylabel('Mean Score')
plt.title('Teaching Method Effectiveness Comparison')

# Show the plot
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Data for two teaching methods
method1_scores = [85, 78, 92, 88, 76]
method2_scores = [90, 95, 80, 92, 88]

# Calculate mean and standard deviation for each method
method1_mean = np.mean(method1_scores)
method1_std = np.std(method1_scores)
method2_mean = np.mean(method2_scores)
method2_std = np.std(method2_scores)

# Create plot with error bars (standard errors)
plt.plot([method1_mean, method2_mean], 'o-')
plt.errorbar([method1_mean, method2_mean], [method1_std, method2_std], fmt='o-')
plt.xlabel('Teaching Method')
plt.ylabel('Mean Score')
plt.title('Teaching Method Effectiveness Comparison')

# Show the plot
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:05:14.430281,llama-3-8b-v2,v2,False
missing_uncertainty,warning,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import t

# Load data
data = pd.read_csv('pricing_data.csv')

# Calculate mean and standard deviation of prices
mean_price = data['price'].mean()
std_price = data['price'].std()

# Create a plot of prices with point estimates only
plt.scatter(data['price'], label='Actual Prices')
plt.axhline(mean_price, color='r', label='Mean Price')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.title('Pricing Strategy Impact Analysis')
plt.legend()
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import t

# Load data
data = pd.read_csv('pricing_data.csv')

# Calculate mean and standard deviation of prices
mean_price = data['price'].mean()
std_price = data['price'].std()

# Calculate confidence interval for the mean
alpha = 0.05
t_critical = t.ppf(1 - alpha / 2, len(data) - 1)
margin_of_error = t_critical * std_price / (len(data) ** 0.5)

# Create a plot of prices with point estimates and confidence intervals
plt.scatter(data['price'], label='Actual Prices')
plt.axhline(mean_price, color='r', label='Mean Price')
plt.fill_between(range(len(data)), mean_price - margin_of_error, mean_price + margin_of_error, alpha=0.3, color='r')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.title('Pricing Strategy Impact Analysis')
plt.legend()
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:06:12.704109,llama-3-8b-v2,v2,False
missing_uncertainty,warning,healthcare,observational study of patient outcomes,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load data
df = pd.read_csv('clinical_trial_data.csv')

# Calculate means and standard errors for treatment and control groups
treatment_mean = df.groupby('treatment')['outcome'].mean()
control_mean = df.groupby('treatment')['outcome'].mean()

# Plot point estimates
sns.set()
plt.figure(figsize=(8, 6))
plt.plot(treatment_mean, label='Treatment')
plt.plot(control_mean, label='Control')
plt.xlabel('Time')
plt.ylabel('Outcome')
plt.title('Mean Outcomes by Treatment Group')
plt.legend()
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Load data
df = pd.read_csv('clinical_trial_data.csv')

# Calculate means and standard errors for treatment and control groups
treatment_mean = df.groupby('treatment')['outcome'].mean()
treatment_se = df.groupby('treatment')['outcome'].sem()
control_mean = df.groupby('treatment')['outcome'].mean()
control_se = df.groupby('treatment')['outcome'].sem()

# Plot point estimates with error bars
sns.set()
plt.figure(figsize=(8, 6))
plt.errorbar(range(len(treatment_mean)), treatment_mean, yerr=treatment_se, label='Treatment')
plt.errorbar(range(len(control_mean)), control_mean, yerr=control_se, label='Control')
plt.xlabel('Time')
plt.ylabel('Outcome')
plt.title('Mean Outcomes by Treatment Group')
plt.legend()
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:07:10.307113,llama-3-8b-v2,v2,False
missing_uncertainty,warning,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import t

# Load data
data = pd.read_csv('hospital_data.csv')

# Calculate treatment effectiveness
treatment_effectiveness = data['treatment_group'].mean() - data['control_group'].mean()

# Create a plot
plt.bar(['Treated', 'Control'], [treatment_effectiveness, -treatment_effectiveness])
plt.xlabel('Group')
plt.ylabel('Effectiveness')
plt.title('Treatment Effectiveness')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import t
import seaborn as sns

# Load data
data = pd.read_csv('hospital_data.csv')

# Calculate treatment effectiveness and confidence interval
treatment_effectiveness, se = data['treatment_group'].mean(), data['treatment_group'].sem()
conf_int = t.interval(alpha=0.95, df=len(data['treatment_group']) - 1, loc=treatment_effectiveness, scale=se)

# Create a plot
plt.figure(figsize=(8, 6))
sns.barplot(x=['Treated', 'Control'], y=[treatment_effectiveness, 0], ci=conf_int)
plt.xlabel('Group')
plt.ylabel('Effectiveness')
plt.title('Treatment Effectiveness')
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:08:02.096289,llama-3-8b-v2,v2,False
missing_uncertainty,warning,social_science,crime rate factors,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load voting data
voting_data = pd.read_csv('voting_data.csv')

# Group by party and calculate mean vote share
party_votes = voting_data.groupby('party')['vote_share'].mean()

# Plot the results
plt.bar(party_votes.index, party_votes.values)
plt.xlabel('Party')
plt.ylabel('Mean Vote Share')
plt.title('Mean Vote Share by Party')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind
import seaborn as sns

# Load voting data
voting_data = pd.read_csv('voting_data.csv')

# Group by party and calculate mean vote share with standard error
party_votes = voting_data.groupby('party')['vote_share'].agg(['mean','sem'])

# Plot the results with error bars
sns.set()
sns.barplot(x=party_votes.index, y=party_votes['mean'], yerr=party_votes['sem'], ci=95)
plt.xlabel('Party')
plt.ylabel('Mean Vote Share')
plt.title('Mean Vote Share by Party')
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:08:53.292719,llama-3-8b-v2,v2,False
missing_uncertainty,warning,business,sales performance across regions,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
data = pd.read_csv('sales_data.csv')

# Group by region and calculate the mean sales
grouped_data = data.groupby('region')['sales'].mean()

# Plot the results
plt.figure(figsize=(8, 6))
sns.barplot(x=grouped_data.index, y=grouped_data.values)
plt.xlabel('Region')
plt.ylabel('Mean Sales')
plt.title('Mean Sales by Region')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Load the data
data = pd.read_csv('sales_data.csv')

# Group by region and calculate the mean sales with confidence intervals
grouped_data = data.groupby('region')['sales'].agg(['mean','std']).reset_index()
grouped_data.columns = ['region','mean_sales','std_sales']

# Calculate the confidence intervals
alpha = 0.05
n = len(data)
std_err = grouped_data['std_sales'] / np.sqrt(n)
conf_int = grouped_data['mean_sales'] + 1.96 * std_err

# Plot the results
plt.figure(figsize=(8, 6))
sns.barplot(x=grouped_data['region'], y=grouped_data['mean_sales'], yerr=std_err)
plt.xlabel('Region')
plt.ylabel('Mean Sales')
plt.title('Mean Sales by Region with Confidence Intervals')
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:09:49.020411,llama-3-8b-v2,v2,False
missing_uncertainty,warning,education,teaching method effectiveness comparison,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Simulate data
np.random.seed(0)
n_students = 100
n_groups = 2
n_measures = 5
means = np.random.normal(loc=0, scale=1, size=(n_groups, n_measures))
stds = np.ones((n_groups, n_measures))

# Calculate point estimates
means_by_group = np.mean(means, axis=1)

# Create plot
plt.bar(np.arange(n_groups), means_by_group)
plt.xlabel(""Group"")
plt.ylabel(""Mean"")
plt.title(""Teaching Method Effectiveness"")
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Simulate data
np.random.seed(0)
n_students = 100
n_groups = 2
n_measures = 5
means = np.random.normal(loc=0, scale=1, size=(n_groups, n_measures))
stds = np.ones((n_groups, n_measures))

# Calculate point estimates and confidence intervals
means_by_group = np.mean(means, axis=1)
ci = np.std(means, axis=1) * 1.96 / np.sqrt(n_students)

# Create plot
plt.bar(np.arange(n_groups), means_by_group, yerr=ci)
plt.xlabel(""Group"")
plt.ylabel(""Mean"")
plt.title(""Teaching Method Effectiveness"")
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:10:43.354014,llama-3-8b-v2,v2,False
missing_uncertainty,warning,business,marketing campaign effectiveness study,python,intermediate,"import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Load data
df = pd.read_csv('churn_data.csv')

# Calculate the proportion of customers who churned
churn_rate = df['churned'].mean()

# Plot the proportion
plt.figure(figsize=(8, 6))
sns.barplot(x='feature_name', y='churn_rate', data=df)

plt.title('Churn Rate by Feature')
plt.xlabel('Feature')
plt.ylabel('Churn Rate')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from scipy.stats import t

# Load data
df = pd.read_csv('churn_data.csv')

# Calculate the proportion of customers who churned
churn_rate, std_err = df['churned'].mean(), df['churned'].std() / df['churned'].count() ** 0.5

# Calculate the confidence interval
conf_int = t.interval(alpha=0.95, df=df['churned'].count() - 1, scale=std_err)

# Plot the proportion with confidence intervals
plt.figure(figsize=(8, 6))
sns.barplot(x='feature_name', y='churn_rate', data=df)
plt.errorbar(x='feature_name', y=churn_rate, yerr=conf_int, fmt='o', color='black', capsize=5)

plt.title('Churn Rate by Feature with Confidence Intervals')
plt.xlabel('Feature')
plt.ylabel('Churn Rate')
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:11:41.242146,llama-3-8b-v2,v2,False
missing_uncertainty,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load dataset
df = pd.read_csv('hospital_data.csv')

# Calculate treatment effect
treatment_effect = df.groupby('treatment')['outcome'].mean()

# Create bar plot
sns.barplot(x='treatment', y='outcome', data=df)

# Add a title and labels
plt.title('Treatment Effect')
plt.xlabel('Treatment')
plt.ylabel('Mean Outcome')

# Show the plot
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv('hospital_data.csv')

# Calculate treatment effect with confidence intervals
treatment_effect = df.groupby('treatment')['outcome'].sem()  # standard error of the mean
conf_int = treatment_effect * 1.96  # 95% confidence interval

# Create bar plot with error bars
sns.barplot(x='treatment', y='outcome', data=df)
plt.errorbar(x='treatment', y='outcome', yerr=conf_int, fmt='o')

# Add a title and labels
plt.title('Treatment Effect with Confidence Intervals')
plt.xlabel('Treatment')
plt.ylabel('Mean Outcome')

# Show the plot
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:12:35.363209,llama-3-8b-v2,v2,False
missing_uncertainty,warning,social_science,crime rate factors,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
crime_data = pd.read_csv('crime_rate.csv')

# Plot the data
sns.set()
plt.figure(figsize=(8, 6))
sns.barplot(x='factor', y='rate', data=crime_data)
plt.title('Crime Rate by Factor')
plt.xlabel('Factor')
plt.ylabel('Crime Rate')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Load the data
crime_data = pd.read_csv('crime_rate.csv')

# Calculate the confidence interval for each factor
ci_lower = crime_data.groupby('factor')['rate'].apply(lambda x: x.quantile(0.025))
ci_upper = crime_data.groupby('factor')['rate'].apply(lambda x: x.quantile(0.975))

# Plot the data with confidence intervals
sns.set()
plt.figure(figsize=(8, 6))
sns.barplot(x='factor', y='rate', data=crime_data)

# Add error bars for the confidence intervals
plt.errorbar(x=crime_data['factor'], y=crime_data['rate'], yerr=[ci_lower, ci_upper], fmt='o', capsize=2)

plt.title('Crime Rate by Factor with Confidence Intervals')
plt.xlabel('Factor')
plt.ylabel('Crime Rate')
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:13:32.422938,llama-3-8b-v2,v2,False
missing_uncertainty,warning,social_science,voting behavior analysis,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Load data
income_data = np.random.normal(loc=50000, scale=10000, size=100)

# Calculate mean and standard deviation
mean_income = np.mean(income_data)
std_income = np.std(income_data)

# Create plot
plt.bar(x=np.arange(len(income_data)), height=income_data)
plt.xlabel('Individual')
plt.ylabel('Income')
plt.title('Income Distribution')

# Add point estimate
plt.plot(0, mean_income, 'o', markersize=10, color='r')
plt.annotate(f'Mean Income: {mean_income:.2f}', (0, mean_income), xytext=(0, 100), textcoords=""offset points"", ha='center', va='top')

plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np

# Load data
income_data = np.random.normal(loc=50000, scale=10000, size=100)

# Calculate mean and standard deviation
mean_income = np.mean(income_data)
std_income = np.std(income_data)

# Calculate 95% confidence interval
CI_lower = mean_income - 1.96 * std_income / np.sqrt(len(income_data))
CI_upper = mean_income + 1.96 * std_income / np.sqrt(len(income_data))

# Create plot
plt.bar(x=np.arange(len(income_data)), height=income_data)
plt.xlabel('Individual')
plt.ylabel('Income')
plt.title('Income Distribution')

# Add point estimate with error bars
plt.plot(0, mean_income, 'o', markersize=10, color='r')
plt.errorbar(0, mean_income, yerr=[CI_lower, CI_upper], fmt='o', capsize=5, color='r')

plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:14:37.201867,llama-3-8b-v2,v2,False
missing_uncertainty,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Simulate data
np.random.seed(0)
treatment_effect = np.random.normal(5, 2, 100)  # mean 5, std 2, sample size 100
control_effect = np.random.normal(3, 2, 100)  # mean 3, std 2, sample size 100

# Calculate treatment effect
treatment_effect_mean = np.mean(treatment_effect)
control_effect_mean = np.mean(control_effect)

# Plot the results
plt.scatter(range(100), treatment_effect)
plt.scatter(range(100), control_effect)
plt.plot([0, 100], [treatment_effect_mean, treatment_effect_mean], 'r--')
plt.plot([0, 100], [control_effect_mean, control_effect_mean], 'b--')
plt.xlabel('Patient ID')
plt.ylabel('Effect')
plt.title('Treatment Effect')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Simulate data
np.random.seed(0)
treatment_effect = np.random.normal(5, 2, 100)  # mean 5, std 2, sample size 100
control_effect = np.random.normal(3, 2, 100)  # mean 3, std 2, sample size 100

# Calculate treatment effect and CI
treatment_effect_mean = np.mean(treatment_effect)
treatment_effect_ci = np.percentile(treatment_effect, [2.5, 97.5])  # 95% CI

control_effect_mean = np.mean(control_effect)
control_effect_ci = np.percentile(control_effect, [2.5, 97.5])  # 95% CI

# Plot the results with error bars
plt.figure(figsize=(8, 6))
sns.boxplot(x=""group"", y=""effect"", data=pd.DataFrame({""group"": np.concatenate([np.ones(100), np.zeros(100)]), ""effect"": np.concatenate([treatment_effect, control_effect])}))
sns.lineplot(x=[0, 100], y=[treatment_effect_mean, treatment_effect_mean], color=""r"")
sns.lineplot(x=[0, 100], y=[control_effect_mean, control_effect_mean], color=""b"")
plt.fill_between(range(100), [treatment_effect_mean - (treatment_effect_ci[1] - treatment_effect_mean), treatment_effect_mean - (treatment_effect_ci[1] - treatment_effect_mean)], [treatment_effect_mean + (treatment_effect_ci[1] - treatment_effect_mean), treatment_effect_mean + (treatment_effect_ci[1] - treatment_effect_mean)], alpha=0.3, color=""r"")
plt.fill_between(range(100), [control_effect_mean - (control_effect_ci[1] - control_effect_mean), control_effect_mean - (control_effect_ci[1] - control_effect_mean)], [control_effect_mean + (control_effect_ci[1] - control_effect_mean), control_effect_mean + (control_effect_ci[1] - control_effect_mean)], alpha=0.3, color=""b"")
plt.xlabel('Patient ID')
plt.ylabel('Effect')
plt.title('Treatment Effect')
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:16:13.763682,llama-3-8b-v2,v2,False
missing_uncertainty,warning,healthcare,disease prevalence analysis across demographics,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
df = pd.read_csv('hospital_data.csv')

# Calculate the mean and standard deviation of treatment effectiveness
mean_effectiveness = df['treatment_effectiveness'].mean()
std_dev = df['treatment_effectiveness'].std()

# Create a bar chart with the point estimates
plt.figure(figsize=(8, 6))
sns.barplot(x='treatment', y=mean_effectiveness, data=df)
plt.title('Treatment Effectiveness')
plt.xlabel('Treatment')
plt.ylabel('Effectiveness')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Load the data
df = pd.read_csv('hospital_data.csv')

# Calculate the mean and standard deviation of treatment effectiveness
mean_effectiveness = df['treatment_effectiveness'].mean()
std_dev = df['treatment_effectiveness'].std()

# Create a bar chart with error bars representing the standard deviation
plt.figure(figsize=(8, 6))
sns.barplot(x='treatment', y=mean_effectiveness, yerr=std_dev, data=df)
plt.title('Treatment Effectiveness')
plt.xlabel('Treatment')
plt.ylabel('Effectiveness')
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:17:03.617370,llama-3-8b-v2,v2,False
missing_uncertainty,warning,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import linregress

# Load data
df = pd.read_csv('student_performance.csv')

# Define x and y variables
x = df['hours_studied']
y = df['grade']

# Fit linear regression model
slope, intercept, r_value, p_value, std_err = linregress(x, y)

# Plot the data
plt.scatter(x, y)
plt.plot(x, slope*x + intercept, 'r')
plt.xlabel('Hours Studied')
plt.ylabel('Grade')
plt.title('Student Performance')

# Show the plot
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import linregress
import seaborn as sns

# Load data
df = pd.read_csv('student_performance.csv')

# Define x and y variables
x = df['hours_studied']
y = df['grade']

# Fit linear regression model
slope, intercept, r_value, p_value, std_err = linregress(x, y)

# Calculate confidence interval for the slope
conf_int = slope * std_err

# Plot the data
sns.set()
plt.scatter(x, y)
plt.plot(x, slope*x + intercept, 'r')

# Add error bars to the regression line
plt.plot(x, slope*x + intercept, 'r', lw=2)
plt.fill_between(x, (slope*x + intercept - conf_int), (slope*x + intercept + conf_int), alpha=0.3)

# Add a legend
plt.legend(['Regression Line', '95% CI'])

# Add labels and title
plt.xlabel('Hours Studied')
plt.ylabel('Grade')
plt.title('Student Performance')

# Show the plot
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:18:09.016807,llama-3-8b-v2,v2,False
missing_uncertainty,warning,social_science,social media usage and mental health,python,intermediate,"import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Load the data
data = pd.read_csv('social_media_data.csv')

# Calculate the mean and standard deviation of social media usage
mean_usage = data['social_media_usage'].mean()
std_usage = data['social_media_usage'].std()

# Plot the data
plt.figure(figsize=(8, 6))
sns.barplot(x='variable', y='value', data=data)

# Add a title and labels
plt.title('Social Media Usage and Mental Health')
plt.xlabel('Social Media Usage')
plt.ylabel('Mean Usage')

# Show the plot
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Load the data
data = pd.read_csv('social_media_data.csv')

# Calculate the mean and standard deviation of social media usage
mean_usage = data['social_media_usage'].mean()
std_usage = data['social_media_usage'].std()

# Calculate the confidence interval for the mean
conf_int = np.percentile(data['social_media_usage'], [2.5, 97.5])

# Plot the data
plt.figure(figsize=(8, 6))
sns.barplot(x='variable', y='value', data=data)

# Add a title and labels
plt.title('Social Media Usage and Mental Health')
plt.xlabel('Social Media Usage')
plt.ylabel('Mean Usage')

# Add error bars for the confidence interval
plt.errorbar(x='variable', y=mean_usage, yerr=[conf_int[0], conf_int[1]], fmt='o')

# Show the plot
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:19:05.849764,llama-3-8b-v2,v2,False
missing_uncertainty,warning,business,sales performance across regions,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
np.random.seed(0)
n = 30
x = np.random.normal(0, 1, n)
y = 2 + 3*x + np.random.normal(0, 0.5, n)

# Calculate the mean and standard error of the estimate
mean_estimate = np.mean(y)
std_err = np.std(y) / np.sqrt(n)

# Create a plot
fig, ax = plt.subplots()
ax.plot(x, y, 'o')
ax.axhline(mean_estimate, color='r', linestyle='--')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import statsmodels.api as sm

# Generate some sample data
np.random.seed(0)
n = 30
x = np.random.normal(0, 1, n)
y = 2 + 3*x + np.random.normal(0, 0.5, n)

# Calculate the mean and standard error of the estimate
mean_estimate = np.mean(y)
std_err = np.std(y) / np.sqrt(n)

# Calculate the 95% confidence interval
CI = sm.stats.DescrStatsW(y).tconfint_mean

# Create a plot with error bars
fig, ax = plt.subplots()
ax.plot(x, y, 'o')
ax.axhline(mean_estimate, color='r', linestyle='--')
ax.fill_between([], [], mean_estimate + CI[1], alpha=0.3, color='r')
ax.fill_between([], [], mean_estimate + CI[0], alpha=0.3, color='r')
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:20:12.162134,llama-3-8b-v2,v2,False
missing_uncertainty,warning,business,sales performance across regions,python,intermediate,"import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
regions = ['North', 'South', 'East', 'West']
sales = [1000, 1200, 1500, 800]

# Plot the data
plt.bar(regions, sales)
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Sales Performance Across Regions')
plt.show()","{'summary': 'Missing Uncertainty Visualization', 'detailed_explanation': 'Missing Uncertainty Visualization: Point estimates without uncertainty measures overstate confidence', 'line_numbers': [10, 15], 'consequences': 'Could lead to incorrect conclusions'}","import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Generate some sample data with uncertainty
regions = ['North', 'South', 'East', 'West']
sales = [1000, 1200, 1500, 800]
uncertainty = [100, 50, 200, 100]

# Calculate the mean and standard deviation
mean_sales = np.mean(sales)
std_sales = np.std(sales)

# Plot the data with error bars
plt.bar(regions, sales, yerr=uncertainty)
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Sales Performance Across Regions')
plt.axhline(mean_sales, color='black', linestyle='--', label='Mean')
plt.legend()
plt.show()",['Point estimates without uncertainty measures overstate confidence'],Point estimates without uncertainty measures overstate confidence,2025-12-16T13:20:57.236237,llama-3-8b-v2,v2,False
none,none,healthcare,disease prevalence analysis across demographics,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('data.csv')

# Calculate the means and standard errors for the two groups
group1_mean = df['group1'].mean()
group2_mean = df['group2'].mean()
group1_se = df['group1'].std() / len(df['group1'] ** 0.5)
group2_se = df['group2'].std() / len(df['group2'] ** 0.5)

# Calculate the t-statistic and p-value
t_stat, p_val = ttest_ind(df['group1'], df['group2'], equal_var=False)

# Create a plot to visualize the results
plt.figure(figsize=(8, 6))
plt.bar(['Group 1', 'Group 2'], [group1_mean, group2_mean], yerr=[group1_se, group2_se])
plt.xlabel('Group')
plt.ylabel('Mean Response')
plt.title('Comparison of Group Means')
plt.show()

print(f'T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}')","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Code Explanation:**\n\nThis code demonstrates good statistical practices by:\n\n1. Using appropriate causal language: We use the term ""association"" instead of ""causality"" to describe the relationship between the two groups.\n2. Including uncertainty: We calculate the standard errors for the means and display them in the plot, acknowledging the uncertainty in the results.\n3. Acknowledging limitations: We do not make any claims about the cause-and-effect relationship between the two groups, and', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('data.csv')

# Calculate the means and standard errors for the two groups
group1_mean = df['group1'].mean()
group2_mean = df['group2'].mean()
group1_se = df['group1'].std() / len(df['group1'] ** 0.5)
group2_se = df['group2'].std() / len(df['group2'] ** 0.5)

# Calculate the t-statistic and p-value
t_stat, p_val = ttest_ind(df['group1'], df['group2'], equal_var=False)

# Create a plot to visualize the results
plt.figure(figsize=(8, 6))
plt.bar(['Group 1', 'Group 2'], [group1_mean, group2_mean], yerr=[group1_se, group2_se])
plt.xlabel('Group')
plt.ylabel('Mean Response')
plt.title('Comparison of Group Means')
plt.show()

print(f'T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}')",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:21:55.759079,llama-3-8b-v2,v2,True
none,none,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('hospital_data.csv')

# Define the outcome variable (response) and predictor variable (treatment)
response = 'length_of_stay'
predictor = 'treatment_group'

# Summary statistics and descriptive plots
print(f""Summary statistics for {response}: {df[response].describe()}"")
plt.hist(df[response], bins=10); plt.title(f""Histogram of {response}""); plt.show()

# Association analysis (correlation, not causation)
correlation_coefficient, p_value = np.corrcoef(df[predictor], df[response])[0, 1]
print(f""Correlation coefficient: {correlation_coefficient:.3f}, p-value: {p_value:.3f}"")

# Causal inference (t-test for independent samples)
t_stat, p_value = ttest_ind(df[predictor == 0][response], df[predictor == 1][response])
print(f""t-statistic: {t_stat:.3f}, p-value: {p_value:.3f}"")

# Confidence intervals for mean difference
ci_lower, ci_upper = np.percentile(df[predictor == 0][response] - df[predictor == 1][response], [2.5, 97.5])
print(f""Confidence interval for mean difference: [{ci_lower:.3f}, {ci_upper:.3f}]"")

# Limitations and confounders
print(""Limitations and confounders:"")
print(""* Self-reported data may be biased"")
print(""* Patient characteristics not controlled for (e.g., age, comorbidities)"")
print(""* Other factors may influence outcome (e.g., treatment duration, patient load)"")

# Professional but cautious interpretation
print(""In conclusion, the results suggest a statistically significant association between treatment group and length of stay, but we must exercise caution in interpreting this finding due to the limitations mentioned above."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected. The code demonstrates good statistical practices by:\n\n* Using appropriate causal language (association, not causation) and acknowledging limitations and confounders.\n* Including uncertainty through confidence intervals and p-values.\n* Providing a clear and concise summary of the results, avoiding over-interpretation.\n* Using descriptive plots to visualize the data and facilitate understanding.\n\nNote that this is just a simplified exa', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('hospital_data.csv')

# Define the outcome variable (response) and predictor variable (treatment)
response = 'length_of_stay'
predictor = 'treatment_group'

# Summary statistics and descriptive plots
print(f""Summary statistics for {response}: {df[response].describe()}"")
plt.hist(df[response], bins=10); plt.title(f""Histogram of {response}""); plt.show()

# Association analysis (correlation, not causation)
correlation_coefficient, p_value = np.corrcoef(df[predictor], df[response])[0, 1]
print(f""Correlation coefficient: {correlation_coefficient:.3f}, p-value: {p_value:.3f}"")

# Causal inference (t-test for independent samples)
t_stat, p_value = ttest_ind(df[predictor == 0][response], df[predictor == 1][response])
print(f""t-statistic: {t_stat:.3f}, p-value: {p_value:.3f}"")

# Confidence intervals for mean difference
ci_lower, ci_upper = np.percentile(df[predictor == 0][response] - df[predictor == 1][response], [2.5, 97.5])
print(f""Confidence interval for mean difference: [{ci_lower:.3f}, {ci_upper:.3f}]"")

# Limitations and confounders
print(""Limitations and confounders:"")
print(""* Self-reported data may be biased"")
print(""* Patient characteristics not controlled for (e.g., age, comorbidities)"")
print(""* Other factors may influence outcome (e.g., treatment duration, patient load)"")

# Professional but cautious interpretation
print(""In conclusion, the results suggest a statistically significant association between treatment group and length of stay, but we must exercise caution in interpreting this finding due to the limitations mentioned above."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:22:55.833321,llama-3-8b-v2,v2,True
none,none,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('patient_outcomes.csv')

# Explore the data
print(df.describe())

# Visualize the relationship between variables
sns.pairplot(df[['outcome', 'treatment', 'age','sex'])
plt.show()

# Perform a two-sample t-test to compare means
t_stat, p_val = ttest_ind(df['outcome'][df['treatment'] == 'control'], 
                            df['outcome'][df['treatment'] == 'treatment'])

print(f""T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")

# Calculate the confidence interval for the mean difference
CI = t.cdf(t_stat, len(df) - 1) * 2

print(f""Confidence interval: ({CI[0]:.2f}, {CI[1]:.2f})"")

# Acknowledge limitations
print(""Note: This analysis assumes that the treatment and control groups are independent and that there are no confounding variables."")
print(""In reality, there may be other factors that influence the outcome variable."")

# Interpret the results
print(""Based on the t-test, we find a statistically significant difference in the mean outcome between the treatment and control groups."")
print(""However, we must be cautious when interpreting this result, as it may not necessarily mean that the treatment causes the observed effect."")
print(""Future studies should consider controlling for confounding variables and using a more robust statistical approach."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**BRIEF REVIEW**\n\nNo major statistical issues detected in this code. Two good practices observed:\n\n1. The code uses proper statistical language, avoiding causal language and instead highlighting the association between variables.\n2. The code includes uncertainty in the form of confidence intervals and p-values, acknowledging the limitations of the analysis.\n\nNote that this code is just an example and may need to be modified to fit the specific needs of the study. Additionally, a real-world', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('patient_outcomes.csv')

# Explore the data
print(df.describe())

# Visualize the relationship between variables
sns.pairplot(df[['outcome', 'treatment', 'age','sex'])
plt.show()

# Perform a two-sample t-test to compare means
t_stat, p_val = ttest_ind(df['outcome'][df['treatment'] == 'control'], 
                            df['outcome'][df['treatment'] == 'treatment'])

print(f""T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")

# Calculate the confidence interval for the mean difference
CI = t.cdf(t_stat, len(df) - 1) * 2

print(f""Confidence interval: ({CI[0]:.2f}, {CI[1]:.2f})"")

# Acknowledge limitations
print(""Note: This analysis assumes that the treatment and control groups are independent and that there are no confounding variables."")
print(""In reality, there may be other factors that influence the outcome variable."")

# Interpret the results
print(""Based on the t-test, we find a statistically significant difference in the mean outcome between the treatment and control groups."")
print(""However, we must be cautious when interpreting this result, as it may not necessarily mean that the treatment causes the observed effect."")
print(""Future studies should consider controlling for confounding variables and using a more robust statistical approach."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:23:46.274514,llama-3-8b-v2,v2,True
none,none,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('hospital_data.csv')

# Define the outcome variable and treatment group
outcome_var ='survival_rate'
treatment_group = 'new_treatment'

# Calculate the means and standard errors for each group
group1_mean = np.mean(df.loc[df[treatment_group] == 0, outcome_var])
group2_mean = np.mean(df.loc[df[treatment_group] == 1, outcome_var])
group1_sem = np.std(df.loc[df[treatment_group] == 0, outcome_var]) / np.sqrt(len(df.loc[df[treatment_group] == 0, outcome_var]))
group2_sem = np.std(df.loc[df[treatment_group] == 1, outcome_var]) / np.sqrt(len(df.loc[df[treatment_group] == 1, outcome_var]))

# Perform a two-sample t-test to compare the means
t_stat, p_val = ttest_ind(df.loc[df[treatment_group] == 0, outcome_var], df.loc[df[treatment_group] == 1, outcome_var])

# Calculate the 95% confidence interval for the difference in means
diff_mean_ci = np.mean([group1_mean - group2_mean, group1_mean + group2_mean])

# Plot the results
plt.bar(['Group 1', 'Group 2'], [group1_mean, group2_mean])
plt.errorbar(['Group 1', 'Group 2'], [group1_mean, group2_mean], [group1_sem, group2_sem])
plt.xlabel('Treatment Group')
plt.ylabel('Survival Rate')
plt.title(f'Treatment Effect: {t_stat:.2f} ({p_val:.2f})')
plt.show()

print(f'The difference in means is {diff_mean_ci:.2f} ({p_val:.2f})')","{'summary': 'No major statistical issues detected', 'detailed_explanation': ""```\n\n**Code Commentary:**\n\nThis code demonstrates good statistical practices in the context of a hospital system, where we're evaluating the effectiveness of a new treatment. We use proper causal language, acknowledging that the results only show an association between the treatment and the outcome, and not a direct cause-and-effect relationship.\n\nWe include uncertainty by calculating the standard errors and confidence intervals for the difference in means, and we plot the results with error ba"", 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('hospital_data.csv')

# Define the outcome variable and treatment group
outcome_var ='survival_rate'
treatment_group = 'new_treatment'

# Calculate the means and standard errors for each group
group1_mean = np.mean(df.loc[df[treatment_group] == 0, outcome_var])
group2_mean = np.mean(df.loc[df[treatment_group] == 1, outcome_var])
group1_sem = np.std(df.loc[df[treatment_group] == 0, outcome_var]) / np.sqrt(len(df.loc[df[treatment_group] == 0, outcome_var]))
group2_sem = np.std(df.loc[df[treatment_group] == 1, outcome_var]) / np.sqrt(len(df.loc[df[treatment_group] == 1, outcome_var]))

# Perform a two-sample t-test to compare the means
t_stat, p_val = ttest_ind(df.loc[df[treatment_group] == 0, outcome_var], df.loc[df[treatment_group] == 1, outcome_var])

# Calculate the 95% confidence interval for the difference in means
diff_mean_ci = np.mean([group1_mean - group2_mean, group1_mean + group2_mean])

# Plot the results
plt.bar(['Group 1', 'Group 2'], [group1_mean, group2_mean])
plt.errorbar(['Group 1', 'Group 2'], [group1_mean, group2_mean], [group1_sem, group2_sem])
plt.xlabel('Treatment Group')
plt.ylabel('Survival Rate')
plt.title(f'Treatment Effect: {t_stat:.2f} ({p_val:.2f})')
plt.show()

print(f'The difference in means is {diff_mean_ci:.2f} ({p_val:.2f})')",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:24:56.630150,llama-3-8b-v2,v2,True
none,none,healthcare,observational study of patient outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the dataset
df = pd.read_csv('data.csv')

# Define the demographics and disease prevalence variables
demographics = ['age','sex', 'race']
disease_prevalence = 'disease_prevalence'

# Calculate the correlation between demographics and disease prevalence
correlation_matrix = df[demographics].corr()
print(""Correlation Matrix:"")
print(correlation_matrix)

# Perform a t-test to determine if there is a significant association between age and disease prevalence
t_stat, p_val = ttest_ind(df[demographics[0]], df[disease_prevalence])
print(f""t-statistic: {t_stat}, p-value: {p_val}"")

# Visualize the relationship between age and disease prevalence
plt.scatter(df[demographics[0]], df[disease_prevalence])
plt.xlabel('Age')
plt.ylabel('Disease Prevalence')
plt.title('Association between Age and Disease Prevalence')
plt.show()

# Calculate the confidence interval for the mean disease prevalence by age group
age_groups = df[demographics[0]].unique()
confidence_intervals = []
for age_group in age_groups:
    group_df = df[df[demographics[0]] == age_group]
    mean_prevalence = group_df[disease_prevalence].mean()
    ci = t.interval(0.95, len(group_df) - 1, mean_prevalence, group_df[disease_prevalence].std() / len(group_df)**0.5)
    confidence_intervals.append((age_group, ci))

print(""Confidence Intervals:"")
print(confidence_intervals)","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues were detected in the code example. The following good practices were observed:\n\n1. **Causal language**: The code uses appropriate causal language, such as ""association"" and ""correlation"", to describe the relationships between variables, rather than implying causality.\n2. **Uncertainty quantification**: The code includes uncertainty quantification through the use of confidence intervals and p-values, which provide a sense of the precision and s', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the dataset
df = pd.read_csv('data.csv')

# Define the demographics and disease prevalence variables
demographics = ['age','sex', 'race']
disease_prevalence = 'disease_prevalence'

# Calculate the correlation between demographics and disease prevalence
correlation_matrix = df[demographics].corr()
print(""Correlation Matrix:"")
print(correlation_matrix)

# Perform a t-test to determine if there is a significant association between age and disease prevalence
t_stat, p_val = ttest_ind(df[demographics[0]], df[disease_prevalence])
print(f""t-statistic: {t_stat}, p-value: {p_val}"")

# Visualize the relationship between age and disease prevalence
plt.scatter(df[demographics[0]], df[disease_prevalence])
plt.xlabel('Age')
plt.ylabel('Disease Prevalence')
plt.title('Association between Age and Disease Prevalence')
plt.show()

# Calculate the confidence interval for the mean disease prevalence by age group
age_groups = df[demographics[0]].unique()
confidence_intervals = []
for age_group in age_groups:
    group_df = df[df[demographics[0]] == age_group]
    mean_prevalence = group_df[disease_prevalence].mean()
    ci = t.interval(0.95, len(group_df) - 1, mean_prevalence, group_df[disease_prevalence].std() / len(group_df)**0.5)
    confidence_intervals.append((age_group, ci))

print(""Confidence Intervals:"")
print(confidence_intervals)",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:25:57.446210,llama-3-8b-v2,v2,True
none,none,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('hospital_data.csv')

# Define outcome variable (treatment effectiveness) and treatment group
outcome = 'length_of_stay'
treatment = 'treatment_group'

# Calculate descriptive statistics
print(""Descriptive statistics:"")
print(df.groupby(treatment)[outcome].describe())

# Perform two-sample t-test for independent samples
t_stat, p_val = ttest_ind(df.loc[df[treatment] == 'treatment'], 
                           df.loc[df[treatment] == 'control'], equal_var=False)
print(f""Two-sample t-test: t-statistic = {t_stat}, p-value = {p_val:.4f}"")

# Calculate confidence intervals for the mean difference
ci = np.percentile(df[outcome] - df[outcome].mean(), [2.5, 97.5])
print(f""95% CI for mean difference: [{ci[0]:.2f}, {ci[1]:.2f}]"")

# Acknowledge limitations
print(""Note: The analysis assumes that the treatment group and control group are independent, and that the outcome variable is normally distributed."")
print(""However, in practice, there may be confounding variables that affect the treatment effectiveness."")

# Professional but cautious interpretation
print(""Based on the analysis, we can conclude that there is a statistically significant difference in treatment effectiveness between the treatment and control groups (p-value = {:.4f})."".format(p_val))
print(""However, we must be cautious when interpreting this result, as the analysis does not account for potential confounding variables."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**BRIEF REVIEW**\n\nNo major statistical issues detected.\n\nThe code demonstrates good statistical practices by:\n\n1. Using appropriate causal language: The code does not claim that the treatment causes the outcome, but rather reports the statistical association between the treatment and the outcome.\n2. Including uncertainty: The code provides confidence intervals for the mean difference and a p-value for the two-sample t-test, which gives an idea of the uncertainty associated with the estimat', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('hospital_data.csv')

# Define outcome variable (treatment effectiveness) and treatment group
outcome = 'length_of_stay'
treatment = 'treatment_group'

# Calculate descriptive statistics
print(""Descriptive statistics:"")
print(df.groupby(treatment)[outcome].describe())

# Perform two-sample t-test for independent samples
t_stat, p_val = ttest_ind(df.loc[df[treatment] == 'treatment'], 
                           df.loc[df[treatment] == 'control'], equal_var=False)
print(f""Two-sample t-test: t-statistic = {t_stat}, p-value = {p_val:.4f}"")

# Calculate confidence intervals for the mean difference
ci = np.percentile(df[outcome] - df[outcome].mean(), [2.5, 97.5])
print(f""95% CI for mean difference: [{ci[0]:.2f}, {ci[1]:.2f}]"")

# Acknowledge limitations
print(""Note: The analysis assumes that the treatment group and control group are independent, and that the outcome variable is normally distributed."")
print(""However, in practice, there may be confounding variables that affect the treatment effectiveness."")

# Professional but cautious interpretation
print(""Based on the analysis, we can conclude that there is a statistically significant difference in treatment effectiveness between the treatment and control groups (p-value = {:.4f})."".format(p_val))
print(""However, we must be cautious when interpreting this result, as the analysis does not account for potential confounding variables."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:26:50.288260,llama-3-8b-v2,v2,True
none,none,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('hospital_data.csv')

# Define outcome variable and treatment group
outcome_var = 'length_of_stay'
treatment_group = 'treatment_group'

# Calculate descriptive statistics
print(""Descriptive statistics:"")
print(df.groupby(treatment_group)[outcome_var].describe())

# Calculate association between treatment group and outcome variable
pearson_corr, _ = np.polyfit(df[treatment_group], df[outcome_var], 1)
print(f""Pearson correlation coefficient: {pearson_corr:.3f}"")

# Perform two-sample t-test to compare treatment groups
t_stat, p_val = ttest_ind(df.loc[df[treatment_group] == 1, outcome_var], 
                           df.loc[df[treatment_group] == 0, outcome_var])
print(f""Two-sample t-test p-value: {p_val:.3f}"")

# Visualize data
plt.scatter(df[treatment_group], df[outcome_var])
plt.xlabel(treatment_group)
plt.ylabel(outcome_var)
plt.title(f""{treatment_group} and {outcome_var} Association"")
plt.show()

# Interpret results
print(""Results:"")
print(f""The Pearson correlation coefficient suggests a strong association between {treatment_group} and {outcome_var} ({pearson_corr:.3f})."")
print(f""The two-sample t-test indicates a statistically significant difference between treatment groups (p-value: {p_val:.3f})."")
print(""However, it is essential to acknowledge that this analysis is observational and does not establish causality."")
print(""Future studies should consider controlling for potential confounders, such as patient demographics and comorbidities."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\nNo major statistical issues detected in this code. The following good practices were observed:\n\n* The code uses clear and concise variable names, making it easy to understand the analysis.\n* The code calculates descriptive statistics to provide context for the analysis.\n* The code performs a two-sample t-test to compare treatment groups, which is an appropriate statistical test for this problem.\n* The code includes uncertainty in the results by reporting p-values and conf', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('hospital_data.csv')

# Define outcome variable and treatment group
outcome_var = 'length_of_stay'
treatment_group = 'treatment_group'

# Calculate descriptive statistics
print(""Descriptive statistics:"")
print(df.groupby(treatment_group)[outcome_var].describe())

# Calculate association between treatment group and outcome variable
pearson_corr, _ = np.polyfit(df[treatment_group], df[outcome_var], 1)
print(f""Pearson correlation coefficient: {pearson_corr:.3f}"")

# Perform two-sample t-test to compare treatment groups
t_stat, p_val = ttest_ind(df.loc[df[treatment_group] == 1, outcome_var], 
                           df.loc[df[treatment_group] == 0, outcome_var])
print(f""Two-sample t-test p-value: {p_val:.3f}"")

# Visualize data
plt.scatter(df[treatment_group], df[outcome_var])
plt.xlabel(treatment_group)
plt.ylabel(outcome_var)
plt.title(f""{treatment_group} and {outcome_var} Association"")
plt.show()

# Interpret results
print(""Results:"")
print(f""The Pearson correlation coefficient suggests a strong association between {treatment_group} and {outcome_var} ({pearson_corr:.3f})."")
print(f""The two-sample t-test indicates a statistically significant difference between treatment groups (p-value: {p_val:.3f})."")
print(""However, it is essential to acknowledge that this analysis is observational and does not establish causality."")
print(""Future studies should consider controlling for potential confounders, such as patient demographics and comorbidities."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:27:45.485415,llama-3-8b-v2,v2,True
none,none,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load dataset
df = pd.read_csv('hospital_data.csv')

# Calculate treatment effect on patient outcomes
treatment_group = df[df['treatment'] == 'new_treatment']
control_group = df[df['treatment'] =='standard_treatment']

# Calculate mean outcomes for each group
treatment_mean = treatment_group['outcome'].mean()
control_mean = control_group['outcome'].mean()

# Calculate standard error and confidence interval
treatment_se = treatment_mean * (1 - treatment_mean) / treatment_group.shape[0]
control_se = control_mean * (1 - control_mean) / control_group.shape[0]
treatment_ci = ttest_ind(treatment_mean, control_mean, equal_var=False)[1]
control_ci = ttest_ind(treatment_mean, control_mean, equal_var=False)[1]

# Calculate p-value
p_value = ttest_ind(treatment_mean, control_mean, equal_var=False)[0]

print(f""Treatment effect: {treatment_mean:.2f}  {treatment_se:.2f} ({treatment_ci[0]:.2f}, {treatment_ci[1]:.2f})"")
print(f""Control effect: {control_mean:.2f}  {control_se:.2f} ({control_ci[0]:.2f}, {control_ci[1]:.2f})"")
print(f""P-value: {p_value:.4f}"")

# Plot the data
sns.set()
plt.figure(figsize=(8, 6))
sns.barplot(x='treatment', y='outcome', data=df)
plt.xlabel('Treatment')
plt.ylabel('Outcome')
plt.title('Treatment Effect on Patient Outcomes')
plt.show()","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n**Code Commentary:**\n\n* The code uses proper statistical language, avoiding claims of causality and instead focusing on associations between the treatment and outcome variables.\n* The code includes uncertainty estimates in the form of standard errors and confidence intervals for both treatment and control groups.\n* The code acknowledges limitations by not over-interpreting the results and by not controlling for potential confounders.\n* The code uses a professional tone and cautious language', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load dataset
df = pd.read_csv('hospital_data.csv')

# Calculate treatment effect on patient outcomes
treatment_group = df[df['treatment'] == 'new_treatment']
control_group = df[df['treatment'] =='standard_treatment']

# Calculate mean outcomes for each group
treatment_mean = treatment_group['outcome'].mean()
control_mean = control_group['outcome'].mean()

# Calculate standard error and confidence interval
treatment_se = treatment_mean * (1 - treatment_mean) / treatment_group.shape[0]
control_se = control_mean * (1 - control_mean) / control_group.shape[0]
treatment_ci = ttest_ind(treatment_mean, control_mean, equal_var=False)[1]
control_ci = ttest_ind(treatment_mean, control_mean, equal_var=False)[1]

# Calculate p-value
p_value = ttest_ind(treatment_mean, control_mean, equal_var=False)[0]

print(f""Treatment effect: {treatment_mean:.2f}  {treatment_se:.2f} ({treatment_ci[0]:.2f}, {treatment_ci[1]:.2f})"")
print(f""Control effect: {control_mean:.2f}  {control_se:.2f} ({control_ci[0]:.2f}, {control_ci[1]:.2f})"")
print(f""P-value: {p_value:.4f}"")

# Plot the data
sns.set()
plt.figure(figsize=(8, 6))
sns.barplot(x='treatment', y='outcome', data=df)
plt.xlabel('Treatment')
plt.ylabel('Outcome')
plt.title('Treatment Effect on Patient Outcomes')
plt.show()",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:28:50.809044,llama-3-8b-v2,v2,True
none,none,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('disease_prevalence_data.csv')

# Calculate the proportion of disease prevalence across demographics
demographics = ['age_group','sex']
disease_prevalence = df.groupby(demographics)['disease'].mean()

# Perform a two-sample t-test to compare disease prevalence between age groups
t_stat, p_val = ttest_ind(disease_prevalence['age_group_18-24'], disease_prevalence['age_group_25-34'], equal_var=False)

# Calculate the confidence interval for the difference in disease prevalence
conf_int = t.cdf(t_stat, df=(len(disease_prevalence['age_group_18-24']) - 1), loc=0, scale=1) * 100

# Plot the results
plt.bar(disease_prevalence.index, disease_prevalence.values, yerr=conf_int)
plt.xlabel('Age Group')
plt.ylabel('Disease Prevalence (%)')
plt.title('Disease Prevalence by Age Group')
plt.show()

# Interpret the results
print(f""The difference in disease prevalence between age groups 18-24 and 25-34 is {t_stat:.2f} with a p-value of {p_val:.4f}."")
print(f""The 95% confidence interval for this difference is [{conf_int[0]:.2f}, {conf_int[1]:.2f}]."")
print(""While the results suggest a statistically significant association between age group and disease prevalence, it is important to note that this analysis does not establish causality."")
print(""Additionally, this analysis does not account for potential confounders such as smoking status or medication use."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**BRIEF REVIEW**\n\nNo major statistical issues detected in this code. The following good practices were observed:\n\n* The code uses appropriate causal language, noting that the analysis does not establish causality between age group and disease prevalence.\n* The code includes uncertainty in the form of confidence intervals and a p-value, providing a sense of the precision of the estimated difference in disease prevalence.\n* The code acknowledges limitations, noting that the analysis does not', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('disease_prevalence_data.csv')

# Calculate the proportion of disease prevalence across demographics
demographics = ['age_group','sex']
disease_prevalence = df.groupby(demographics)['disease'].mean()

# Perform a two-sample t-test to compare disease prevalence between age groups
t_stat, p_val = ttest_ind(disease_prevalence['age_group_18-24'], disease_prevalence['age_group_25-34'], equal_var=False)

# Calculate the confidence interval for the difference in disease prevalence
conf_int = t.cdf(t_stat, df=(len(disease_prevalence['age_group_18-24']) - 1), loc=0, scale=1) * 100

# Plot the results
plt.bar(disease_prevalence.index, disease_prevalence.values, yerr=conf_int)
plt.xlabel('Age Group')
plt.ylabel('Disease Prevalence (%)')
plt.title('Disease Prevalence by Age Group')
plt.show()

# Interpret the results
print(f""The difference in disease prevalence between age groups 18-24 and 25-34 is {t_stat:.2f} with a p-value of {p_val:.4f}."")
print(f""The 95% confidence interval for this difference is [{conf_int[0]:.2f}, {conf_int[1]:.2f}]."")
print(""While the results suggest a statistically significant association between age group and disease prevalence, it is important to note that this analysis does not establish causality."")
print(""Additionally, this analysis does not account for potential confounders such as smoking status or medication use."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:29:48.174508,llama-3-8b-v2,v2,True
none,none,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('hospital_data.csv')

# Define the outcome variable and treatment group
outcome_var ='survival_rate'
treatment_group = 'treatment_group'

# Calculate the mean survival rate for each group
group1_mean = df[df[treatment_group] == 'control'].groupby(outcome_var).mean().reset_index()
group2_mean = df[df[treatment_group] == 'treatment'].groupby(outcome_var).mean().reset_index()

# Perform a two-sample t-test to compare the means
t_stat, p_val = ttest_ind(group1_mean[outcome_var], group2_mean[outcome_var], equal_var=False)

print(f""t-statistic: {t_stat:.2f}"")
print(f""p-value: {p_val:.4f}"")

# Calculate the confidence interval for the difference in means
diff_ci = t.cdf(t_stat, len(group1_mean) - 1) * (1 - t.cdf(t_stat, len(group2_mean) - 1))

print(f""95% CI for the difference in means: ({diff_ci[0]:.2f}, {diff_ci[1]:.2f})"")

# Visualize the results
plt.figure(figsize=(8, 6))
sns.barplot(x=outcome_var, y='mean', hue=treatment_group, data=df)
plt.xlabel(outcome_var)
plt.ylabel('Mean Survival Rate')
plt.title('Survival Rate by Treatment Group')
plt.show()

# Acknowledge limitations
print(""Limitations: The sample size is relatively small, and there may be confounding variables present."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected. The code demonstrates good statistical practices by:\n\n* Using appropriate causal language, such as ""correlation"" and ""association"" instead of ""causes"".\n* Including uncertainty through the calculation of confidence intervals and the reporting of p-values.\n* Acknowledging limitations and potential confounders in the interpretation of the results.\n* Providing a clear and concise summary of the findings.\n\nThe code also demonstrates good', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('hospital_data.csv')

# Define the outcome variable and treatment group
outcome_var ='survival_rate'
treatment_group = 'treatment_group'

# Calculate the mean survival rate for each group
group1_mean = df[df[treatment_group] == 'control'].groupby(outcome_var).mean().reset_index()
group2_mean = df[df[treatment_group] == 'treatment'].groupby(outcome_var).mean().reset_index()

# Perform a two-sample t-test to compare the means
t_stat, p_val = ttest_ind(group1_mean[outcome_var], group2_mean[outcome_var], equal_var=False)

print(f""t-statistic: {t_stat:.2f}"")
print(f""p-value: {p_val:.4f}"")

# Calculate the confidence interval for the difference in means
diff_ci = t.cdf(t_stat, len(group1_mean) - 1) * (1 - t.cdf(t_stat, len(group2_mean) - 1))

print(f""95% CI for the difference in means: ({diff_ci[0]:.2f}, {diff_ci[1]:.2f})"")

# Visualize the results
plt.figure(figsize=(8, 6))
sns.barplot(x=outcome_var, y='mean', hue=treatment_group, data=df)
plt.xlabel(outcome_var)
plt.ylabel('Mean Survival Rate')
plt.title('Survival Rate by Treatment Group')
plt.show()

# Acknowledge limitations
print(""Limitations: The sample size is relatively small, and there may be confounding variables present."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:30:42.709684,llama-3-8b-v2,v2,True
none,none,healthcare,treatment effectiveness in hospital system,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load data
df = pd.read_csv('data.csv')

# Merge demographic data with disease prevalence data
merged_df = pd.merge(df, demographics_data, on='patient_id')

# Calculate prevalence rates by demographic group
prevalence_rates = merged_df.groupby(['demographic_group', 'disease_status']).size().reset_index(name='count')
prevalence_rates['rate'] = prevalence_rates['count'] / prevalence_rates.groupby('demographic_group')['count'].transform('sum')

# Calculate correlation between demographic group and disease prevalence
correlation, p_value = pearsonr(merged_df['demographic_group'], merged_df['disease_status'])

# Visualize results
sns.barplot(x='demographic_group', y='rate', hue='disease_status', data=prevalence_rates)
plt.errorbar(x='demographic_group', y='rate', xerr=0.05, yerr=0.01, data=prevalence_rates)

# Print results
print(f""Correlation coefficient: {correlation:.3f} (p-value: {p_value:.4f})"")
print(""Prevalence rates by demographic group:"")
print(prevalence_rates.groupby('demographic_group').mean()['rate'].round(3))","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n**Code Commentary:**\n\n* We use the `pearsonr` function from `scipy.stats` to calculate the correlation between the demographic group and disease prevalence, and report the correlation coefficient and p-value.\n* We use the `errorbar` function from `matplotlib` to visualize the prevalence rates with error bars representing the standard error of the mean.\n* We report the results with caution, acknowledging the limitations of our analysis and the potential confounders that may have influenced o', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load data
df = pd.read_csv('data.csv')

# Merge demographic data with disease prevalence data
merged_df = pd.merge(df, demographics_data, on='patient_id')

# Calculate prevalence rates by demographic group
prevalence_rates = merged_df.groupby(['demographic_group', 'disease_status']).size().reset_index(name='count')
prevalence_rates['rate'] = prevalence_rates['count'] / prevalence_rates.groupby('demographic_group')['count'].transform('sum')

# Calculate correlation between demographic group and disease prevalence
correlation, p_value = pearsonr(merged_df['demographic_group'], merged_df['disease_status'])

# Visualize results
sns.barplot(x='demographic_group', y='rate', hue='disease_status', data=prevalence_rates)
plt.errorbar(x='demographic_group', y='rate', xerr=0.05, yerr=0.01, data=prevalence_rates)

# Print results
print(f""Correlation coefficient: {correlation:.3f} (p-value: {p_value:.4f})"")
print(""Prevalence rates by demographic group:"")
print(prevalence_rates.groupby('demographic_group').mean()['rate'].round(3))",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:31:41.611723,llama-3-8b-v2,v2,True
none,none,healthcare,clinical trial comparing drug efficacy,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('hospital_data.csv')

# Define treatment and outcome variables
treatment = df['treatment']
outcome = df['outcome']

# Calculate descriptive statistics
print(""Descriptive Statistics:"")
print(""Mean outcome for treatment group:"", treatment[treatment == 1].mean())
print(""Mean outcome for control group:"", treatment[treatment == 0].mean())

# Calculate confidence intervals for mean outcomes
CI_treatment = pd.tconfint_mean(treatment[treatment == 1], axis=None)
CI_control = pd.tconfint_mean(treatment[treatment == 0], axis=None)

# Perform two-sample t-test
t_stat, p_val = ttest_ind(treatment[treatment == 1], treatment[treatment == 0])

# Plot the data with confidence intervals
plt.figure(figsize=(8, 6))
plt.hist(treatment[treatment == 1], alpha=0.5, label='Treatment Group', density=True)
plt.hist(treatment[treatment == 0], alpha=0.5, label='Control Group', density=True)
plt.xlabel('Outcome')
plt.ylabel('Frequency')
plt.title('Histogram of Outcome by Treatment Group')
plt.legend()
plt.show()

print(""Confidence Intervals:"")
print(""Treatment Group:"", CI_treatment)
print(""Control Group:"", CI_control)
print(""p-value:"", p_val)

# Interpretation
print(""Based on the data, there is a statistically significant difference in the mean outcomes between the treatment and control groups (p ="", p_val, ""). However, we must acknowledge that this result may be influenced by confounding variables, such as patient demographics or disease severity. Future studies should consider controlling for these factors to establish a more robust conclusion."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected in this code example. The code demonstrates good practices such as:\n\n* Using appropriate causal language, stating that the result is an association and not a cause-and-effect relationship.\n* Including uncertainty through confidence intervals and a p-value.\n* Acknowledging limitations by mentioning the potential influence of confounding variables.\n\nThese practices ensure that the results are presented in a transparent and unbiased mann', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('hospital_data.csv')

# Define treatment and outcome variables
treatment = df['treatment']
outcome = df['outcome']

# Calculate descriptive statistics
print(""Descriptive Statistics:"")
print(""Mean outcome for treatment group:"", treatment[treatment == 1].mean())
print(""Mean outcome for control group:"", treatment[treatment == 0].mean())

# Calculate confidence intervals for mean outcomes
CI_treatment = pd.tconfint_mean(treatment[treatment == 1], axis=None)
CI_control = pd.tconfint_mean(treatment[treatment == 0], axis=None)

# Perform two-sample t-test
t_stat, p_val = ttest_ind(treatment[treatment == 1], treatment[treatment == 0])

# Plot the data with confidence intervals
plt.figure(figsize=(8, 6))
plt.hist(treatment[treatment == 1], alpha=0.5, label='Treatment Group', density=True)
plt.hist(treatment[treatment == 0], alpha=0.5, label='Control Group', density=True)
plt.xlabel('Outcome')
plt.ylabel('Frequency')
plt.title('Histogram of Outcome by Treatment Group')
plt.legend()
plt.show()

print(""Confidence Intervals:"")
print(""Treatment Group:"", CI_treatment)
print(""Control Group:"", CI_control)
print(""p-value:"", p_val)

# Interpretation
print(""Based on the data, there is a statistically significant difference in the mean outcomes between the treatment and control groups (p ="", p_val, ""). However, we must acknowledge that this result may be influenced by confounding variables, such as patient demographics or disease severity. Future studies should consider controlling for these factors to establish a more robust conclusion."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:32:36.300707,llama-3-8b-v2,v2,True
none,none,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load sales data
df = pd.read_csv('sales_data.csv')

# Calculate correlation between region and sales
region_sales_corr, region_sales_pval = pearsonr(df['Region'], df['Sales'])

print(f""Correlation between region and sales: {region_sales_corr:.2f} (p-value: {region_sales_pval:.4f})"")

# Plot the data
plt.scatter(df['Region'], df['Sales'])
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Association between Region and Sales')
plt.show()

# Calculate confidence interval for the correlation
region_sales_ci = np.around(1.96 * region_sales_corr * np.sqrt((1 - region_sales_corr**2) / (df.shape[0] - 2)), 4)
print(f""95% confidence interval for the correlation: ({region_sales_ci[0]:.4f}, {region_sales_ci[1]:.4f})"")

# Acknowledge limitations
print(""Note: This analysis assumes a linear relationship between region and sales, and does not account for potential confounding variables such as seasonality or marketing campaigns."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected in this code example. The following good practices were observed:\n\n* The code uses the correct statistical terminology, avoiding causal language and instead describing the correlation between region and sales as an association.\n* The code includes uncertainty in the form of a p-value and a confidence interval for the correlation, allowing for a more nuanced interpretation of the results.\n* The code acknowledges the limitations of the', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load sales data
df = pd.read_csv('sales_data.csv')

# Calculate correlation between region and sales
region_sales_corr, region_sales_pval = pearsonr(df['Region'], df['Sales'])

print(f""Correlation between region and sales: {region_sales_corr:.2f} (p-value: {region_sales_pval:.4f})"")

# Plot the data
plt.scatter(df['Region'], df['Sales'])
plt.xlabel('Region')
plt.ylabel('Sales')
plt.title('Association between Region and Sales')
plt.show()

# Calculate confidence interval for the correlation
region_sales_ci = np.around(1.96 * region_sales_corr * np.sqrt((1 - region_sales_corr**2) / (df.shape[0] - 2)), 4)
print(f""95% confidence interval for the correlation: ({region_sales_ci[0]:.4f}, {region_sales_ci[1]:.4f})"")

# Acknowledge limitations
print(""Note: This analysis assumes a linear relationship between region and sales, and does not account for potential confounding variables such as seasonality or marketing campaigns."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:33:21.747753,llama-3-8b-v2,v2,True
none,none,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load data
df = pd.read_csv('pricing_data.csv')

# Explore data
print(df.head())
print(df.describe())

# Check for missing values
print(df.isnull().sum())

# Visualize distribution of prices
plt.figure(figsize=(8, 6))
sns.distplot(df['price'], kde=False)
plt.title('Distribution of Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

# Analyze relationship between features
corr_matrix = df.corr()
print(corr_matrix)

# Identify significant correlations
significant_corrs = corr_matrix[corr_matrix > 0.5]
print(significant_corrs)

# Analyze relationship between 'price' and 'feature1'
feature1_corr, feature1_pval = pearsonr(df['price'], df['feature1'])
print(f""Correlation between 'price' and 'feature1': {feature1_corr:.3f} (p-value: {feature1_pval:.3f})"")

# Visualize relationship between 'price' and 'feature1'
plt.figure(figsize=(8, 6))
sns.scatterplot(x='feature1', y='price', data=df)
plt.title('Relationship between ""feature1"" and ""price""')
plt.xlabel('feature1')
plt.ylabel('Price')
plt.show()

# Interpret results
print(""No major statistical issues detected. The analysis suggests a moderate positive correlation between 'feature1' and 'price' (r = {:.3f}, p-value = {:.3f})."".format(feature1_corr, feature1_pval))","{'summary': 'No major statistical issues detected', 'detailed_explanation': ""```\n\n**BRIEF REVIEW**\n\nNo major statistical issues detected in this code. Two good practices observed include:\n\n* The use of descriptive statistics and visualizations to explore the data distribution and identify correlations, which helps to establish a baseline understanding of the data.\n* The application of a significance test (Pearson's r) to quantify the strength and uncertainty of the relationship between 'price' and 'feature1', acknowledging the limitations of correlation and the importan"", 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load data
df = pd.read_csv('pricing_data.csv')

# Explore data
print(df.head())
print(df.describe())

# Check for missing values
print(df.isnull().sum())

# Visualize distribution of prices
plt.figure(figsize=(8, 6))
sns.distplot(df['price'], kde=False)
plt.title('Distribution of Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

# Analyze relationship between features
corr_matrix = df.corr()
print(corr_matrix)

# Identify significant correlations
significant_corrs = corr_matrix[corr_matrix > 0.5]
print(significant_corrs)

# Analyze relationship between 'price' and 'feature1'
feature1_corr, feature1_pval = pearsonr(df['price'], df['feature1'])
print(f""Correlation between 'price' and 'feature1': {feature1_corr:.3f} (p-value: {feature1_pval:.3f})"")

# Visualize relationship between 'price' and 'feature1'
plt.figure(figsize=(8, 6))
sns.scatterplot(x='feature1', y='price', data=df)
plt.title('Relationship between ""feature1"" and ""price""')
plt.xlabel('feature1')
plt.ylabel('Price')
plt.show()

# Interpret results
print(""No major statistical issues detected. The analysis suggests a moderate positive correlation between 'feature1' and 'price' (r = {:.3f}, p-value = {:.3f})."".format(feature1_corr, feature1_pval))",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:34:17.935154,llama-3-8b-v2,v2,True
none,none,business,sales performance across regions,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load the data
df = pd.read_csv('churn_data.csv')

# Explore the data
print(df.head())
print(df.describe())

# Correlation analysis
corr_matrix = df.corr()
print(corr_matrix)

# Scatter plot of age vs. churn
plt.figure(figsize=(8, 6))
sns.scatterplot(x='age', y='churn', data=df)
plt.title('Age vs. Churn')
plt.xlabel('Age')
plt.ylabel('Churn')
plt.show()

# Calculate the Pearson correlation coefficient
age_corr, age_pvalue = pearsonr(df['age'], df['churn'])
print(f""Pearson correlation coefficient: {age_corr:.3f}, p-value: {age_pvalue:.3f}"")

# Interpretation:
print(""Based on the correlation analysis, there is a moderate positive correlation between age and churn (r = {:.3f}, p-value = {:.3f}). However, we must be cautious when interpreting this result, as correlation does not imply causation. Additionally, other factors such as tenure and monthly charges may also be influencing the relationship between age and churn. Further investigation is needed to determine the underlying mechanisms driving this association.""
     .format(age_corr, age_pvalue))","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected. Good practices observed include:\n\n1. **Clear and concise language**: We used proper statistical language, avoiding causality claims and instead focusing on associations.\n2. **Uncertainty quantification**: We reported the p-value and correlation coefficient to provide a sense of the uncertainty associated with our findings.\n3. **Acknowledgment of limitations**: We acknowledged the need for further investigation to understand the under', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load the data
df = pd.read_csv('churn_data.csv')

# Explore the data
print(df.head())
print(df.describe())

# Correlation analysis
corr_matrix = df.corr()
print(corr_matrix)

# Scatter plot of age vs. churn
plt.figure(figsize=(8, 6))
sns.scatterplot(x='age', y='churn', data=df)
plt.title('Age vs. Churn')
plt.xlabel('Age')
plt.ylabel('Churn')
plt.show()

# Calculate the Pearson correlation coefficient
age_corr, age_pvalue = pearsonr(df['age'], df['churn'])
print(f""Pearson correlation coefficient: {age_corr:.3f}, p-value: {age_pvalue:.3f}"")

# Interpretation:
print(""Based on the correlation analysis, there is a moderate positive correlation between age and churn (r = {:.3f}, p-value = {:.3f}). However, we must be cautious when interpreting this result, as correlation does not imply causation. Additionally, other factors such as tenure and monthly charges may also be influencing the relationship between age and churn. Further investigation is needed to determine the underlying mechanisms driving this association.""
     .format(age_corr, age_pvalue))",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:35:10.106417,llama-3-8b-v2,v2,True
none,none,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load data
df = pd.read_csv('churn_data.csv')

# Explore relationship between variables
print(""Relationship between 'total_minutes_invoices' and 'churned':"")
correlation, p_value = pearsonr(df['total_minutes_invoices'], df['churned'])
print(f""Correlation coefficient: {correlation:.2f}, p-value: {p_value:.4f}"")

# Visualize relationship
plt.scatter(df['total_minutes_invoices'], df['churned'])
plt.xlabel('Total minutes on invoices')
plt.ylabel('Churned (0/1)')
plt.title('Relationship between total minutes on invoices and churned')
plt.show()

# Calculate confidence interval for mean difference
from scipy.stats import ttest_ind
mean_diff, conf_int = ttest_ind(df['total_minutes_invoices'][df['churned'] == 0], 
                                df['total_minutes_invoices'][df['churned'] == 1], equal_var=False)
print(f""Mean difference in total minutes on invoices between churned and non-churned customers: {mean_diff:.2f}, ""
      f""95% confidence interval: {conf_int[0]:.2f}, {conf_int[1]:.2f}"")

# Acknowledge limitations
print(""Note: This analysis assumes a linear relationship between the variables and does not account for potential confounders."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n**Brief Review:**\n\nNo major statistical issues detected in this code. The following good practices were observed:\n\n* The code uses appropriate causal language, describing the relationship between variables as correlation and association, rather than causality.\n* The code includes uncertainty by reporting p-values and confidence intervals, allowing for a nuanced understanding of the results.\n* The code acknowledges limitations by noting potential confounders and the assumption of a linear re', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load data
df = pd.read_csv('churn_data.csv')

# Explore relationship between variables
print(""Relationship between 'total_minutes_invoices' and 'churned':"")
correlation, p_value = pearsonr(df['total_minutes_invoices'], df['churned'])
print(f""Correlation coefficient: {correlation:.2f}, p-value: {p_value:.4f}"")

# Visualize relationship
plt.scatter(df['total_minutes_invoices'], df['churned'])
plt.xlabel('Total minutes on invoices')
plt.ylabel('Churned (0/1)')
plt.title('Relationship between total minutes on invoices and churned')
plt.show()

# Calculate confidence interval for mean difference
from scipy.stats import ttest_ind
mean_diff, conf_int = ttest_ind(df['total_minutes_invoices'][df['churned'] == 0], 
                                df['total_minutes_invoices'][df['churned'] == 1], equal_var=False)
print(f""Mean difference in total minutes on invoices between churned and non-churned customers: {mean_diff:.2f}, ""
      f""95% confidence interval: {conf_int[0]:.2f}, {conf_int[1]:.2f}"")

# Acknowledge limitations
print(""Note: This analysis assumes a linear relationship between the variables and does not account for potential confounders."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:35:56.537345,llama-3-8b-v2,v2,True
none,none,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv(""pricing_data.csv"")

# Explore relationships between variables
corr_matrix = df.corr()
print(corr_matrix)

# Analyze the impact of pricing strategy on sales
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x=""price"", y=""sales"", data=df)
plt.xlabel(""Price"")
plt.ylabel(""Sales"")
plt.title(""Relationship between Price and Sales"")
plt.show()

# Calculate the correlation coefficient and p-value
correlation_coefficient, p_value = corr_matrix.iloc[0, 1]
print(f""Correlation Coefficient: {correlation_coefficient:.2f}"")
print(f""P-value: {p_value:.4f}"")

# Perform a t-test to determine if the mean sales differ between two price groups
t_stat, p_value = ttest_ind(df[df[""price""] < 100][""sales""], df[df[""price""] >= 100][""sales""])
print(f""T-statistic: {t_stat:.2f}"")
print(f""P-value: {p_value:.4f}"")

# Visualize the results
sns.set()
plt.figure(figsize=(8, 6))
sns.boxplot(x=""price"", y=""sales"", data=df)
plt.xlabel(""Price"")
plt.ylabel(""Sales"")
plt.title(""Boxplot of Sales by Price"")
plt.show()","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**BRIEF REVIEW**\n\nNo major statistical issues detected in this code example. The following good practices were observed:\n\n* The code uses appropriate causal language, such as ""correlation"" and ""association"", to describe the relationships between variables, rather than making claims about causality.\n* The code includes uncertainty in the form of confidence intervals (p-values) and error bars (boxplot) to provide a sense of the variability in the data.\n* The code acknowledges the limitations', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv(""pricing_data.csv"")

# Explore relationships between variables
corr_matrix = df.corr()
print(corr_matrix)

# Analyze the impact of pricing strategy on sales
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x=""price"", y=""sales"", data=df)
plt.xlabel(""Price"")
plt.ylabel(""Sales"")
plt.title(""Relationship between Price and Sales"")
plt.show()

# Calculate the correlation coefficient and p-value
correlation_coefficient, p_value = corr_matrix.iloc[0, 1]
print(f""Correlation Coefficient: {correlation_coefficient:.2f}"")
print(f""P-value: {p_value:.4f}"")

# Perform a t-test to determine if the mean sales differ between two price groups
t_stat, p_value = ttest_ind(df[df[""price""] < 100][""sales""], df[df[""price""] >= 100][""sales""])
print(f""T-statistic: {t_stat:.2f}"")
print(f""P-value: {p_value:.4f}"")

# Visualize the results
sns.set()
plt.figure(figsize=(8, 6))
sns.boxplot(x=""price"", y=""sales"", data=df)
plt.xlabel(""Price"")
plt.ylabel(""Sales"")
plt.title(""Boxplot of Sales by Price"")
plt.show()",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:36:52.976187,llama-3-8b-v2,v2,True
none,none,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('customer_data.csv')

# Explore the data
print(df.head())
print(df.describe())

# Calculate the correlation between variables
corr_matrix = df.corr()
print(corr_matrix)

# Visualize the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)
plt.show()

# Perform a t-test to compare the mean churn rates between two groups
churn_rate_group1 = df[df['churn'] == 1]['churn_rate'].mean()
churn_rate_group2 = df[df['churn'] == 0]['churn_rate'].mean()
t_stat, p_val = ttest_ind(df['churn_rate'][df['churn'] == 1], df['churn_rate'][df['churn'] == 0])
print(f'T-statistic: {t_stat}, p-value: {p_val}')

# Calculate the confidence interval for the mean churn rate
conf_int = t.cdf(t_stat, df.shape[0] - 1) * 100
print(f'Confidence interval: ({conf_int[0]:.2f}, {conf_int[1]:.2f})')

# Visualize the distribution of churn rates
plt.figure(figsize=(8, 6))
sns.distplot(df['churn_rate'], hist=True, kde=True)
plt.xlabel('Churn Rate')
plt.ylabel('Frequency')
plt.title('Distribution of Churn Rates')
plt.show()

# Acknowledge limitations
print(""Note: The results are based on the assumption that the data is representative of the population."")
print(""Future studies should consider controlling for potential confounders, such as seasonality and demographics."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**BRIEF REVIEW**\n\nNo major statistical issues detected in this code. The following good practices were observed:\n\n* The code uses appropriate causal language, avoiding the claim that a correlation is a cause-and-effect relationship.\n* The code includes uncertainty by calculating the confidence interval for the mean churn rate and reporting the p-value from the t-test.\n* The code acknowledges limitations by noting the potential impact of uncontrolled confounders on the results.\n\nOverall, th', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('customer_data.csv')

# Explore the data
print(df.head())
print(df.describe())

# Calculate the correlation between variables
corr_matrix = df.corr()
print(corr_matrix)

# Visualize the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', square=True)
plt.show()

# Perform a t-test to compare the mean churn rates between two groups
churn_rate_group1 = df[df['churn'] == 1]['churn_rate'].mean()
churn_rate_group2 = df[df['churn'] == 0]['churn_rate'].mean()
t_stat, p_val = ttest_ind(df['churn_rate'][df['churn'] == 1], df['churn_rate'][df['churn'] == 0])
print(f'T-statistic: {t_stat}, p-value: {p_val}')

# Calculate the confidence interval for the mean churn rate
conf_int = t.cdf(t_stat, df.shape[0] - 1) * 100
print(f'Confidence interval: ({conf_int[0]:.2f}, {conf_int[1]:.2f})')

# Visualize the distribution of churn rates
plt.figure(figsize=(8, 6))
sns.distplot(df['churn_rate'], hist=True, kde=True)
plt.xlabel('Churn Rate')
plt.ylabel('Frequency')
plt.title('Distribution of Churn Rates')
plt.show()

# Acknowledge limitations
print(""Note: The results are based on the assumption that the data is representative of the population."")
print(""Future studies should consider controlling for potential confounders, such as seasonality and demographics."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:37:50.960570,llama-3-8b-v2,v2,True
none,none,business,pricing strategy impact analysis,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('sales_data.csv')

# Merge data by region and quarter
merged_data = data.groupby(['region', 'quarter']).agg({'sales':'mean'}).reset_index()

# Visualize sales by region
sns.barplot(x='region', y='sales', data=merged_data)
plt.xlabel('Region')
plt.ylabel('Average Sales')
plt.title('Sales by Region')
plt.show()

# Calculate the association between region and sales
correlation_coefficient, _ = sns.rpearsonr(merged_data['region'], merged_data['sales'])
print(f""Correlation coefficient: {correlation_coefficient:.3f}"")

# Perform a t-test to check for statistical significance
t_stat, p_value = ttest_ind(merged_data['sales'], alternative='two-sided')
print(f""T-statistic: {t_stat:.3f}, p-value: {p_value:.3f}"")

# Calculate the 95% confidence interval for the mean sales by region
ci = merged_data['sales'].std() * 1.96 / len(merged_data)  # 1.96 is the z-score for 95% confidence
print(f""95% confidence interval: ({ci:.2f}, {ci:.2f})"")

# Acknowledge limitations
print(""Note: This analysis assumes that the data is normally distributed and that there are no confounding variables. Further exploration of the data and consideration of potential confounders is recommended."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Code Review:**\n\nNo major statistical issues detected.\n\nTwo good practices observed in this code include:\n\n1. **Proper use of causal language**: The code uses phrases such as ""association"" and ""correlation coefficient"" to describe the relationship between region and sales, rather than making causality claims.\n2. **Inclusion of uncertainty**: The code calculates the confidence interval for the mean sales by region and reports the p-value from the t-test, acknowledging the uncertainty assoc', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('sales_data.csv')

# Merge data by region and quarter
merged_data = data.groupby(['region', 'quarter']).agg({'sales':'mean'}).reset_index()

# Visualize sales by region
sns.barplot(x='region', y='sales', data=merged_data)
plt.xlabel('Region')
plt.ylabel('Average Sales')
plt.title('Sales by Region')
plt.show()

# Calculate the association between region and sales
correlation_coefficient, _ = sns.rpearsonr(merged_data['region'], merged_data['sales'])
print(f""Correlation coefficient: {correlation_coefficient:.3f}"")

# Perform a t-test to check for statistical significance
t_stat, p_value = ttest_ind(merged_data['sales'], alternative='two-sided')
print(f""T-statistic: {t_stat:.3f}, p-value: {p_value:.3f}"")

# Calculate the 95% confidence interval for the mean sales by region
ci = merged_data['sales'].std() * 1.96 / len(merged_data)  # 1.96 is the z-score for 95% confidence
print(f""95% confidence interval: ({ci:.2f}, {ci:.2f})"")

# Acknowledge limitations
print(""Note: This analysis assumes that the data is normally distributed and that there are no confounding variables. Further exploration of the data and consideration of potential confounders is recommended."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:38:40.782173,llama-3-8b-v2,v2,True
none,none,business,sales performance across regions,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('marketing_data.csv')

# Define the outcome variable and predictor variable
outcome_var ='sales'
predictor_var = 'advertising'

# Calculate the correlation coefficient and p-value
corr_coef, p_val = pearsonr(df[predictor_var], df[outcome_var])

print(f""Correlation coefficient: {corr_coef:.3f}"")
print(f""P-value: {p_val:.3f}"")

# Plot the scatter plot with 95% confidence interval
plt.scatter(df[predictor_var], df[outcome_var])
plt.xlabel(predictor_var)
plt.ylabel(outcome_var)
plt.title(f""{predictor_var} vs. {outcome_var}"")
plt.errorbar(range(len(df)), df[outcome_var], yerr=1.96 * df[outcome_var].std() / len(df), fmt='o', color='r', ecolor='r', capsize=2)
plt.show()

# Perform a two-sample t-test to compare the mean sales between two groups
group1 = df[df[predictor_var] < 100]
group2 = df[df[predictor_var] >= 100]
t_stat, p_val = ttest_ind(group1[outcome_var], group2[outcome_var])

print(f""T-statistic: {t_stat:.3f}"")
print(f""P-value: {p_val:.3f}"")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected in this code example. The following good practices are observed:\n\n1. **Proper causal language**: The code uses correlation and association language, avoiding claims of causality between the predictor variable and outcome variable.\n2. **Uncertainty quantification**: The code provides p-values and confidence intervals to quantify the uncertainty associated with the statistical results.\n3. **Acknowledgement of limitations**: The code doe', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('marketing_data.csv')

# Define the outcome variable and predictor variable
outcome_var ='sales'
predictor_var = 'advertising'

# Calculate the correlation coefficient and p-value
corr_coef, p_val = pearsonr(df[predictor_var], df[outcome_var])

print(f""Correlation coefficient: {corr_coef:.3f}"")
print(f""P-value: {p_val:.3f}"")

# Plot the scatter plot with 95% confidence interval
plt.scatter(df[predictor_var], df[outcome_var])
plt.xlabel(predictor_var)
plt.ylabel(outcome_var)
plt.title(f""{predictor_var} vs. {outcome_var}"")
plt.errorbar(range(len(df)), df[outcome_var], yerr=1.96 * df[outcome_var].std() / len(df), fmt='o', color='r', ecolor='r', capsize=2)
plt.show()

# Perform a two-sample t-test to compare the mean sales between two groups
group1 = df[df[predictor_var] < 100]
group2 = df[df[predictor_var] >= 100]
t_stat, p_val = ttest_ind(group1[outcome_var], group2[outcome_var])

print(f""T-statistic: {t_stat:.3f}"")
print(f""P-value: {p_val:.3f}"")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:39:32.518839,llama-3-8b-v2,v2,True
none,none,business,sales performance across regions,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('marketing_data.csv')

# Explore data
print(df.head())
print(df.describe())

# Check for missing values
print(df.isnull().sum())

# Define outcome variable and predictor variable
outcome = 'conversion_rate'
predictor = 'ad_spend'

# Calculate correlation between outcome and predictor
correlation = df[predictor].corr(df[outcome])
print(f""Correlation between {predictor} and {outcome}: {correlation:.2f}"")

# Perform t-test to check for significant association
t_stat, p_val = ttest_ind(df[predictor], df[outcome])
print(f""T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")

# Plot scatter plot to visualize relationship
plt.scatter(df[predictor], df[outcome])
plt.xlabel(predictor)
plt.ylabel(outcome)
plt.title(f""{predictor} vs. {outcome}"")
plt.show()

# Calculate confidence interval for mean difference
mean_diff = df[predictor].mean() - df[outcome].mean()
conf_int = t.cdf(0.975, len(df)-2) * (1 + 1/(len(df)-2))
print(f""Confidence interval for mean difference: ({mean_diff - conf_int:.2f}, {mean_diff + conf_int:.2f})"")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Review:**\nNo major statistical issues detected in this code. The following good practices were observed:\n\n* The code uses proper causal language, stating that there is an association between the predictor and outcome variables, rather than implying causality.\n* The code includes uncertainty by reporting the correlation coefficient, t-statistic, and p-value, as well as calculating a confidence interval for the mean difference.\n* The code acknowledges limitations by checking for missing va', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('marketing_data.csv')

# Explore data
print(df.head())
print(df.describe())

# Check for missing values
print(df.isnull().sum())

# Define outcome variable and predictor variable
outcome = 'conversion_rate'
predictor = 'ad_spend'

# Calculate correlation between outcome and predictor
correlation = df[predictor].corr(df[outcome])
print(f""Correlation between {predictor} and {outcome}: {correlation:.2f}"")

# Perform t-test to check for significant association
t_stat, p_val = ttest_ind(df[predictor], df[outcome])
print(f""T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")

# Plot scatter plot to visualize relationship
plt.scatter(df[predictor], df[outcome])
plt.xlabel(predictor)
plt.ylabel(outcome)
plt.title(f""{predictor} vs. {outcome}"")
plt.show()

# Calculate confidence interval for mean difference
mean_diff = df[predictor].mean() - df[outcome].mean()
conf_int = t.cdf(0.975, len(df)-2) * (1 + 1/(len(df)-2))
print(f""Confidence interval for mean difference: ({mean_diff - conf_int:.2f}, {mean_diff + conf_int:.2f})"")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:40:28.369337,llama-3-8b-v2,v2,True
none,none,business,customer churn prediction analysis,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('churn_data.csv')

# Convert categorical variables to numerical
df['churn'] = df['churn'].astype(int)

# Calculate summary statistics
print(""Summary Statistics:"")
print(df.describe())

# Calculate correlation matrix
corr_matrix = df.corr()
print(""\nCorrelation Matrix:"")
print(corr_matrix)

# Visualize correlation matrix
plt.figure(figsize=(10, 8))
plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')
plt.title(""Correlation Matrix"")
plt.colorbar()
plt.show()

# Perform t-test to compare mean of 'income' between churners and non-churners
t_stat, p_val = ttest_ind(df[df['churn'] == 1]['income'], df[df['churn'] == 0]['income'])
print(""\nT-statistic and p-value for income comparison:"")
print(f""T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")

# Visualize the distribution of 'income' for churners and non-churners
plt.figure(figsize=(8, 6))
plt.hist(df[df['churn'] == 1]['income'], alpha=0.5, label='Churners', density=True)
plt.hist(df[df['churn'] == 0]['income'], alpha=0.5, label='Non-Churners', density=True)
plt.title(""Distribution of Income for Churners and Non-Churners"")
plt.xlabel(""Income"")
plt.ylabel(""Density"")
plt.legend()
plt.show()

# Acknowledge limitations
print(""\nLimitations:"")
print(""1. The data may contain confounders that affect the results."")
print(""2. The sample size may be too small to generalize to the population."")
print(""3. The t-test assumes normality of the data, which may not be the case."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected in this code. The following good practices were observed:\n\n1. **Causal language:** The code avoids using causal language, instead using phrases like ""correlation"" and ""association"" to describe the relationships between variables.\n2. **Uncertainty:** The code includes uncertainty in the form of p-values and confidence intervals, allowing for a more nuanced interpretation of the results.\n3. **Acknowledgement of limitations:** The code a', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('churn_data.csv')

# Convert categorical variables to numerical
df['churn'] = df['churn'].astype(int)

# Calculate summary statistics
print(""Summary Statistics:"")
print(df.describe())

# Calculate correlation matrix
corr_matrix = df.corr()
print(""\nCorrelation Matrix:"")
print(corr_matrix)

# Visualize correlation matrix
plt.figure(figsize=(10, 8))
plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')
plt.title(""Correlation Matrix"")
plt.colorbar()
plt.show()

# Perform t-test to compare mean of 'income' between churners and non-churners
t_stat, p_val = ttest_ind(df[df['churn'] == 1]['income'], df[df['churn'] == 0]['income'])
print(""\nT-statistic and p-value for income comparison:"")
print(f""T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")

# Visualize the distribution of 'income' for churners and non-churners
plt.figure(figsize=(8, 6))
plt.hist(df[df['churn'] == 1]['income'], alpha=0.5, label='Churners', density=True)
plt.hist(df[df['churn'] == 0]['income'], alpha=0.5, label='Non-Churners', density=True)
plt.title(""Distribution of Income for Churners and Non-Churners"")
plt.xlabel(""Income"")
plt.ylabel(""Density"")
plt.legend()
plt.show()

# Acknowledge limitations
print(""\nLimitations:"")
print(""1. The data may contain confounders that affect the results."")
print(""2. The sample size may be too small to generalize to the population."")
print(""3. The t-test assumes normality of the data, which may not be the case."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:41:32.071324,llama-3-8b-v2,v2,True
none,none,business,marketing campaign effectiveness study,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('sales_data.csv')

# Explore data
print(""Summary statistics:"")
print(data.describe())

# Visualize data
sns.set()
plt.figure(figsize=(8, 6))
sns.barplot(x='region', y='sales', data=data)
plt.title('Sales Performance Across Regions')
plt.xlabel('Region')
plt.ylabel('Sales')
plt.show()

# Calculate correlation between region and sales
corr_coef, p_value = sns.correlate(data['region'], data['sales'])
print(f""Correlation Coefficient: {corr_coef:.3f}"")
print(f""P-value: {p_value:.3f}"")

# Perform t-test to compare sales between regions
t_stat, p_value = ttest_ind(data['sales'][data['region'] == 'North'], data['sales'][data['region'] == 'South'])
print(f""T-statistic: {t_stat:.3f}"")
print(f""P-value: {p_value:.3f}"")

# Interpret results
print(""Interpretation:"")
print(""There is a statistically significant association between region and sales (p-value = {:.3f}, t-statistic = {:.3f})"".format(p_value, t_stat))
print(""However, we cannot conclude that region is the cause of the observed effect."")
print(""Other factors, such as marketing campaigns or seasonality, may be confounding the relationship."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Code Review:**\n\nNo major statistical issues detected in this code. The following good practices were observed:\n\n1. **Causal language:** The code uses appropriate causal language, avoiding claims of causality and instead highlighting associations and correlations.\n2. **Uncertainty:** The code includes uncertainty through the use of confidence intervals (p-values) and error bars (standard deviations) in the statistical results.\n3. **Limitations:** The code acknowledges the limitations of t', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('sales_data.csv')

# Explore data
print(""Summary statistics:"")
print(data.describe())

# Visualize data
sns.set()
plt.figure(figsize=(8, 6))
sns.barplot(x='region', y='sales', data=data)
plt.title('Sales Performance Across Regions')
plt.xlabel('Region')
plt.ylabel('Sales')
plt.show()

# Calculate correlation between region and sales
corr_coef, p_value = sns.correlate(data['region'], data['sales'])
print(f""Correlation Coefficient: {corr_coef:.3f}"")
print(f""P-value: {p_value:.3f}"")

# Perform t-test to compare sales between regions
t_stat, p_value = ttest_ind(data['sales'][data['region'] == 'North'], data['sales'][data['region'] == 'South'])
print(f""T-statistic: {t_stat:.3f}"")
print(f""P-value: {p_value:.3f}"")

# Interpret results
print(""Interpretation:"")
print(""There is a statistically significant association between region and sales (p-value = {:.3f}, t-statistic = {:.3f})"".format(p_value, t_stat))
print(""However, we cannot conclude that region is the cause of the observed effect."")
print(""Other factors, such as marketing campaigns or seasonality, may be confounding the relationship."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:42:23.207590,llama-3-8b-v2,v2,True
none,none,business,sales performance across regions,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('marketing_data.csv')

# Convert categorical variables to numerical
df['Region'] = pd.get_dummies(df['Region'])

# Calculate the mean response for each region
region_means = df.groupby('Region')['Response'].mean()

# Calculate the standard error for each region
region_se = df.groupby('Region')['Response'].std() / np.sqrt(df.groupby('Region')['Response'].count())

# Calculate the 95% confidence intervals for each region
region_ci = region_means + 1.96 * region_se

# Perform a two-sample t-test to compare the means
t_stat, p_val = ttest_ind(df['Response'][df['Region'] == 'North'], df['Response'][df['Region'] == 'South'])

# Create a plot to visualize the results
plt.bar(range(len(region_means)), region_means, yerr=region_se)
plt.xlabel('Region')
plt.ylabel('Mean Response')
plt.title('Mean Response by Region')
plt.show()

print(f'The estimated mean response for the North region is {region_means.iloc[0]:.2f} ({region_ci.iloc[0][0]:.2f}, {region_ci.iloc[0][1]:.2f}).')
print(f'The estimated mean response for the South region is {region_means.iloc[1]:.2f} ({region_ci.iloc[1][0]:.2f}, {region_ci.iloc[1][1]:.2f}).')
print(f'The p-value for the two-sample t-test is {p_val:.4f}.')","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Review**\n\nNo major statistical issues detected. The code demonstrates several good practices, including:\n\n* Using appropriate causal language, such as ""estimated mean response"" instead of ""causal effect"" to avoid implying causality.\n* Including uncertainty in the results through the use of confidence intervals and standard errors.\n* Acknowledging potential limitations, such as the possibility that the observed differences between regions may be due to confounding variables.\n* Interpretin', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('marketing_data.csv')

# Convert categorical variables to numerical
df['Region'] = pd.get_dummies(df['Region'])

# Calculate the mean response for each region
region_means = df.groupby('Region')['Response'].mean()

# Calculate the standard error for each region
region_se = df.groupby('Region')['Response'].std() / np.sqrt(df.groupby('Region')['Response'].count())

# Calculate the 95% confidence intervals for each region
region_ci = region_means + 1.96 * region_se

# Perform a two-sample t-test to compare the means
t_stat, p_val = ttest_ind(df['Response'][df['Region'] == 'North'], df['Response'][df['Region'] == 'South'])

# Create a plot to visualize the results
plt.bar(range(len(region_means)), region_means, yerr=region_se)
plt.xlabel('Region')
plt.ylabel('Mean Response')
plt.title('Mean Response by Region')
plt.show()

print(f'The estimated mean response for the North region is {region_means.iloc[0]:.2f} ({region_ci.iloc[0][0]:.2f}, {region_ci.iloc[0][1]:.2f}).')
print(f'The estimated mean response for the South region is {region_means.iloc[1]:.2f} ({region_ci.iloc[1][0]:.2f}, {region_ci.iloc[1][1]:.2f}).')
print(f'The p-value for the two-sample t-test is {p_val:.4f}.')",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:43:17.457275,llama-3-8b-v2,v2,True
none,none,education,student performance prediction,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv(""teaching_methods.csv"")

# Convert categorical variable to numerical
df['method'] = pd.Categorical(df['method']).codes

# Perform correlation analysis
corr_matrix = df.corr()
print(""Correlation Matrix:"")
print(corr_matrix)

# Visualize the relationship between variables
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x=""variable1"", y=""variable2"", data=df)
plt.title(""Relationship between Variables"")
plt.show()

# Perform t-test to compare means
t_stat, p_val = ttest_ind(df[""variable1""], df[""variable2""], equal_var=False)
print(""T-statistic:"", t_stat)
print(""p-value:"", p_val)

# Calculate confidence intervals for the mean difference
conf_int = t.cdf(t_stat, df[""variable1""].shape[0)-1) * (1 - 0.05)
print(""Confidence Interval:"", conf_int)

# Visualize the results
plt.figure(figsize=(8, 6))
plt.plot(df[""variable1""], label=""Method A"")
plt.plot(df[""variable2""], label=""Method B"")
plt.xlabel(""Method"")
plt.ylabel(""Mean Score"")
plt.title(""Comparison of Teaching Methods"")
plt.legend()
plt.show()

# Interpretation
print(""Based on the analysis, we observe a statistically significant difference between the two teaching methods (p ="", p_val, ""). The confidence interval suggests that the mean difference is likely to be within the range of ["", conf_int[0], "", "", conf_int[1], ""). However, we must acknowledge that this analysis is limited by the potential confounders, such as student demographics and prior knowledge."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review**\n\nNo major statistical issues detected in this code example. The following good practices were observed:\n\n1. **Causal language**: The code avoids making causal claims and instead uses phrases such as ""statistically significant difference"" and ""likely to be within the range,"" indicating a correlation or association rather than a direct cause-and-effect relationship.\n2. **Uncertainty quantification**: The code includes p-values and confidence intervals to quantify the uncerta', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv(""teaching_methods.csv"")

# Convert categorical variable to numerical
df['method'] = pd.Categorical(df['method']).codes

# Perform correlation analysis
corr_matrix = df.corr()
print(""Correlation Matrix:"")
print(corr_matrix)

# Visualize the relationship between variables
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x=""variable1"", y=""variable2"", data=df)
plt.title(""Relationship between Variables"")
plt.show()

# Perform t-test to compare means
t_stat, p_val = ttest_ind(df[""variable1""], df[""variable2""], equal_var=False)
print(""T-statistic:"", t_stat)
print(""p-value:"", p_val)

# Calculate confidence intervals for the mean difference
conf_int = t.cdf(t_stat, df[""variable1""].shape[0)-1) * (1 - 0.05)
print(""Confidence Interval:"", conf_int)

# Visualize the results
plt.figure(figsize=(8, 6))
plt.plot(df[""variable1""], label=""Method A"")
plt.plot(df[""variable2""], label=""Method B"")
plt.xlabel(""Method"")
plt.ylabel(""Mean Score"")
plt.title(""Comparison of Teaching Methods"")
plt.legend()
plt.show()

# Interpretation
print(""Based on the analysis, we observe a statistically significant difference between the two teaching methods (p ="", p_val, ""). The confidence interval suggests that the mean difference is likely to be within the range of ["", conf_int[0], "", "", conf_int[1], ""). However, we must acknowledge that this analysis is limited by the potential confounders, such as student demographics and prior knowledge."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:44:16.722111,llama-3-8b-v2,v2,True
none,none,education,student performance prediction,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv(""graduation_rates.csv"")

# Explore the data
print(df.head())
print(df.describe())

# Visualize the data
plt.figure(figsize=(8, 6))
sns.barplot(x=""demographics"", y=""grad_rate"", data=df)
plt.xlabel(""Demographics"")
plt.ylabel(""Graduation Rate"")
plt.title(""Graduation Rate by Demographics"")
plt.show()

# Perform a t-test to compare graduation rates between different demographics
t_stat, p_val = ttest_ind(df[""grad_rate""][df[""demographics""] == ""male""], 
                            df[""grad_rate""][df[""demographics""] == ""female""])
print(f""t-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")

# Calculate confidence intervals for the mean graduation rate
CI = t.cdf(t_stat, df[""grad_rate""].shape[0]-1) * (1 - t.cdf(t_stat, df[""grad_rate""].shape[0]-1))
print(f""95% Confidence Interval: ({CI[0]:.2f}, {CI[1]:.2f})"")

# Interpret the results
print(""Based on the analysis, we found a significant association between demographics and graduation rates."")
print(""However, we cannot conclude that demographics cause graduation rates. Further investigation is needed to establish causality."")
print(""The 95% confidence interval suggests that the true mean graduation rate for males is likely to be within the range of [CI[0], CI[1]]."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected in this code. The following good practices were observed:\n\n* The code uses proper statistical language, avoiding claims of causality and instead highlighting associations and correlations.\n* The code includes uncertainty estimates in the form of confidence intervals and p-values, providing a sense of the precision of the results.\n* The code acknowledges limitations, specifically the need for further investigation to establish causalit', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv(""graduation_rates.csv"")

# Explore the data
print(df.head())
print(df.describe())

# Visualize the data
plt.figure(figsize=(8, 6))
sns.barplot(x=""demographics"", y=""grad_rate"", data=df)
plt.xlabel(""Demographics"")
plt.ylabel(""Graduation Rate"")
plt.title(""Graduation Rate by Demographics"")
plt.show()

# Perform a t-test to compare graduation rates between different demographics
t_stat, p_val = ttest_ind(df[""grad_rate""][df[""demographics""] == ""male""], 
                            df[""grad_rate""][df[""demographics""] == ""female""])
print(f""t-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")

# Calculate confidence intervals for the mean graduation rate
CI = t.cdf(t_stat, df[""grad_rate""].shape[0]-1) * (1 - t.cdf(t_stat, df[""grad_rate""].shape[0]-1))
print(f""95% Confidence Interval: ({CI[0]:.2f}, {CI[1]:.2f})"")

# Interpret the results
print(""Based on the analysis, we found a significant association between demographics and graduation rates."")
print(""However, we cannot conclude that demographics cause graduation rates. Further investigation is needed to establish causality."")
print(""The 95% confidence interval suggests that the true mean graduation rate for males is likely to be within the range of [CI[0], CI[1]]."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:45:07.398319,llama-3-8b-v2,v2,True
none,none,education,student performance prediction,python,intermediate,"import pandas as pd
import numpy as np
from scipy.stats import pearsonr
from matplotlib import pyplot as plt

# Load the data
df = pd.read_csv('student_data.csv')

# Calculate the correlation between GPA and SAT scores
corr_coef, p_value = pearsonr(df['GPA'], df['SAT'])

# Create a confidence interval for the correlation coefficient
confidence_level = 0.95
se = corr_coef * np.sqrt((1 - corr_coef**2) / (df.shape[0] - 2))
lower_ci, upper_ci = corr_coef - 1.96 * se, corr_coef + 1.96 * se

# Plot the data with error bars
plt.scatter(df['GPA'], df['SAT'])
plt.xlabel('GPA')
plt.ylabel('SAT Scores')
plt.errorbar(df['GPA'], df['SAT'], xerr=None, yerr=None, fmt='o', capsize=5)
plt.title(f'Correlation between GPA and SAT Scores (r = {corr_coef:.3f}, p = {p_value:.4f})')
plt.show()

print(f""The correlation coefficient between GPA and SAT scores is {corr_coef:.3f} (95% CI: {lower_ci:.3f}, {upper_ci:.3f}), with a p-value of {p_value:.4f}."")
print(""This analysis does not imply a causal relationship between GPA and SAT scores. The observed correlation may be due to confounding variables or other factors."")

# Acknowledge limitations
print(""Limitations: The dataset may contain missing values, and the analysis assumes a linear relationship between GPA and SAT scores."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n**BRIEF REVIEW**\n\nNo major statistical issues detected in this code. Two good practices observed are:\n\n1. **Uncertainty quantification**: The code calculates a confidence interval for the correlation coefficient, providing a range of values within which the true correlation coefficient is likely to lie.\n2. **Cautious interpretation**: The code explicitly acknowledges that the observed correlation may not imply a causal relationship between GPA and SAT scores, and suggests that confounding v', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
from scipy.stats import pearsonr
from matplotlib import pyplot as plt

# Load the data
df = pd.read_csv('student_data.csv')

# Calculate the correlation between GPA and SAT scores
corr_coef, p_value = pearsonr(df['GPA'], df['SAT'])

# Create a confidence interval for the correlation coefficient
confidence_level = 0.95
se = corr_coef * np.sqrt((1 - corr_coef**2) / (df.shape[0] - 2))
lower_ci, upper_ci = corr_coef - 1.96 * se, corr_coef + 1.96 * se

# Plot the data with error bars
plt.scatter(df['GPA'], df['SAT'])
plt.xlabel('GPA')
plt.ylabel('SAT Scores')
plt.errorbar(df['GPA'], df['SAT'], xerr=None, yerr=None, fmt='o', capsize=5)
plt.title(f'Correlation between GPA and SAT Scores (r = {corr_coef:.3f}, p = {p_value:.4f})')
plt.show()

print(f""The correlation coefficient between GPA and SAT scores is {corr_coef:.3f} (95% CI: {lower_ci:.3f}, {upper_ci:.3f}), with a p-value of {p_value:.4f}."")
print(""This analysis does not imply a causal relationship between GPA and SAT scores. The observed correlation may be due to confounding variables or other factors."")

# Acknowledge limitations
print(""Limitations: The dataset may contain missing values, and the analysis assumes a linear relationship between GPA and SAT scores."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:46:06.630581,llama-3-8b-v2,v2,True
none,none,education,student performance prediction,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv(""teaching_method_data.csv"")

# Convert categorical variables to numerical
data['method'] = pd.Categorical(data['method']).codes

# Calculate means and standard deviations for each method
means = data.groupby('method')['score'].mean()
stds = data.groupby('method')['score'].std()

# Perform two-sample t-test to compare means
t_stat, p_val = ttest_ind(data['score'][data['method'] == 0], data['score'][data['method'] == 1])

# Calculate 95% confidence interval for the difference in means
ci = t.cdf(0.975, df=len(data)-2) * (t_stat / (stds[0] + stds[1]))

print(""Mean scores for each method:"")
print(means)
print(""Standard deviations for each method:"")
print(stds)
print(""Two-sample t-test results:"")
print(f""t-statistic: {t_stat:.3f}"")
print(f""P-value: {p_val:.4f}"")
print(f""95% confidence interval for the difference in means: ({ci[0]:.3f}, {ci[1]:.3f})"")

# Plot the data with error bars
plt.errorbar(range(2), means, yerr=stds, fmt='o')
plt.xlabel(""Method"")
plt.ylabel(""Mean Score"")
plt.title(""Comparison of Teaching Methods"")
plt.show()","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review**\n\nNo major statistical issues detected in this code. The following good practices were observed:\n\n* The code uses appropriate causal language, stating that the results are associations rather than causal relationships.\n* The code includes uncertainty by providing p-values and confidence intervals, which give a sense of the precision of the estimates.\n* The code acknowledges limitations by noting the potential confounder of the categorical variable ""method"" and the possibili', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv(""teaching_method_data.csv"")

# Convert categorical variables to numerical
data['method'] = pd.Categorical(data['method']).codes

# Calculate means and standard deviations for each method
means = data.groupby('method')['score'].mean()
stds = data.groupby('method')['score'].std()

# Perform two-sample t-test to compare means
t_stat, p_val = ttest_ind(data['score'][data['method'] == 0], data['score'][data['method'] == 1])

# Calculate 95% confidence interval for the difference in means
ci = t.cdf(0.975, df=len(data)-2) * (t_stat / (stds[0] + stds[1]))

print(""Mean scores for each method:"")
print(means)
print(""Standard deviations for each method:"")
print(stds)
print(""Two-sample t-test results:"")
print(f""t-statistic: {t_stat:.3f}"")
print(f""P-value: {p_val:.4f}"")
print(f""95% confidence interval for the difference in means: ({ci[0]:.3f}, {ci[1]:.3f})"")

# Plot the data with error bars
plt.errorbar(range(2), means, yerr=stds, fmt='o')
plt.xlabel(""Method"")
plt.ylabel(""Mean Score"")
plt.title(""Comparison of Teaching Methods"")
plt.show()",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:46:57.574092,llama-3-8b-v2,v2,True
none,none,education,student performance prediction,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
data = pd.read_csv(""online_vs_inperson_learning_outcomes.csv"")

# Define the outcome variable and the grouping variables
outcome_var = ""test_score""
group_vars = [""learning_format"", ""student_level""]

# Perform a two-sample t-test to compare online and in-person learning outcomes
t_stat, p_val = ttest_ind(data[outcome_var][data[""learning_format""] == ""online""],
                          data[outcome_var][data[""learning_format""] == ""inperson""])

# Calculate the effect size (Cohen's d)
effect_size = (data[outcome_var][data[""learning_format""] == ""online""].mean() -
              data[outcome_var][data[""learning_format""] == ""inperson""].mean()) / (data[outcome_var].std())

# Plot the results
plt.scatter(data[""learning_format""], data[outcome_var])
plt.xlabel(""Learning Format"")
plt.ylabel(""Test Score"")
plt.title(""Online vs In-Person Learning Outcomes"")
plt.errorbar(range(2), [data[outcome_var][data[""learning_format""] == ""online""].mean(), data[outcome_var][data[""learning_format""] == ""inperson""].mean()], xerr=[0, 0], fmt=""o"", color=""black"", capsize=5)
plt.show()

print(""Two-sample t-test results:"")
print(f""t-statistic: {t_stat:.2f}"")
print(f""p-value: {p_val:.4f}"")
print(f""Effect size (Cohen's d): {effect_size:.2f}"")

# Interpret the results
print(""Based on the results, there is a statistically significant difference in test scores between online and in-person learning formats (p ="", p_val, ""). The effect size suggests a moderate difference in favor of in-person learning. However, it is essential to note that this analysis does not establish causality and may be influenced by confounding variables such as student background and prior knowledge."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected. The code demonstrates good statistical practices by:\n\n1. Using appropriate causal language, highlighting the correlation between learning format and test scores rather than implying causality.\n2. Including uncertainty through the presentation of the p-value and confidence intervals (although not explicitly shown in this code snippet).\n3. Acknowledging limitations by noting the potential influence of confounding variables on the resul', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
data = pd.read_csv(""online_vs_inperson_learning_outcomes.csv"")

# Define the outcome variable and the grouping variables
outcome_var = ""test_score""
group_vars = [""learning_format"", ""student_level""]

# Perform a two-sample t-test to compare online and in-person learning outcomes
t_stat, p_val = ttest_ind(data[outcome_var][data[""learning_format""] == ""online""],
                          data[outcome_var][data[""learning_format""] == ""inperson""])

# Calculate the effect size (Cohen's d)
effect_size = (data[outcome_var][data[""learning_format""] == ""online""].mean() -
              data[outcome_var][data[""learning_format""] == ""inperson""].mean()) / (data[outcome_var].std())

# Plot the results
plt.scatter(data[""learning_format""], data[outcome_var])
plt.xlabel(""Learning Format"")
plt.ylabel(""Test Score"")
plt.title(""Online vs In-Person Learning Outcomes"")
plt.errorbar(range(2), [data[outcome_var][data[""learning_format""] == ""online""].mean(), data[outcome_var][data[""learning_format""] == ""inperson""].mean()], xerr=[0, 0], fmt=""o"", color=""black"", capsize=5)
plt.show()

print(""Two-sample t-test results:"")
print(f""t-statistic: {t_stat:.2f}"")
print(f""p-value: {p_val:.4f}"")
print(f""Effect size (Cohen's d): {effect_size:.2f}"")

# Interpret the results
print(""Based on the results, there is a statistically significant difference in test scores between online and in-person learning formats (p ="", p_val, ""). The effect size suggests a moderate difference in favor of in-person learning. However, it is essential to note that this analysis does not establish causality and may be influenced by confounding variables such as student background and prior knowledge."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:47:56.449759,llama-3-8b-v2,v2,True
none,none,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv(""online_vs_inperson_learning_outcomes.csv"")

# Merge datasets
merged_df = pd.merge(df1, df2, on=""student_id"")

# Check for missing values
print(""Missing values:"", merged_df.isnull().sum())

# Calculate correlation between online and in-person learning outcomes
correlation = sns.corrcoef(merged_df[""online_score""], merged_df[""inperson_score""])[0, 1]
print(""Correlation coefficient:"", correlation)

# Calculate confidence interval for the correlation coefficient
ci = t.cdf(0.95, 1, correlation, 1 - correlation)
print(""Confidence interval:"", ci)

# Calculate p-value for the correlation coefficient
t_stat, p_value = ttest_ind(merged_df[""online_score""], merged_df[""inperson_score""])
print(""p-value:"", p_value)

# Visualize the data
sns.scatterplot(x=""online_score"", y=""inperson_score"", data=merged_df)
plt.show()

# Interpretation
print(""There is a statistically significant association between online and in-person learning outcomes (p-value = {:.4f})."".format(p_value))
print(""However, we cannot infer causality from this correlation. There may be unobserved confounding variables that affect the relationship."")
print(""Furthermore, the confidence interval for the correlation coefficient indicates that there is some uncertainty in the estimate."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected in this code. The following good practices were observed:\n\n* The code uses appropriate causal language, stating that there is an association between online and in-person learning outcomes, but not a cause-and-effect relationship.\n* The code includes uncertainty in the form of a confidence interval and a p-value, allowing for a more nuanced interpretation of the results.\n* The code acknowledges limitations by mentioning the possibility', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv(""online_vs_inperson_learning_outcomes.csv"")

# Merge datasets
merged_df = pd.merge(df1, df2, on=""student_id"")

# Check for missing values
print(""Missing values:"", merged_df.isnull().sum())

# Calculate correlation between online and in-person learning outcomes
correlation = sns.corrcoef(merged_df[""online_score""], merged_df[""inperson_score""])[0, 1]
print(""Correlation coefficient:"", correlation)

# Calculate confidence interval for the correlation coefficient
ci = t.cdf(0.95, 1, correlation, 1 - correlation)
print(""Confidence interval:"", ci)

# Calculate p-value for the correlation coefficient
t_stat, p_value = ttest_ind(merged_df[""online_score""], merged_df[""inperson_score""])
print(""p-value:"", p_value)

# Visualize the data
sns.scatterplot(x=""online_score"", y=""inperson_score"", data=merged_df)
plt.show()

# Interpretation
print(""There is a statistically significant association between online and in-person learning outcomes (p-value = {:.4f})."".format(p_value))
print(""However, we cannot infer causality from this correlation. There may be unobserved confounding variables that affect the relationship."")
print(""Furthermore, the confidence interval for the correlation coefficient indicates that there is some uncertainty in the estimate."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:48:47.420053,llama-3-8b-v2,v2,True
none,none,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
grad_rates = pd.read_csv('grad_rates.csv')

# Melt the data to long format
grad_rates_melt = pd.melt(grad_rates, id_vars=['Demographic'], value_vars=['Graduation Rate'])

# Visualize the data using a bar plot
plt.figure(figsize=(8, 6))
sns.barplot(x='variable', y='value', data=grad_rates_melt)
plt.xlabel('Demographic')
plt.ylabel('Graduation Rate')
plt.title('Graduation Rate by Demographic')
plt.show()

# Perform a t-test to compare the means of two groups
t_stat, p_val = ttest_ind(grad_rates_melt['value'][grad_rates_melt['variable'] == 'Female'], 
                            grad_rates_melt['value'][grad_rates_melt['variable'] == 'Male'])
print(f'T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}')

# Calculate confidence intervals for the means
ci_lower, ci_upper = t.cdf(0.025, len(grad_rates_melt['value'][grad_rates_melt['variable'] == 'Female']) - 1) * (t_stat, 
                                                                                                  t_stat), 
                        1 - t.cdf(0.025, len(grad_rates_melt['value'][grad_rates_melt['variable'] == 'Female']) - 1) * (t_stat, 
                                                                                                  t_stat)
print(f'Confidence interval for female mean: ({ci_lower:.2f}, {ci_upper:.2f})')

print(f'Confidence interval for male mean: ({t_stat - ci_upper, t_stat + ci_lower:.2f})')","{'summary': 'No major statistical issues detected', 'detailed_explanation': ""```\n\n**Code Explanation:**\n\n1. The code loads the data from a CSV file and melts it to long format using the `melt` function from Pandas.\n2. A bar plot is created using Seaborn's `barplot` function to visualize the graduation rates by demographic.\n3. A t-test is performed using SciPy's `ttest_ind` function to compare the means of the female and male groups.\n4. The code calculates the confidence intervals for the means using the `t.cdf` function from SciPy.\n\n**Brief Review:**\n\nNo major statistic"", 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
grad_rates = pd.read_csv('grad_rates.csv')

# Melt the data to long format
grad_rates_melt = pd.melt(grad_rates, id_vars=['Demographic'], value_vars=['Graduation Rate'])

# Visualize the data using a bar plot
plt.figure(figsize=(8, 6))
sns.barplot(x='variable', y='value', data=grad_rates_melt)
plt.xlabel('Demographic')
plt.ylabel('Graduation Rate')
plt.title('Graduation Rate by Demographic')
plt.show()

# Perform a t-test to compare the means of two groups
t_stat, p_val = ttest_ind(grad_rates_melt['value'][grad_rates_melt['variable'] == 'Female'], 
                            grad_rates_melt['value'][grad_rates_melt['variable'] == 'Male'])
print(f'T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}')

# Calculate confidence intervals for the means
ci_lower, ci_upper = t.cdf(0.025, len(grad_rates_melt['value'][grad_rates_melt['variable'] == 'Female']) - 1) * (t_stat, 
                                                                                                  t_stat), 
                        1 - t.cdf(0.025, len(grad_rates_melt['value'][grad_rates_melt['variable'] == 'Female']) - 1) * (t_stat, 
                                                                                                  t_stat)
print(f'Confidence interval for female mean: ({ci_lower:.2f}, {ci_upper:.2f})')

print(f'Confidence interval for male mean: ({t_stat - ci_upper, t_stat + ci_lower:.2f})')",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:49:56.828642,llama-3-8b-v2,v2,True
none,none,education,student performance prediction,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('education_data.csv')

# Explore the data
print(df.head())
print(df.describe())

# Visualize the data
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x='online', y='outcomes', data=df)
plt.xlabel('Online Learning')
plt.ylabel('Outcomes')
plt.title('Association between Online Learning and Outcomes')
plt.show()

# Perform statistical analysis
online_outcomes = df[df['online'] == 1]['outcomes']
in_person_outcomes = df[df['online'] == 0]['outcomes']
t_stat, p_val = ttest_ind(online_outcomes, in_person_outcomes)
print(f't-statistic: {t_stat:.2f}, p-value: {p_val:.4f}')

# Interpret the results
print(f""The results suggest a significant association between online learning and outcomes (t-statistic = {t_stat}, p-value = {p_val:.4f})."")
print(""However, we must acknowledge that this analysis does not establish causality, and other factors may influence the outcome."")
print(""Additionally, the data may be subject to confounding variables, such as student demographics or instructor quality."")

# Create a confidence interval for the mean difference
from scipy.stats import t.cdf
mean_diff = online_outcomes.mean() - in_person_outcomes.mean()
se = online_outcomes.std() / np.sqrt(len(online_outcomes)) + in_person_outcomes.std() / np.sqrt(len(in_person_outcomes))
t_crit = t.cdf(t_stat, len(online_outcomes) + len(in_person_outcomes) - 2, loc=0, scale=se)
print(f""95% confidence interval for the mean difference: ({mean_diff - 2*se}, {mean_diff + 2*se})"")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected in this code. Two good practices observed:\n\n1. The code uses appropriate causal language, stating that the results suggest an association between online learning and outcomes, rather than claiming causality.\n2. The code includes uncertainty by providing the p-value and confidence interval for the mean difference, allowing the reader to understand the precision of the results.\n3. The code acknowledges limitations by mentioning the poss', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('education_data.csv')

# Explore the data
print(df.head())
print(df.describe())

# Visualize the data
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x='online', y='outcomes', data=df)
plt.xlabel('Online Learning')
plt.ylabel('Outcomes')
plt.title('Association between Online Learning and Outcomes')
plt.show()

# Perform statistical analysis
online_outcomes = df[df['online'] == 1]['outcomes']
in_person_outcomes = df[df['online'] == 0]['outcomes']
t_stat, p_val = ttest_ind(online_outcomes, in_person_outcomes)
print(f't-statistic: {t_stat:.2f}, p-value: {p_val:.4f}')

# Interpret the results
print(f""The results suggest a significant association between online learning and outcomes (t-statistic = {t_stat}, p-value = {p_val:.4f})."")
print(""However, we must acknowledge that this analysis does not establish causality, and other factors may influence the outcome."")
print(""Additionally, the data may be subject to confounding variables, such as student demographics or instructor quality."")

# Create a confidence interval for the mean difference
from scipy.stats import t.cdf
mean_diff = online_outcomes.mean() - in_person_outcomes.mean()
se = online_outcomes.std() / np.sqrt(len(online_outcomes)) + in_person_outcomes.std() / np.sqrt(len(in_person_outcomes))
t_crit = t.cdf(t_stat, len(online_outcomes) + len(in_person_outcomes) - 2, loc=0, scale=se)
print(f""95% confidence interval for the mean difference: ({mean_diff - 2*se}, {mean_diff + 2*se})"")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:51:00.296854,llama-3-8b-v2,v2,True
none,none,education,teaching method effectiveness comparison,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('online_learning_outcomes.csv')

# Explore data
print(data.head())
print(data.describe())

# Visualize data
sns.set()
plt.figure(figsize=(8, 6))
sns.barplot(x='Learning Mode', y='Score', data=data)
plt.xlabel('Learning Mode')
plt.ylabel('Score')
plt.title('Online vs In-Person Learning Outcomes')
plt.show()

# Perform statistical analysis
online_means = data[data['Learning Mode'] == 'Online']['Score'].mean()
in_person_means = data[data['Learning Mode'] == 'In-Person']['Score'].mean()
t_stat, p_val = ttest_ind(data[data['Learning Mode'] == 'Online']['Score'], 
                          data[data['Learning Mode'] == 'In-Person']['Score'])

print(f""t-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")

# Interpret results
print(""No major statistical issues detected. However, we observe a significant difference in scores between online and in-person learning modes (t-statistic: {:.2f}, p-value: {:.4f})."".format(t_stat, p_val))
print(""This result suggests an association between learning mode and score, but it does not imply causality. Further research is needed to understand the underlying mechanisms."")

# Acknowledge limitations
print(""Limitations of this study include:"")
print(""- Self-reported data may be biased"")
print(""- Sample size is limited"")
print(""- Confounding variables, such as student demographics and instructor quality, were not controlled for"")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected in this code example. Two good practices observed include:\n\n1. **Proper use of causal language**: The code correctly states that the result suggests an association between learning mode and score, but does not imply causality, acknowledging the limitations of the study.\n2. **Inclusion of uncertainty**: The code provides a p-value to quantify the significance of the result, and the interpretation is cautious and nuanced, acknowledging', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('online_learning_outcomes.csv')

# Explore data
print(data.head())
print(data.describe())

# Visualize data
sns.set()
plt.figure(figsize=(8, 6))
sns.barplot(x='Learning Mode', y='Score', data=data)
plt.xlabel('Learning Mode')
plt.ylabel('Score')
plt.title('Online vs In-Person Learning Outcomes')
plt.show()

# Perform statistical analysis
online_means = data[data['Learning Mode'] == 'Online']['Score'].mean()
in_person_means = data[data['Learning Mode'] == 'In-Person']['Score'].mean()
t_stat, p_val = ttest_ind(data[data['Learning Mode'] == 'Online']['Score'], 
                          data[data['Learning Mode'] == 'In-Person']['Score'])

print(f""t-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")

# Interpret results
print(""No major statistical issues detected. However, we observe a significant difference in scores between online and in-person learning modes (t-statistic: {:.2f}, p-value: {:.4f})."".format(t_stat, p_val))
print(""This result suggests an association between learning mode and score, but it does not imply causality. Further research is needed to understand the underlying mechanisms."")

# Acknowledge limitations
print(""Limitations of this study include:"")
print(""- Self-reported data may be biased"")
print(""- Sample size is limited"")
print(""- Confounding variables, such as student demographics and instructor quality, were not controlled for"")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:51:57.611677,llama-3-8b-v2,v2,True
none,none,education,online vs in-person learning outcomes,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load dataset
grad_rates = pd.read_csv('grad_rates.csv')

# Define demographic groups
demographics = ['gender', 'race', 'income']

# Calculate graduation rates by demographics
grad_rates_groupby = grad_rates.groupby(demographics).mean()

# Calculate correlation between graduation rates and demographics
correlation_matrix = grad_rates_groupby.corr()

# Visualize correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)
plt.title('Correlation Matrix of Graduation Rates and Demographics')
plt.show()

# Perform t-test to compare graduation rates between demographics
t_test_results = {}
for demographic in demographics:
    t_stat, p_val = ttest_ind(grad_rates[demographic], grad_rates[demographic].values)
    t_test_results[demographic] = (t_stat, p_val)

# Print results
print(""Correlation Matrix:"")
print(correlation_matrix)
print(""\nT-test Results:"")
for demographic, (t_stat, p_val) in t_test_results.items():
    print(f""{demographic}: t-statistic = {t_stat:.2f}, p-value = {p_val:.4f}"")

# Limitations and confounders
print(""\nLimitations:"")
print(""1. This analysis assumes that the data is representative of the entire population."")
print(""2. The correlation between graduation rates and demographics may be due to confounding variables not included in the analysis."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected. The code demonstrates good statistical practices by:\n\n1. Using appropriate causal language, such as ""correlation"" and ""association"", instead of ""causes"".\n2. Including uncertainty by providing error bars, confidence intervals, and p-values.\n3. Acknowledging limitations by highlighting potential confounders and the limitations of the analysis.\n\nNote that this is a simplified example, and in a real-world scenario, you would need to cons', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load dataset
grad_rates = pd.read_csv('grad_rates.csv')

# Define demographic groups
demographics = ['gender', 'race', 'income']

# Calculate graduation rates by demographics
grad_rates_groupby = grad_rates.groupby(demographics).mean()

# Calculate correlation between graduation rates and demographics
correlation_matrix = grad_rates_groupby.corr()

# Visualize correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)
plt.title('Correlation Matrix of Graduation Rates and Demographics')
plt.show()

# Perform t-test to compare graduation rates between demographics
t_test_results = {}
for demographic in demographics:
    t_stat, p_val = ttest_ind(grad_rates[demographic], grad_rates[demographic].values)
    t_test_results[demographic] = (t_stat, p_val)

# Print results
print(""Correlation Matrix:"")
print(correlation_matrix)
print(""\nT-test Results:"")
for demographic, (t_stat, p_val) in t_test_results.items():
    print(f""{demographic}: t-statistic = {t_stat:.2f}, p-value = {p_val:.4f}"")

# Limitations and confounders
print(""\nLimitations:"")
print(""1. This analysis assumes that the data is representative of the entire population."")
print(""2. The correlation between graduation rates and demographics may be due to confounding variables not included in the analysis."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:52:47.362200,llama-3-8b-v2,v2,True
none,none,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('data.csv')

# Define the research question and hypotheses
research_question = ""Is the new teaching method more effective than the traditional method?""
hypothesis_null = ""There is no significant difference in student outcomes between the two methods""
hypothesis_alternative = ""There is a significant difference in student outcomes between the two methods""

# Perform the t-test
t_stat, p_value = ttest_ind(df['new_method'], df['traditional_method'])

# Calculate the confidence interval
confidence_interval = np.array([t_stat - 1.96 * np.sqrt(t_stat / (df['new_method'].shape[0] + df['traditional_method'].shape[0] - 2)), 
                                t_stat + 1.96 * np.sqrt(t_stat / (df['new_method'].shape[0] + df['traditional_method'].shape[0] - 2))])

# Visualize the results
plt.bar(['New Method', 'Traditional Method'], [np.mean(df['new_method']), np.mean(df['traditional_method'])], yerr=[np.std(df['new_method']) / np.sqrt(df['new_method'].shape[0]), np.std(df['traditional_method']) / np.sqrt(df['traditional_method'].shape[0])])
plt.xlabel('Teaching Method')
plt.ylabel('Student Outcomes')
plt.title('Comparison of Student Outcomes between New and Traditional Methods')
plt.show()

# Interpret the results
print(f""The p-value is {p_value:.4f}, indicating a {p_value < 0.05 and'significant' or 'not significant'} difference between the two methods."")
print(f""The 95% confidence interval is [{confidence_interval[0]:.2f}, {confidence_interval[1]:.2f}]."")
print(""While the results suggest a potential difference between the two methods, it is essential to acknowledge the limitations of this study, including the small sample size and potential confounders such as student demographics and prior knowledge."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**BRIEF REVIEW**\n\nNo major statistical issues detected in this code. Two good practices observed are:\n\n1. The code uses appropriate causal language, stating that there is an association between the teaching method and student outcomes, rather than implying causality.\n2. The code includes uncertainty in the form of a confidence interval and p-value, providing a more comprehensive understanding of the results.', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv('data.csv')

# Define the research question and hypotheses
research_question = ""Is the new teaching method more effective than the traditional method?""
hypothesis_null = ""There is no significant difference in student outcomes between the two methods""
hypothesis_alternative = ""There is a significant difference in student outcomes between the two methods""

# Perform the t-test
t_stat, p_value = ttest_ind(df['new_method'], df['traditional_method'])

# Calculate the confidence interval
confidence_interval = np.array([t_stat - 1.96 * np.sqrt(t_stat / (df['new_method'].shape[0] + df['traditional_method'].shape[0] - 2)), 
                                t_stat + 1.96 * np.sqrt(t_stat / (df['new_method'].shape[0] + df['traditional_method'].shape[0] - 2))])

# Visualize the results
plt.bar(['New Method', 'Traditional Method'], [np.mean(df['new_method']), np.mean(df['traditional_method'])], yerr=[np.std(df['new_method']) / np.sqrt(df['new_method'].shape[0]), np.std(df['traditional_method']) / np.sqrt(df['traditional_method'].shape[0])])
plt.xlabel('Teaching Method')
plt.ylabel('Student Outcomes')
plt.title('Comparison of Student Outcomes between New and Traditional Methods')
plt.show()

# Interpret the results
print(f""The p-value is {p_value:.4f}, indicating a {p_value < 0.05 and'significant' or 'not significant'} difference between the two methods."")
print(f""The 95% confidence interval is [{confidence_interval[0]:.2f}, {confidence_interval[1]:.2f}]."")
print(""While the results suggest a potential difference between the two methods, it is essential to acknowledge the limitations of this study, including the small sample size and potential confounders such as student demographics and prior knowledge."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:53:47.266351,llama-3-8b-v2,v2,True
none,none,education,graduation rate analysis by demographics,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv(""data.csv"")

# Define the treatment and control groups
treatment = df[df[""method""] == ""treatment""]
control = df[df[""method""] == ""control""]

# Calculate the mean scores for each group
treatment_mean = np.mean(treatment[""score""])
control_mean = np.mean(control[""score""])

# Calculate the standard error of the difference
se_diff = np.sqrt((np.var(treatment[""score""]) / len(treatment)) + (np.var(control[""score""]) / len(control)))

# Perform the t-test
t_stat, p_val = ttest_ind(treatment[""score""], control[""score""], equal_var=False)

# Calculate the confidence interval for the difference
ci = np.arange(treatment_mean - 1.96 * se_diff, treatment_mean + 1.96 * se_diff)

# Plot the results
plt.bar([""Treatment"", ""Control""], [treatment_mean, control_mean])
plt.errorbar([""Treatment"", ""Control""], [treatment_mean, control_mean], yerr=[se_diff, se_diff], fmt='o')
plt.xlabel(""Method"")
plt.ylabel(""Mean Score"")
plt.title(""Comparison of Mean Scores"")
plt.show()

# Print the results
print(f""Mean score in treatment group: {treatment_mean:.2f}  {se_diff:.2f}"")
print(f""Mean score in control group: {control_mean:.2f}  {se_diff:.2f}"")
print(f""T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")
print(f""Confidence interval: {ci[0]:.2f} to {ci[1]:.2f}"")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Review:**\n\nNo major statistical issues detected in this code. The following good practices are observed:\n\n1. **Causal language:** The code uses appropriate causal language, avoiding claims of causality and instead focusing on associations and correlations.\n2. **Uncertainty:** The code includes uncertainty estimates in the form of standard errors, confidence intervals, and p-values, providing a comprehensive picture of the results.\n3. **Acknowledgement of limitations:** The code acknowled', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv(""data.csv"")

# Define the treatment and control groups
treatment = df[df[""method""] == ""treatment""]
control = df[df[""method""] == ""control""]

# Calculate the mean scores for each group
treatment_mean = np.mean(treatment[""score""])
control_mean = np.mean(control[""score""])

# Calculate the standard error of the difference
se_diff = np.sqrt((np.var(treatment[""score""]) / len(treatment)) + (np.var(control[""score""]) / len(control)))

# Perform the t-test
t_stat, p_val = ttest_ind(treatment[""score""], control[""score""], equal_var=False)

# Calculate the confidence interval for the difference
ci = np.arange(treatment_mean - 1.96 * se_diff, treatment_mean + 1.96 * se_diff)

# Plot the results
plt.bar([""Treatment"", ""Control""], [treatment_mean, control_mean])
plt.errorbar([""Treatment"", ""Control""], [treatment_mean, control_mean], yerr=[se_diff, se_diff], fmt='o')
plt.xlabel(""Method"")
plt.ylabel(""Mean Score"")
plt.title(""Comparison of Mean Scores"")
plt.show()

# Print the results
print(f""Mean score in treatment group: {treatment_mean:.2f}  {se_diff:.2f}"")
print(f""Mean score in control group: {control_mean:.2f}  {se_diff:.2f}"")
print(f""T-statistic: {t_stat:.2f}, p-value: {p_val:.4f}"")
print(f""Confidence interval: {ci[0]:.2f} to {ci[1]:.2f}"")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:54:49.884568,llama-3-8b-v2,v2,True
none,none,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load the data
df = pd.read_csv('crime_rate_data.csv')

# Explore the data
print(df.head())
print(df.describe())

# Visualize the relationship between variables
sns.pairplot(df[['crime_rate', 'poverty_rate', 'unemployment_rate']])
plt.show()

# Calculate the correlation between variables
corr_matrix = df.corr()
print(corr_matrix)

# Perform a regression analysis
X = df[['poverty_rate', 'unemployment_rate']]
y = df['crime_rate']
model = sm.OLS(y, X).fit()
print(model.summary())

# Calculate the p-value and confidence interval
p_value = model.f_test().pvalue
confidence_interval = model.conf_int()

# Print the results
print(f""Correlation between poverty_rate and crime_rate: {corr_matrix['crime_rate']['poverty_rate']:.2f}"")
print(f""P-value: {p_value:.4f}"")
print(f""95% Confidence Interval: {confidence_interval[0]:.2f} - {confidence_interval[1]:.2f}"")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Code Review:**\n\nNo major statistical issues detected in this code. The following good practices were observed:\n\n1. **Causal language:** The code uses appropriate causal language, avoiding claims of causality and instead describing the relationships between variables as correlations or associations.\n2. **Uncertainty quantification:** The code includes uncertainty quantification through the calculation of p-values and confidence intervals, providing a sense of the reliability of the result', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load the data
df = pd.read_csv('crime_rate_data.csv')

# Explore the data
print(df.head())
print(df.describe())

# Visualize the relationship between variables
sns.pairplot(df[['crime_rate', 'poverty_rate', 'unemployment_rate']])
plt.show()

# Calculate the correlation between variables
corr_matrix = df.corr()
print(corr_matrix)

# Perform a regression analysis
X = df[['poverty_rate', 'unemployment_rate']]
y = df['crime_rate']
model = sm.OLS(y, X).fit()
print(model.summary())

# Calculate the p-value and confidence interval
p_value = model.f_test().pvalue
confidence_interval = model.conf_int()

# Print the results
print(f""Correlation between poverty_rate and crime_rate: {corr_matrix['crime_rate']['poverty_rate']:.2f}"")
print(f""P-value: {p_value:.4f}"")
print(f""95% Confidence Interval: {confidence_interval[0]:.2f} - {confidence_interval[1]:.2f}"")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:55:55.707689,llama-3-8b-v2,v2,True
none,none,social_science,crime rate factors,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load the data
df = pd.read_csv('social_media_usage_and_mental_health_data.csv')

# Explore the data
print(df.describe())

# Calculate the correlation between social media usage and mental health scores
correlation, p_value = pearsonr(df['social_media_usage'], df['mental_health_scores'])

# Create a confidence interval for the correlation
confidence_interval = 0.95
z_score = 1.96
margin_of_error = z_score * (1 - confidence_interval / 2) / (df.shape[0] ** 0.5)

print(f""Correlation coefficient: {correlation:.3f}  {margin_of_error:.3f}"")

# Visualize the relationship between social media usage and mental health scores
plt.scatter(df['social_media_usage'], df['mental_health_scores'])
sns.regplot(x='social_media_usage', y='mental_health_scores', data=df)
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health Scores')
plt.title('Relationship between Social Media Usage and Mental Health Scores')
plt.show()

# Interpret the results
print(f""The correlation coefficient between social media usage and mental health scores is {correlation:.3f}, which is statistically significant (p-value = {p_value:.3f})."")
print(""However, it is important to note that correlation does not imply causation. Further research is needed to establish a causal link between social media usage and mental health scores."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Review:**\n\nNo major statistical issues detected in this code. The following good practices were observed:\n\n* The code uses appropriate causal language, noting that correlation does not imply causation.\n* The code includes uncertainty by calculating a confidence interval for the correlation coefficient and reporting the p-value.\n* The code acknowledges limitations by noting the need for further research to establish a causal link between social media usage and mental health scores.\n* The', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load the data
df = pd.read_csv('social_media_usage_and_mental_health_data.csv')

# Explore the data
print(df.describe())

# Calculate the correlation between social media usage and mental health scores
correlation, p_value = pearsonr(df['social_media_usage'], df['mental_health_scores'])

# Create a confidence interval for the correlation
confidence_interval = 0.95
z_score = 1.96
margin_of_error = z_score * (1 - confidence_interval / 2) / (df.shape[0] ** 0.5)

print(f""Correlation coefficient: {correlation:.3f}  {margin_of_error:.3f}"")

# Visualize the relationship between social media usage and mental health scores
plt.scatter(df['social_media_usage'], df['mental_health_scores'])
sns.regplot(x='social_media_usage', y='mental_health_scores', data=df)
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health Scores')
plt.title('Relationship between Social Media Usage and Mental Health Scores')
plt.show()

# Interpret the results
print(f""The correlation coefficient between social media usage and mental health scores is {correlation:.3f}, which is statistically significant (p-value = {p_value:.3f})."")
print(""However, it is important to note that correlation does not imply causation. Further research is needed to establish a causal link between social media usage and mental health scores."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:56:48.916881,llama-3-8b-v2,v2,True
none,none,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('voting_data.csv')

# Explore the data
print(df.head())
print(df.describe())

# Correlation analysis
corr_matrix = df.corr()
print(corr_matrix)

# Plot the correlation matrix
plt.figure(figsize=(10, 8))
plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')
plt.title('Correlation Matrix')
plt.colorbar()
plt.show()

# Test the association between voting behavior and demographics
t_stat, p_val = ttest_ind(df['voting_behavior'], df['demographic_variable'], equal_var=False)
print(f'T-statistic: {t_stat}, p-value: {p_val}')

# Calculate the 95% confidence interval for the mean voting behavior
ci = np.percentile(df['voting_behavior'], [2.5, 97.5])
print(f'95% CI: ({ci[0]}, {ci[1]})')

# Limitations and confounders
print('Note: The results are limited to the data available and may be influenced by confounding variables not accounted for in this analysis.')","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected in this code example. The following good practices were observed:\n\n* The code uses appropriate causal language, stating associations and correlations rather than causality.\n* Uncertainty is acknowledged through the calculation of confidence intervals and the presentation of p-values.\n* The code acknowledges limitations and confounders, recognizing that the results are limited to the data available and may be influenced by variables no', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('voting_data.csv')

# Explore the data
print(df.head())
print(df.describe())

# Correlation analysis
corr_matrix = df.corr()
print(corr_matrix)

# Plot the correlation matrix
plt.figure(figsize=(10, 8))
plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')
plt.title('Correlation Matrix')
plt.colorbar()
plt.show()

# Test the association between voting behavior and demographics
t_stat, p_val = ttest_ind(df['voting_behavior'], df['demographic_variable'], equal_var=False)
print(f'T-statistic: {t_stat}, p-value: {p_val}')

# Calculate the 95% confidence interval for the mean voting behavior
ci = np.percentile(df['voting_behavior'], [2.5, 97.5])
print(f'95% CI: ({ci[0]}, {ci[1]})')

# Limitations and confounders
print('Note: The results are limited to the data available and may be influenced by confounding variables not accounted for in this analysis.')",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:57:36.985386,llama-3-8b-v2,v2,True
none,none,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load data
df = pd.read_csv(""social_media_data.csv"")

# Calculate correlation between social media usage and mental health scores
correlation, p_value = pearsonr(df[""social_media_usage""], df[""mental_health_scores""])

# Calculate confidence interval for the correlation
conf_int = pearsonr(df[""social_media_usage""], df[""mental_health_scores""], confidence=0.95)

# Create a plot to visualize the relationship
sns.scatterplot(x=""social_media_usage"", y=""mental_health_scores"", data=df)
plt.xlabel(""Social Media Usage"")
plt.ylabel(""Mental Health Scores"")
plt.title(""Relationship between Social Media Usage and Mental Health Scores"")
plt.show()

# Print the results
print(f""The correlation between social media usage and mental health scores is {correlation:.3f}, with a p-value of {p_value:.3f}."")
print(f""The 95% confidence interval for the correlation is ({conf_int[0]:.3f}, {conf_int[1]:.3f})."")

# Acknowledge limitations and confounders
print(""Note: This analysis does not establish a causal relationship between social media usage and mental health scores. There may be confounding variables that affect the observed correlation."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': ""```\n\n**Code Review:**\n\nNo major statistical issues detected!\n\nGood practices observed in this code include:\n\n* The use of a robust correlation coefficient (Pearson's r) to quantify the association between social media usage and mental health scores, rather than making claims about causality.\n* The inclusion of a p-value to provide a measure of the statistical significance of the observed correlation.\n* The calculation of a confidence interval to provide a range of plausible values for the corre"", 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load data
df = pd.read_csv(""social_media_data.csv"")

# Calculate correlation between social media usage and mental health scores
correlation, p_value = pearsonr(df[""social_media_usage""], df[""mental_health_scores""])

# Calculate confidence interval for the correlation
conf_int = pearsonr(df[""social_media_usage""], df[""mental_health_scores""], confidence=0.95)

# Create a plot to visualize the relationship
sns.scatterplot(x=""social_media_usage"", y=""mental_health_scores"", data=df)
plt.xlabel(""Social Media Usage"")
plt.ylabel(""Mental Health Scores"")
plt.title(""Relationship between Social Media Usage and Mental Health Scores"")
plt.show()

# Print the results
print(f""The correlation between social media usage and mental health scores is {correlation:.3f}, with a p-value of {p_value:.3f}."")
print(f""The 95% confidence interval for the correlation is ({conf_int[0]:.3f}, {conf_int[1]:.3f})."")

# Acknowledge limitations and confounders
print(""Note: This analysis does not establish a causal relationship between social media usage and mental health scores. There may be confounding variables that affect the observed correlation."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:58:23.571735,llama-3-8b-v2,v2,True
none,none,social_science,social media usage and mental health,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load data
crime_data = pd.read_csv('crime_rate_data.csv')

# Explore relationships between variables
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x='unemployment_rate', y='crime_rate', data=crime_data)
plt.xlabel('Unemployment Rate')
plt.ylabel('Crime Rate')
plt.title('Relationship between Unemployment Rate and Crime Rate')
plt.show()

# Calculate Pearson's r and p-value
r, p_value = pearsonr(crime_data['unemployment_rate'], crime_data['crime_rate'])
print(f""Correlation coefficient: {r:.3f}"")
print(f""P-value: {p_value:.4f}"")

# Interpret results
print(""The correlation between unemployment rate and crime rate is statistically significant (p-value < 0.05),"")
print(""indicating a moderate positive association between the two variables. However, we must acknowledge"")
print(""that this analysis is limited by the potential presence of confounding variables, such as"")
print(""poverty rates or education levels, which may influence both the unemployment rate and crime rate."")

# Visualize uncertainty (95% confidence interval for the correlation coefficient)
CI = np.around(pearsonr(crime_data['unemployment_rate'], crime_data['crime_rate'])[0] * np.sqrt((1 - r**2) / (crime_data.shape[0] - 2)), 3)
print(f""95% Confidence Interval for the correlation coefficient: [{CI[0]:.3f}, {CI[1]:.3f}]"")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review**\nNo major statistical issues detected.\n\nThis code demonstrates several good practices:\n\n1. **Causal language**: We avoid claiming causality and instead focus on the association between the variables, using phrases like ""moderate positive association"" and ""statistically significant"".\n2. **Uncertainty**: We include the p-value and confidence interval to quantify the uncertainty associated with our estimate of the correlation coefficient.\n3. **Acknowledgment of limitations**:', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load data
crime_data = pd.read_csv('crime_rate_data.csv')

# Explore relationships between variables
sns.set()
plt.figure(figsize=(8, 6))
sns.scatterplot(x='unemployment_rate', y='crime_rate', data=crime_data)
plt.xlabel('Unemployment Rate')
plt.ylabel('Crime Rate')
plt.title('Relationship between Unemployment Rate and Crime Rate')
plt.show()

# Calculate Pearson's r and p-value
r, p_value = pearsonr(crime_data['unemployment_rate'], crime_data['crime_rate'])
print(f""Correlation coefficient: {r:.3f}"")
print(f""P-value: {p_value:.4f}"")

# Interpret results
print(""The correlation between unemployment rate and crime rate is statistically significant (p-value < 0.05),"")
print(""indicating a moderate positive association between the two variables. However, we must acknowledge"")
print(""that this analysis is limited by the potential presence of confounding variables, such as"")
print(""poverty rates or education levels, which may influence both the unemployment rate and crime rate."")

# Visualize uncertainty (95% confidence interval for the correlation coefficient)
CI = np.around(pearsonr(crime_data['unemployment_rate'], crime_data['crime_rate'])[0] * np.sqrt((1 - r**2) / (crime_data.shape[0] - 2)), 3)
print(f""95% Confidence Interval for the correlation coefficient: [{CI[0]:.3f}, {CI[1]:.3f}]"")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T13:59:18.921061,llama-3-8b-v2,v2,True
none,none,social_science,crime rate factors,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Define variables of interest
X = data['social_media_usage_hours']
Y = data['mental_health_index']

# Calculate correlation coefficient and p-value
correlation_coefficient, p_value = pearsonr(X, Y)

print(f""Correlation coefficient: {correlation_coefficient:.3f}"")
print(f""P-value: {p_value:.4f}"")

# Create a scatter plot to visualize the relationship
plt.scatter(X, Y)
plt.xlabel('Social Media Usage Hours')
plt.ylabel('Mental Health Index')
plt.title('Relationship between Social Media Usage and Mental Health')
plt.show()

# Calculate 95% confidence interval for the correlation coefficient
confidence_interval = pearsonr(X, Y)[1][1]
print(f""95% Confidence Interval: ({confidence_interval[0]:.3f}, {confidence_interval[1]:.3f})"")

# Interpretation:
print(""While there is a statistically significant correlation between social media usage and mental health index (p-value < 0.05), we must exercise caution when interpreting this result."")
print(""This correlation does not necessarily imply causality, and other factors may be at play. Future research should investigate the underlying mechanisms driving this relationship."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Review:**\n\nNo major statistical issues detected in this code example. The following good practices were observed:\n\n* The code uses proper statistical language, avoiding causal language and instead emphasizing the correlation between the variables.\n* The code includes uncertainty in the form of a p-value and a 95% confidence interval for the correlation coefficient.\n* The code acknowledges the limitations of the analysis, mentioning the need for future research to investigate the underlyi', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Load data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Define variables of interest
X = data['social_media_usage_hours']
Y = data['mental_health_index']

# Calculate correlation coefficient and p-value
correlation_coefficient, p_value = pearsonr(X, Y)

print(f""Correlation coefficient: {correlation_coefficient:.3f}"")
print(f""P-value: {p_value:.4f}"")

# Create a scatter plot to visualize the relationship
plt.scatter(X, Y)
plt.xlabel('Social Media Usage Hours')
plt.ylabel('Mental Health Index')
plt.title('Relationship between Social Media Usage and Mental Health')
plt.show()

# Calculate 95% confidence interval for the correlation coefficient
confidence_interval = pearsonr(X, Y)[1][1]
print(f""95% Confidence Interval: ({confidence_interval[0]:.3f}, {confidence_interval[1]:.3f})"")

# Interpretation:
print(""While there is a statistically significant correlation between social media usage and mental health index (p-value < 0.05), we must exercise caution when interpreting this result."")
print(""This correlation does not necessarily imply causality, and other factors may be at play. Future research should investigate the underlying mechanisms driving this relationship."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T14:00:09.384520,llama-3-8b-v2,v2,True
none,none,social_science,crime rate factors,python,intermediate,"import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import linregress

# Load data
df = pd.read_csv('income_inequality_data.csv')

# Merge data
merged_df = pd.merge(df, df.groupby('year')['mean_income'].max().reset_index(), on='year')

# Calculate correlation coefficient
corr_coef, _ = linregress(merged_df['year'], merged_df['mean_income'])

# Plot data
plt.figure(figsize=(8, 6))
sns.scatterplot(x='year', y='mean_income', data=merged_df)
plt.xlabel('Year')
plt.ylabel('Mean Income')
plt.title('Income Inequality Trends')
plt.show()

# Calculate confidence interval for correlation coefficient
conf_int = linregress.conf_int(corr_coef, merged_df['year'].shape[0])

print(f""Correlation Coefficient: {corr_coef:.3f} ({conf_int[0]:.3f}, {conf_int[1]:.3f})"")
print(f""P-value: {linregress.pvalue(corr_coef, merged_df['year'].shape[0]):.4f}"")

# Interpretation
print(""The correlation coefficient suggests a positive association between year and mean income. However, this does not imply causality. Other factors, such as economic growth and demographic changes, may contribute to this trend."")
print(""The confidence interval suggests that the true correlation coefficient lies within the range of {:.3f} and {:.3f}."".format(conf_int[0], conf_int[1]))
print(""The p-value indicates that the observed correlation is statistically significant (p-value < 0.05)."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Review:**\n\nNo major statistical issues detected. The code demonstrates several good practices:\n\n* The code uses proper causal language, stating that the correlation coefficient suggests a positive association between year and mean income, but does not imply causality.\n* The code includes uncertainty by calculating the confidence interval for the correlation coefficient and reporting the p-value.\n* The code acknowledges limitations by mentioning that other factors may contribute to the tr', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import linregress

# Load data
df = pd.read_csv('income_inequality_data.csv')

# Merge data
merged_df = pd.merge(df, df.groupby('year')['mean_income'].max().reset_index(), on='year')

# Calculate correlation coefficient
corr_coef, _ = linregress(merged_df['year'], merged_df['mean_income'])

# Plot data
plt.figure(figsize=(8, 6))
sns.scatterplot(x='year', y='mean_income', data=merged_df)
plt.xlabel('Year')
plt.ylabel('Mean Income')
plt.title('Income Inequality Trends')
plt.show()

# Calculate confidence interval for correlation coefficient
conf_int = linregress.conf_int(corr_coef, merged_df['year'].shape[0])

print(f""Correlation Coefficient: {corr_coef:.3f} ({conf_int[0]:.3f}, {conf_int[1]:.3f})"")
print(f""P-value: {linregress.pvalue(corr_coef, merged_df['year'].shape[0]):.4f}"")

# Interpretation
print(""The correlation coefficient suggests a positive association between year and mean income. However, this does not imply causality. Other factors, such as economic growth and demographic changes, may contribute to this trend."")
print(""The confidence interval suggests that the true correlation coefficient lies within the range of {:.3f} and {:.3f}."".format(conf_int[0], conf_int[1]))
print(""The p-value indicates that the observed correlation is statistically significant (p-value < 0.05)."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T14:01:07.743029,llama-3-8b-v2,v2,True
none,none,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('crime_data.csv')

# Define the variables of interest
x = df['poverty_rate']
y = df['crime_rate']

# Calculate the correlation coefficient
corr_coef, _ = np.polyfit(x, y, 1)
print(f""Correlation coefficient: {corr_coef:.3f}"")

# Plot the data with confidence intervals
plt.scatter(x, y)
plt.xlabel('Poverty Rate')
plt.ylabel('Crime Rate')
plt.title('Association between Poverty Rate and Crime Rate')
plt.errorbar(x, y, xerr=np.std(x)/np.sqrt(len(x)), fmt='o', color='r', capsize=5)
plt.show()

# Perform a t-test to check for statistical significance
t_stat, p_val = ttest_ind(x, y)
print(f""t-statistic: {t_stat:.3f}, p-value: {p_val:.4f}"")

# Acknowledge limitations
print(""Note: This analysis is limited by the potential confounder of other socioeconomic factors not included in the dataset."")

# Interpret the results
print(""The results suggest a significant positive association between poverty rate and crime rate, but it is important to note that this does not imply causality. Further investigation is needed to determine the direction of the relationship."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected.\n\nGood practices observed in this code include:\n\n* The use of a correlation coefficient to quantify the strength and direction of the association between poverty rate and crime rate, rather than making causal claims.\n* The inclusion of uncertainty through the use of confidence intervals and a p-value, which provide a measure of the statistical significance of the observed association.\n* Acknowledgment of potential limitations and conf', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('crime_data.csv')

# Define the variables of interest
x = df['poverty_rate']
y = df['crime_rate']

# Calculate the correlation coefficient
corr_coef, _ = np.polyfit(x, y, 1)
print(f""Correlation coefficient: {corr_coef:.3f}"")

# Plot the data with confidence intervals
plt.scatter(x, y)
plt.xlabel('Poverty Rate')
plt.ylabel('Crime Rate')
plt.title('Association between Poverty Rate and Crime Rate')
plt.errorbar(x, y, xerr=np.std(x)/np.sqrt(len(x)), fmt='o', color='r', capsize=5)
plt.show()

# Perform a t-test to check for statistical significance
t_stat, p_val = ttest_ind(x, y)
print(f""t-statistic: {t_stat:.3f}, p-value: {p_val:.4f}"")

# Acknowledge limitations
print(""Note: This analysis is limited by the potential confounder of other socioeconomic factors not included in the dataset."")

# Interpret the results
print(""The results suggest a significant positive association between poverty rate and crime rate, but it is important to note that this does not imply causality. Further investigation is needed to determine the direction of the relationship."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T14:01:56.345033,llama-3-8b-v2,v2,True
none,none,social_science,income inequality trends,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# Load data
df = pd.read_csv(""social_media_data.csv"")

# Explore data
print(df.head())
print(df.describe())

# Visualize correlation between social media usage and mental health
sns.set()
sns.scatterplot(x=""social_media_usage"", y=""mental_health"", data=df)
plt.xlabel(""Social Media Usage (hours/week)"")
plt.ylabel(""Mental Health (self-reported scale)"")
plt.title(""Correlation between Social Media Usage and Mental Health"")
plt.show()

# Calculate Pearson's r and p-value
r, p = pearsonr(df[""social_media_usage""], df[""mental_health""])
print(f""Pearson's r: {r:.2f}, p-value: {p:.4f}"")

# Interpret results
print(""Results suggest a moderate, statistically significant association between social media usage and mental health."")
print(""However, it is important to note that this analysis does not establish causality."")
print(""Future studies should consider confounding variables such as age, gender, and socioeconomic status."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': ""```\n\n**Review:**\n\nNo major statistical issues detected in this code example. The following good practices were observed:\n\n* Clear and concise variable naming and labeling\n* Proper use of correlation language (association, not causality)\n* Inclusion of uncertainty (Pearson's r and p-value) and acknowledgment of limitations (confounding variables)\n\nThis code example demonstrates a basic exploratory data analysis (EDA) and visualization of the relationship between social media usage and mental hea"", 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# Load data
df = pd.read_csv(""social_media_data.csv"")

# Explore data
print(df.head())
print(df.describe())

# Visualize correlation between social media usage and mental health
sns.set()
sns.scatterplot(x=""social_media_usage"", y=""mental_health"", data=df)
plt.xlabel(""Social Media Usage (hours/week)"")
plt.ylabel(""Mental Health (self-reported scale)"")
plt.title(""Correlation between Social Media Usage and Mental Health"")
plt.show()

# Calculate Pearson's r and p-value
r, p = pearsonr(df[""social_media_usage""], df[""mental_health""])
print(f""Pearson's r: {r:.2f}, p-value: {p:.4f}"")

# Interpret results
print(""Results suggest a moderate, statistically significant association between social media usage and mental health."")
print(""However, it is important to note that this analysis does not establish causality."")
print(""Future studies should consider confounding variables such as age, gender, and socioeconomic status."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T14:02:41.397351,llama-3-8b-v2,v2,True
none,none,social_science,crime rate factors,python,intermediate,"import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv(""social_media_usage_and_mental_health.csv"")

# Explore data
print(df.describe())

# Visualize data
plt.scatter(df[""social_media_usage""], df[""mental_health""])
plt.xlabel(""Social Media Usage (hours/day)"")
plt.ylabel(""Mental Health (scale: 1-5)"")
plt.title(""Relationship between Social Media Usage and Mental Health"")
plt.show()

# Perform correlation analysis
corr_coef, p_value = pearsonr(df[""social_media_usage""], df[""mental_health""])
print(f""Correlation Coefficient: {corr_coef:.3f}, p-value: {p_value:.4f}"")

# Perform t-test to compare means
t_stat, p_value = ttest_ind(df[""social_media_usage""], df[""mental_health""], equal_var=False)
print(f""T-statistic: {t_stat:.3f}, p-value: {p_value:.4f}"")

# Interpret results
print(""Based on the data, there appears to be a significant association between social media usage and mental health (p-value < 0.05)."")
print(""However, this does not imply a causal relationship. Further research is needed to establish a causal link."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review**\n\nNo major statistical issues detected in this code. The following good practices were observed:\n\n* The code uses appropriate causal language, explicitly stating that the association between social media usage and mental health is not necessarily causal.\n* The code includes uncertainty by reporting p-values and confidence intervals, acknowledging the limitations of statistical inference.\n* The code acknowledges limitations by mentioning the need for further research to esta', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
df = pd.read_csv(""social_media_usage_and_mental_health.csv"")

# Explore data
print(df.describe())

# Visualize data
plt.scatter(df[""social_media_usage""], df[""mental_health""])
plt.xlabel(""Social Media Usage (hours/day)"")
plt.ylabel(""Mental Health (scale: 1-5)"")
plt.title(""Relationship between Social Media Usage and Mental Health"")
plt.show()

# Perform correlation analysis
corr_coef, p_value = pearsonr(df[""social_media_usage""], df[""mental_health""])
print(f""Correlation Coefficient: {corr_coef:.3f}, p-value: {p_value:.4f}"")

# Perform t-test to compare means
t_stat, p_value = ttest_ind(df[""social_media_usage""], df[""mental_health""], equal_var=False)
print(f""T-statistic: {t_stat:.3f}, p-value: {p_value:.4f}"")

# Interpret results
print(""Based on the data, there appears to be a significant association between social media usage and mental health (p-value < 0.05)."")
print(""However, this does not imply a causal relationship. Further research is needed to establish a causal link."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T14:03:32.205962,llama-3-8b-v2,v2,True
none,none,social_science,voting behavior analysis,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('social_media_usage_and_mental_health_data.csv')

# Convert categorical variables to numerical variables
df['social_media_usage'] = pd.cut(df['social_media_usage'], bins=[0, 1, 2, 3, 4], labels=[0, 1, 2, 3])

# Calculate the correlation between social media usage and mental health scores
corr_coef, p_value = np.polyfit(df['social_media_usage'], df['mental_health_scores'], 1)

print(f""The correlation coefficient is: {corr_coef:.3f} (p-value: {p_value:.3f})"")

# Create a scatter plot to visualize the relationship
plt.scatter(df['social_media_usage'], df['mental_health_scores'])
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health Scores')
plt.title('Correlation between Social Media Usage and Mental Health Scores')
plt.show()

# Perform a t-test to determine if the mean mental health scores differ between social media usage groups
t_stat, p_value = ttest_ind(df.loc[df['social_media_usage'] == 1,'mental_health_scores'], 
                             df.loc[df['social_media_usage'] == 2,'mental_health_scores'])

print(f""The t-statistic is: {t_stat:.3f} (p-value: {p_value:.3f})"")

# Print a note about the limitations of the analysis
print(""Note: The analysis assumes that the data is representative of the population and that there are no major confounders."")","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review:**\n\nNo major statistical issues detected in this code example. The code demonstrates good practices in statistical analysis, including:\n\n* Using appropriate causal language (e.g., ""correlation"" instead of ""causal relationship"")\n* Including uncertainty (error bars, confidence intervals, p-values) in the analysis\n* Acknowledging limitations (e.g., ""Note: The analysis assumes that the data is representative of the population and that there are no major confounders"")\n* Professio', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load the data
df = pd.read_csv('social_media_usage_and_mental_health_data.csv')

# Convert categorical variables to numerical variables
df['social_media_usage'] = pd.cut(df['social_media_usage'], bins=[0, 1, 2, 3, 4], labels=[0, 1, 2, 3])

# Calculate the correlation between social media usage and mental health scores
corr_coef, p_value = np.polyfit(df['social_media_usage'], df['mental_health_scores'], 1)

print(f""The correlation coefficient is: {corr_coef:.3f} (p-value: {p_value:.3f})"")

# Create a scatter plot to visualize the relationship
plt.scatter(df['social_media_usage'], df['mental_health_scores'])
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health Scores')
plt.title('Correlation between Social Media Usage and Mental Health Scores')
plt.show()

# Perform a t-test to determine if the mean mental health scores differ between social media usage groups
t_stat, p_value = ttest_ind(df.loc[df['social_media_usage'] == 1,'mental_health_scores'], 
                             df.loc[df['social_media_usage'] == 2,'mental_health_scores'])

print(f""The t-statistic is: {t_stat:.3f} (p-value: {p_value:.3f})"")

# Print a note about the limitations of the analysis
print(""Note: The analysis assumes that the data is representative of the population and that there are no major confounders."")",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T14:04:25.829272,llama-3-8b-v2,v2,True
none,none,social_science,income inequality trends,python,intermediate,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Explore data
print(data.describe())

# Correlation analysis
corr_matrix = data.corr()
print(corr_matrix)

# Scatter plot of social media usage and mental health scores
plt.scatter(data['social_media_usage'], data['mental_health_scores'])
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health Scores')
plt.title('Correlation Between Social Media Usage and Mental Health Scores')
plt.show()

# Regression analysis
from sklearn.linear_model import LinearRegression
X = data['social_media_usage']
y = data['mental_health_scores']
model = LinearRegression()
model.fit(X, y)
print(f'R-squared: {model.score(X, y):.2f}')
print(f'p-value: {model.p_value:.2f}')

# T-test for comparison between two groups
group1 = data[data['social_media_usage'] > 2]
group2 = data[data['social_media_usage'] <= 2]
t_stat, p_val = ttest_ind(group1['mental_health_scores'], group2['mental_health_scores'])
print(f'T-statistic: {t_stat:.2f}')
print(f'p-value: {p_val:.2f}')

# Print a summary of the results
print('Summary:')
print(f'R-squared: {model.score(X, y):.2f}')
print(f'p-value: {model.p_value:.2f}')
print(f'T-statistic: {t_stat:.2f}')
print(f'p-value: {p_val:.2f}')","{'summary': 'No major statistical issues detected', 'detailed_explanation': '```\n\n**Brief Review**\n\nNo major statistical issues detected. The code demonstrates good practices in statistical analysis, including:\n\n1. **Clear and concise variable naming**: The code uses descriptive variable names, such as `social_media_usage` and `mental_health_scores`, making it easy to understand the variables being analyzed.\n2. **Appropriate use of statistical tests**: The code uses the t-test to compare the means of two groups, and the linear regression analysis to examine the relation', 'line_numbers': [], 'consequences': 'N/A - good practices'}","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind

# Load data
data = pd.read_csv('social_media_usage_and_mental_health.csv')

# Explore data
print(data.describe())

# Correlation analysis
corr_matrix = data.corr()
print(corr_matrix)

# Scatter plot of social media usage and mental health scores
plt.scatter(data['social_media_usage'], data['mental_health_scores'])
plt.xlabel('Social Media Usage')
plt.ylabel('Mental Health Scores')
plt.title('Correlation Between Social Media Usage and Mental Health Scores')
plt.show()

# Regression analysis
from sklearn.linear_model import LinearRegression
X = data['social_media_usage']
y = data['mental_health_scores']
model = LinearRegression()
model.fit(X, y)
print(f'R-squared: {model.score(X, y):.2f}')
print(f'p-value: {model.p_value:.2f}')

# T-test for comparison between two groups
group1 = data[data['social_media_usage'] > 2]
group2 = data[data['social_media_usage'] <= 2]
t_stat, p_val = ttest_ind(group1['mental_health_scores'], group2['mental_health_scores'])
print(f'T-statistic: {t_stat:.2f}')
print(f'p-value: {p_val:.2f}')

# Print a summary of the results
print('Summary:')
print(f'R-squared: {model.score(X, y):.2f}')
print(f'p-value: {model.p_value:.2f}')
print(f'T-statistic: {t_stat:.2f}')
print(f'p-value: {p_val:.2f}')",['Good statistical practices'],Demonstrates proper methodology,2025-12-16T14:05:26.837243,llama-3-8b-v2,v2,True
